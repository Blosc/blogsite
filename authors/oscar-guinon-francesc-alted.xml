<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blosc Main Blog Page  (Posts by Oscar Guiñon, Francesc Alted)</title><link>http://blosc.org/</link><description></description><atom:link href="http://blosc.org/authors/oscar-guinon-francesc-alted.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2022 &lt;a href="mailto:blosc@blosc.org"&gt;The Blosc Developers&lt;/a&gt; </copyright><lastBuildDate>Fri, 03 Jun 2022 11:38:11 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Announcing Support for Lossy ZFP Codec as a Plugin for C-Blosc2</title><link>http://blosc.org/posts/support-lossy-zfp/</link><dc:creator>Oscar Guiñon, Francesc Alted</dc:creator><description>&lt;section id="announcing-support-for-lossy-zfp-codec-as-a-plugin-for-c-blosc2"&gt;
&lt;h2&gt;Announcing Support for Lossy ZFP Codec as a Plugin for C-Blosc2&lt;/h2&gt;
&lt;p&gt;Blosc supports different filters and codecs for compressing data, like e.g. the lossless &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/tree/main/plugins/codecs/ndlz"&gt;NDLZ&lt;/a&gt; codec and the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/tree/main/plugins/filters/ndcell"&gt;NDCELL&lt;/a&gt; filter.  These have been developed explicitly to be used in   multidimensional datasets (via &lt;a class="reference external" href="https://github.com/Blosc/caterva/"&gt;Caterva&lt;/a&gt; or &lt;a class="reference external" href="https://github.com/ironArray/iarray-community"&gt;ironArray Community Edition&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;However, a lossy codec like &lt;a class="reference external" href="https://zfp.readthedocs.io/"&gt;ZFP&lt;/a&gt; allows for much better compression ratios at the expense of loosing some precision in floating point data.  Moreover, while NDLZ is only available for 2-dim datasets, ZFP can be used up to 4-dim datasets.&lt;/p&gt;
&lt;section id="how-zfp-works"&gt;
&lt;h3&gt;How ZFP works?&lt;/h3&gt;
&lt;p&gt;ZFP partitions datasets into cells of 4^(number of dimensions) values, i.e., 4, 16, 64, or 256 values for 1D, 2D, 3D, and 4D arrays, respectively. Each cell is then (de)compressed independently, and the resulting bit strings are concatenated into a single stream of bits.&lt;/p&gt;
&lt;p&gt;Furthermore, ZFP usually truncates each input value either to a fixed number of bits to meet a storage budget or to some variable length needed to meet a chosen error tolerance.  For more info on how this works, see &lt;a class="reference external" href="https://zfp.readthedocs.io/en/release0.5.5/overview.html"&gt;zfp overview docs&lt;/a&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="zfp-implementation"&gt;
&lt;h3&gt;ZFP implementation&lt;/h3&gt;
&lt;p&gt;Similarly to other registered Blosc2 official plugins, this codec is now available at the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/tree/main/plugins/codecs/zfp"&gt;blosc2/plugins directory&lt;/a&gt; of the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;C-Blosc2 repository&lt;/a&gt;.  However, as there are different modes for working with ZFP, there are several associated codec IDs (see later).&lt;/p&gt;
&lt;p&gt;So, in order to use ZFP, users just have to choose the ID for the desired ZFP mode between the ones listed in &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/main/include/blosc2/codecs-registry.h"&gt;blosc2/codecs-registry.h&lt;/a&gt;. For more info on how the plugin selection mechanism works, see &lt;a class="reference external" href="https://www.blosc.org/posts/registering-plugins/"&gt;https://www.blosc.org/posts/registering-plugins/&lt;/a&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="zfp-modes"&gt;
&lt;h3&gt;ZFP modes&lt;/h3&gt;
&lt;p&gt;ZFP is a lossy codec, but it still lets the user to choose the degree of the data loss.  There are different compression modes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;BLOSC_CODEC_ZFP_FIXED_ACCURACY:&lt;/strong&gt; The user can choose the absolute error in truncation.  For example, if the desired absolute error is 0.01, each value loss must be less than or equal to 0.01. With that, if 23.0567 is a value of the original input, after compressing and decompressing this input with error=0.01, the new value must be between 23.0467 and 23.0667.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;BLOSC_CODEC_ZFP_FIXED_PRECISION:&lt;/strong&gt; The user specifies the maximum number of bit planes encoded during compression (relative error). This is, for each input value, the number of most significant bits that will be encoded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;BLOSC_CODEC_ZFP_FIXED_RATE:&lt;/strong&gt; The user chooses the size that the compressed cells must have based on the input cell size. For example, if the cell size is 2000 bytes and user chooses ratio=50, the output cell size will be 50% of 2000 = 1000 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more info, see: &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/main/plugins/codecs/zfp/README.md"&gt;https://github.com/Blosc/c-blosc2/blob/main/plugins/codecs/zfp/README.md&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;section id="benchmark-zfp-fixed-accuracy-vs-fixed-precision-vs-fixed-rate-modes"&gt;
&lt;h3&gt;Benchmark: ZFP FIXED-ACCURACY VS FIXED_PRECISION VS FIXED-RATE modes&lt;/h3&gt;
&lt;p&gt;The dataset used in this benchmark is called &lt;em&gt;precipitation_amount_1hour_Accumulation.zarr&lt;/em&gt; and has been fetched from &lt;a class="reference external" href="https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5"&gt;ERA5 database&lt;/a&gt;, which provides hourly estimates of a large number of atmospheric, land and oceanic climate variables.&lt;/p&gt;
&lt;p&gt;Specifically, the downloaded dataset in Caterva format has this parameters:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;ndim = 3&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;type = float32&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;shape = [720, 721, 1440]&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;chunkshape = [128, 128, 256]&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;blockshape = [16, 32, 64]&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next plots represent the compression results obtained by using the different ZFP modes to compress the already mentioned dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; It is important to remark that this is a specific dataset and the codec may perform differently for other ones.&lt;/p&gt;
&lt;img alt="/images/zfp-plugin/ratio_zfp.png" class="align-center" src="http://blosc.org/images/zfp-plugin/ratio_zfp.png" style="width: 100%;"&gt;
&lt;img alt="/images/zfp-plugin/times_zfp.png" class="align-center" src="http://blosc.org/images/zfp-plugin/times_zfp.png" style="width: 100%;"&gt;
&lt;p&gt;Below the bars it is annotated what parameter is used for each test. For example, for the first column, the different compression modes are setup like this:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;FIXED-ACCURACY: for each input value, the absolute error is 10^(-6) = 0.000001.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FIXED-PRECISION: for each input value, only the 20 most significant bits for the mantissa will be encoded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FIXED-RATE: the size of the output cells is 30% of the input cell size.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the FIXED-PRECISION mode does not obtain great results, we see that with the FIXED-ACCURACY mode we do get better performance as the absolute error increases.  Similarly, we can see how the FIXED-RATE mode gets the requested ratios, which is cool but, in exchange, the amount of data loss is unknown.&lt;/p&gt;
&lt;p&gt;Also, while FIXED-ACCURACY and FIXED-RATE modes consume similar times, the FIXED-PRECISION mode, which seems to have less data loss, also takes longer to compress.  Generally speaking we can see how, the more data loss (more data truncation) achieved by a mode, the faster it operates.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="third-partition"&gt;
&lt;h3&gt;"Third partition"&lt;/h3&gt;
&lt;p&gt;One of the most appealing features of Caterva besides supporting multi-dimensionality is its implementation of a second partition, &lt;a class="reference external" href="https://www.blosc.org/posts/caterva-slicing-perf/"&gt;making slicing more efficient&lt;/a&gt;.  As one of the distinctive characteristics of ZFP is that it compresses data in independent (and small) cells, we have been toying with the idea of implementing a third partition so that slicing of thin selections or just single-point selection can be made faster.&lt;/p&gt;
&lt;p&gt;So, as part of the current ZFP implementation, we have combined the Caterva/Blosc2 partitioning (chunking and blocking) with the independent cell handling of ZFP, allowing to extract single cells within the ZFP streams (blocks in Blosc jargon). Due to the properties and limitations of the different ZFP compression modes, we have been able to implement a sort of "third partition" just for the &lt;em&gt;FIXED-RATE&lt;/em&gt; mode when used together with the &lt;a class="reference external" href="https://c-blosc2.readthedocs.io/en/latest/reference/context.html?highlight=blosc_getitem#c.blosc2_getitem_ctx"&gt;blosc2_getitem_ctx()&lt;/a&gt; function.&lt;/p&gt;
&lt;p&gt;Such a combination of the existing partitioning and single cell extraction is useful for selecting more narrowly the data to extract, saving time and memory.  As an example, below you can see a comparison of the mean times that it takes to retrieve a bunch of single elements out of different multidimensional arrays from the ERA5 dataset (see above).  Here we have used Blosc2 with a regular LZ4 codec compared against the FIXED-RATE mode of the new ZFP codec:&lt;/p&gt;
&lt;img alt="/images/zfp-plugin/zfp_fixed_rate.png" class="align-center" src="http://blosc.org/images/zfp-plugin/zfp_fixed_rate.png" style="width: 100%;"&gt;
&lt;p&gt;As you can see, using the ZFP codec in FIXED-RATE mode allows for a good improvement in speed (up to more than 2x) for retrieving single elements (or, in general an amount not exceeding the cell size) in comparison with the existing codecs (even the fastest ones, like LZ4) inside Blosc2.  As the performance improvement is of the same order than random access time of modern SSDs, we anticipate that this could be a major win in scenarios where random access is important.&lt;/p&gt;
&lt;p&gt;If you are curious on how this new functionality performs for your own datasets and computer, you can use/adapt our &lt;a class="reference external" href="https://github.com/Blosc/caterva/blob/master/bench/bench_zfp_getitem.c"&gt;benchmark code&lt;/a&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="conclusions"&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;The integration of ZFP as a codec plugin will greatly enhance the capabilities of lossy compression inside C-Blosc2.  The current ZFP plugin supports different modes; if users want to specify data loss during compression, it is recommended to use the FIXED-ACCURACY or FIXED-PRECISION modes (and most specially the former because of its better compression performance).&lt;/p&gt;
&lt;p&gt;However, if the priority is to get specific compression ratios without paying too much attention to the amount of data loss, one should use the FIXED-RATE mode, which let choose the desired compression ratio.  This mode also has the advantage that a "third partition" can be used for improving random elements access speed.&lt;/p&gt;
&lt;p&gt;This work has been done thanks to a Small Development Grant from the &lt;a class="reference external" href="https://numfocus.org"&gt;NumFOCUS Foundation&lt;/a&gt;, to whom we are very grateful indeed. NumFOCUS is doing a excellent job in sponsoring scientific projects and you can donate to the Blosc project (or many others under the NumFOCUS umbrella) via its &lt;a class="reference external" href="https://numfocus.org/support#donate"&gt;donation page&lt;/a&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;</description><category>blosc plugins zfp lossy</category><guid>http://blosc.org/posts/support-lossy-zfp/</guid><pubDate>Fri, 11 Mar 2022 10:32:20 GMT</pubDate></item><item><title>Caterva Slicing Performance: A Study</title><link>http://blosc.org/posts/caterva-slicing-perf/</link><dc:creator>Oscar Guiñon, Francesc Alted</dc:creator><description>&lt;img alt="/images/cat_slicing/caterva.png" class="align-center" src="http://blosc.org/images/cat_slicing/caterva.png" style="width: 50%;"&gt;
&lt;p&gt;&lt;a class="reference external" href="https://caterva.readthedocs.io/en/latest/getting_started/overview.html"&gt;Caterva&lt;/a&gt; is a C library for handling multi-dimensional, chunked, compressed datasets in an easy and fast way.  It is build on top of the &lt;a class="reference external" href="https://c-blosc2.readthedocs.io/en/latest/"&gt;C-Blosc2&lt;/a&gt; library, leveraging all its &lt;a class="reference external" href="https://www.blosc.org/posts/blosc2-ready-general-review/"&gt;avantages on modern CPUs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Caterva can be used in a lot of different situations; however, where it really stands out is for extracting multidimensional slices of compressed datasets because, thanks to the double partitioning schema that it implements, the amount of data that has to be decompressed so as to get the slice is minimized, making data extraction faster (usually).  In this installment, you will be offered a rational on how double partitioning works, together with some examples where it shines, and others where it is not that good.&lt;/p&gt;
&lt;section id="double-partitioning"&gt;
&lt;h2&gt;Double partitioning&lt;/h2&gt;
&lt;img alt="/images/cat_slicing/cat_vs_zarr,hdf5.png" class="align-center" src="http://blosc.org/images/cat_slicing/cat_vs_zarr,hdf5.png" style="width: 70%;"&gt;
&lt;p&gt;Some libraries like &lt;a class="reference external" href="https://www.hdfgroup.org/solutions/hdf5/"&gt;HDF5&lt;/a&gt; or &lt;a class="reference external" href="https://zarr.readthedocs.io/en/stable/"&gt;Zarr&lt;/a&gt; store data into multidimensional chunks. This makes slice extraction from compressed datasets more efficient than using monolithic compression, since only the chunks containing the interesting slice are decompressed instead of the entire array.&lt;/p&gt;
&lt;p&gt;In addition, Caterva introduces a new level of partitioning.  Within each chunk, the data is re-partitioned into smaller multidimensional sets called blocks.  This generally improves the slice extraction, since this allows to decompress only the blocks containing the data in desired slice instead of the whole chunks.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="slice-extraction-with-caterva-hdf5-and-zarr"&gt;
&lt;h2&gt;Slice extraction with Caterva, HDF5 and Zarr&lt;/h2&gt;
&lt;p&gt;So as to see how the double partitioning performs with respect to a traditional single partition schema, we are going to compare the ability to extract multidimensional slices from compressed data of Caterva, HDF5 and Zarr. The examples below consist on extracting some hyper-planes from chunked arrays with different properties and seeing how Caterva performs compared with traditional libraries.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; So as to better compare apples with apples, all the benchmarks below have been run using Blosc (with LZ4 as the internal codec) as the compressor by default, with the shuffle filter.  Even if Caterva uses the newest C-Blosc2 compressor, and HDF5 and Zarr uses its C-Blosc(1) antecessor, the performance of both libraries are very similar.  Also, for easier interactivity, we have used the libraries via Python wrappers (&lt;a class="reference external" href="https://python-caterva.readthedocs.io/en/latest/"&gt;python-caterva&lt;/a&gt;, &lt;a class="reference external" href="http://www.h5py.org"&gt;h5py&lt;/a&gt;, &lt;a class="reference external" href="https://zarr.readthedocs.io/en/stable/"&gt;Zarr&lt;/a&gt;).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="dimensional-array"&gt;
&lt;h2&gt;2-dimensional array&lt;/h2&gt;
&lt;p&gt;This is a 2-dimensional array and has the following properties, designed to optimize slice extraction from the second dimension:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code console"&gt;&lt;a id="rest_code_3bd0b86f38bf4af4818138bd85a4784a-1" name="rest_code_3bd0b86f38bf4af4818138bd85a4784a-1" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_3bd0b86f38bf4af4818138bd85a4784a-1"&gt;&lt;/a&gt;&lt;span class="go"&gt;shape = (8_000, 8_000)&lt;/span&gt;
&lt;a id="rest_code_3bd0b86f38bf4af4818138bd85a4784a-2" name="rest_code_3bd0b86f38bf4af4818138bd85a4784a-2" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_3bd0b86f38bf4af4818138bd85a4784a-2"&gt;&lt;/a&gt;&lt;span class="go"&gt;chunkshape = (4_000, 100)&lt;/span&gt;
&lt;a id="rest_code_3bd0b86f38bf4af4818138bd85a4784a-3" name="rest_code_3bd0b86f38bf4af4818138bd85a4784a-3" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_3bd0b86f38bf4af4818138bd85a4784a-3"&gt;&lt;/a&gt;&lt;span class="go"&gt;blockshape = (500, 25)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here we can see that the ratio between chunkshape and blockshape is 8x in dimension 0 and 4x in dimension 1.&lt;/p&gt;
&lt;img alt="/images/cat_slicing/dim0.png" class="align-center" src="http://blosc.org/images/cat_slicing/dim0.png" style="width: 70%;"&gt;
&lt;img alt="/images/cat_slicing/dim1.png" class="align-center" src="http://blosc.org/images/cat_slicing/dim1.png" style="width: 70%;"&gt;
&lt;p&gt;Now we are going to extract some planes from the chunked arrays and will plot the performance. For dimension 0 we extract a hyperplane &lt;cite&gt;[i, :]&lt;/cite&gt;, and for dimension 1, &lt;cite&gt;[:, i]&lt;/cite&gt;, where &lt;em&gt;i&lt;/em&gt; is a random integer.&lt;/p&gt;
&lt;img alt="/images/cat_slicing/2dim.png" class="align-center" src="http://blosc.org/images/cat_slicing/2dim.png" style="width: 80%;"&gt;
&lt;p&gt;Here we see that the slicing times are similar in the dimension 1. However, Caterva performs better in the dimension 0. This is because with double partitioning you only have to decompress the blocks containing the slice instead of the whole chunk.&lt;/p&gt;
&lt;p&gt;In fact, Caterva is around 12x faster than HDF5 and 9x faster than Zarr for slicing the dimension 0, which makes sense since Caterva decompresses 8x less data.
For the dimension 1, Caterva is approximately 3x faster than HDF5 and Zarr; in this case Caterva has to decompress 4x less data.&lt;/p&gt;
&lt;p&gt;That is, the difference in slice extraction speed depends largely on the ratio between the chunk size and the block size. Therefore, for slices where the chunks that contain the slice also have many items that do not belong to it, the existence of blocks (i.e. the second partition) allows to significantly reduce the amount of data to decompress.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="overhead-of-the-second-partition"&gt;
&lt;h2&gt;Overhead of the second partition&lt;/h2&gt;
&lt;p&gt;So as to better assess the possible performance cost of the second partition, let's analyze a new case of a 3-dimensional array with the following parameters:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code console"&gt;&lt;a id="rest_code_23d21b27e2ac4c50ae5f120adaeb31da-1" name="rest_code_23d21b27e2ac4c50ae5f120adaeb31da-1" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_23d21b27e2ac4c50ae5f120adaeb31da-1"&gt;&lt;/a&gt;&lt;span class="go"&gt;shape = (800, 600, 300)&lt;/span&gt;
&lt;a id="rest_code_23d21b27e2ac4c50ae5f120adaeb31da-2" name="rest_code_23d21b27e2ac4c50ae5f120adaeb31da-2" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_23d21b27e2ac4c50ae5f120adaeb31da-2"&gt;&lt;/a&gt;&lt;span class="go"&gt;chunkshape = (200, 100, 80)&lt;/span&gt;
&lt;a id="rest_code_23d21b27e2ac4c50ae5f120adaeb31da-3" name="rest_code_23d21b27e2ac4c50ae5f120adaeb31da-3" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_23d21b27e2ac4c50ae5f120adaeb31da-3"&gt;&lt;/a&gt;&lt;span class="go"&gt;blockshape = (20, 100, 10)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, in the dimensions 0 and 2 the difference between shape and chunkshape is not too big whereas the difference between chunkshape and blockshape is remarkable.&lt;/p&gt;
&lt;p&gt;However, for the dimension 1, there is not a difference at all between chunkshape and blockshape.  This means that in dim 1 the Caterva machinery will make extra work because of the double partitioning, but it will not get any advantage of it since the block size is going to be equal to the chunk size.  This a perfect scenario for measuring the overhead of the second partition.&lt;/p&gt;
&lt;p&gt;The slices to extract will be &lt;cite&gt;[i, :, :]&lt;/cite&gt;, &lt;cite&gt;[:, i, :]&lt;/cite&gt; or &lt;cite&gt;[:, :, i]&lt;/cite&gt;. Let's see the execution times for slicing these planes:&lt;/p&gt;
&lt;img alt="/images/cat_slicing/3dim.png" class="align-center" src="http://blosc.org/images/cat_slicing/3dim.png" style="width: 80%;"&gt;
&lt;p&gt;As we can see, the performance in dim 1 is around the same order than HDF5 and Zarr (Zarr being a bit faster actually), but difference is not large, so that means that the overhead introduced purely by the second partition is not that important.
However, in the other dimensions Caterva still outperforms (by far) Zarr and HDF5.  This is because the two level partitioning works as intended here.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="a-last-hyper-slicing-example"&gt;
&lt;h2&gt;A last hyper-slicing example&lt;/h2&gt;
&lt;p&gt;Let's see a final example showing the double partitioning working on a wide range of dimensions.  In this case we choose a 4-dimensional array with the following parameters:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code console"&gt;&lt;a id="rest_code_781ca00f6b984210950619f5937c2db6-1" name="rest_code_781ca00f6b984210950619f5937c2db6-1" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_781ca00f6b984210950619f5937c2db6-1"&gt;&lt;/a&gt;&lt;span class="go"&gt;shape = (400, 80, 100, 50)&lt;/span&gt;
&lt;a id="rest_code_781ca00f6b984210950619f5937c2db6-2" name="rest_code_781ca00f6b984210950619f5937c2db6-2" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_781ca00f6b984210950619f5937c2db6-2"&gt;&lt;/a&gt;&lt;span class="go"&gt;chunkshape = (100, 40, 10, 50)&lt;/span&gt;
&lt;a id="rest_code_781ca00f6b984210950619f5937c2db6-3" name="rest_code_781ca00f6b984210950619f5937c2db6-3" href="http://blosc.org/posts/caterva-slicing-perf/#rest_code_781ca00f6b984210950619f5937c2db6-3"&gt;&lt;/a&gt;&lt;span class="go"&gt;blockshape = (30, 5, 2, 10)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here the last dimension (3) is not optimized for getting hyper-slices, specially in containers with just single partitioning (Zarr and HDF5).  However, Caterva should still perform well in this situation because of the double partitioning.&lt;/p&gt;
&lt;p&gt;The slices we are going to extract will be &lt;cite&gt;[i, :, :, :]&lt;/cite&gt;, &lt;cite&gt;[:, i, :, :]&lt;/cite&gt;, &lt;cite&gt;[:, :, i, :]&lt;/cite&gt; or &lt;cite&gt;[:, :, :, i]&lt;/cite&gt;. Let's see the execution times for slicing these hyperplanes:&lt;/p&gt;
&lt;img alt="/images/cat_slicing/4dim.png" class="align-center" src="http://blosc.org/images/cat_slicing/4dim.png" style="width: 80%;"&gt;
&lt;p&gt;As we can see, in this case Caterva outperforms Zarr and HDF5 in all dimensions.  However, the advantage is not that important for the last dimension.  The reason is that in this last dimension Caterva has a noticeably lower ratio between its shape and blockshape than in the other dimensions.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="final-thoughts"&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;We have seen that adding a second partition is beneficial for improving slicing performance in general.  Of course, there are some situations where the overhead of the second partition can be noticeable, but the good news is that such an overhead does not get too large when compared with containers with only one level of partitioning.&lt;/p&gt;
&lt;p&gt;Finally, we can conclude that Caterva usually obtains better results due to its second partitioning, but when it shines the most is when the two levels of partitioning are well balanced among them and also with respect to the shape of the container.&lt;/p&gt;
&lt;p&gt;As always, there is no replacement for experimentation so, in case you want to try Caterva by yourself (and you should if you really care about this problem), you can use &lt;a class="reference external" href="https://github.com/Blosc/caterva-scipy21"&gt;our Caterva poster&lt;/a&gt;; it is based on a Jupyter notebook that you can adapt to your own scenarios.&lt;/p&gt;
&lt;/section&gt;</description><category>caterva slicing perf</category><guid>http://blosc.org/posts/caterva-slicing-perf/</guid><pubDate>Mon, 26 Jul 2021 04:32:20 GMT</pubDate></item></channel></rss>