<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blosc Main Blog Page  (Posts by Francesc Alted)</title><link>http://blosc.org/</link><description></description><atom:link href="http://blosc.org/authors/francesc-alted.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2021 &lt;a href="mailto:blosc@blosc.org"&gt;The Blosc Developers&lt;/a&gt; </copyright><lastBuildDate>Mon, 10 May 2021 12:25:22 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>C-Blosc2 Ready for General Review</title><link>http://blosc.org/posts/blosc2-ready-general-review/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;On behalf of the Blosc team, we are happy to announce the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/releases/tag/v2.0.0.rc1"&gt;first C-Blosc2
release (Release Candidate 1)&lt;/a&gt;
that is meant to be reviewed by users.  As of now
we are declaring both the API and the format frozen, and we are seeking for
feedback from the community so as to better check the library and declare it
apt for its use in production.&lt;/p&gt;
&lt;section id="some-history"&gt;
&lt;h2&gt;Some history&lt;/h2&gt;
&lt;p&gt;The next generation Blosc (aka Blosc2) started back in 2015 as a way
to overcome some limitations of the Blosc compressor, mainly the limitation
of 2 GB for the size of data to be compressed.  But it turned out that I wanted
to make thinks a bit more complete, and provide a native serialization too.
During that process Google awarded my contributions to Blosc with the
&lt;a class="reference external" href="https://www.blosc.org/posts/prize-push-Blosc2/"&gt;Open Source Peer Bonus Program&lt;/a&gt; in 2017.
This award represented a big emotional push for me in
persisting in the efforts towards producing a stable release.&lt;/p&gt;
&lt;p&gt;Back in 2018, Zeeman Wang from Huawei invited me to go to their central headquarters in Shenzhen to meet
a series of developers that were trying to use compression in a series of scenarios.
During two weeks we had a series of productive meetings, and I got aware of the many
possibilities that compression is opening in industry: since making phones with
limited hardware to work faster to accelerate computations on high-end computers.
That was also a great opportunity for me to better know a millennial culture; I was
genuinely interested to see how people live, eat and socialize in China.&lt;/p&gt;
&lt;p&gt;In 2020, &lt;a class="reference external" href="https://www.blosc.org/posts/blosc-donation/"&gt;Huawei graciously offered a grant to the Blosc project&lt;/a&gt; to complete the project.  Since then,
we have got donations from several other sources (like NumFOCUS, Python Software Foundation,
ESRF among them).  Lately &lt;a class="reference external" href="https://ironarray.io"&gt;ironArray&lt;/a&gt; is sponsoring
two of us (Aleix Alcacer and myself) to work partial time on Blosc related projects.&lt;/p&gt;
&lt;p&gt;Thanks to all this support, the Blosc development team has been able to grow quite a lot (we are currently 5 people in the core team) and we
have been able to work hard at producing a series of improvements in different projects under the Blosc umbrella, in particular &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;C-Blosc2&lt;/a&gt;,
&lt;a class="reference external" href="https://github.com/Blosc/python-blosc2"&gt;Python-Blosc2&lt;/a&gt;,
&lt;a class="reference external" href="https://github.com/Blosc/caterva"&gt;Caterva&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/Blosc/cat4py"&gt;cat4py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As you see, there is a lot of development going on around C-Blosc2 other than C-Blosc2 itself.  In this installment I am going to focus just on the main features that C-Blosc2 is bringing, but hopefully all the other projects in the ecosystem will also complement its existing functionality.  When all these projects would be ready, we hope that users will be able to use them to store big amounts of data in a way that is both efficient, easy-to-use and most importantly, adapted to their needs.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="new-features-of-c-blosc2"&gt;
&lt;h2&gt;New features of C-Blosc2&lt;/h2&gt;
&lt;p&gt;Here it is the list of the main features that we are releasing today:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;64-bit containers:&lt;/strong&gt; the first-class container in C-Blosc2 is the &lt;cite&gt;super-chunk&lt;/cite&gt; or, for brevity, &lt;cite&gt;schunk&lt;/cite&gt;, that is made by smaller chunks which are essentially C-Blosc1 32-bit containers.  The super-chunk can be backed or not by another container which is called a &lt;cite&gt;frame&lt;/cite&gt; (see later).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;More filters:&lt;/strong&gt; besides &lt;cite&gt;shuffle&lt;/cite&gt; and &lt;cite&gt;bitshuffle&lt;/cite&gt; already present in C-Blosc1, C-Blosc2 already implements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;cite&gt;delta&lt;/cite&gt;: the stored blocks inside a chunk are diff'ed with respect to first block in the chunk.  The idea is that, in some situations, the diff will have more zeros than the original data, leading to better compression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;cite&gt;trunc_prec&lt;/cite&gt;: it zeroes the least significant bits of the mantissa of float32 and float64 types.  When combined with the &lt;cite&gt;shuffle&lt;/cite&gt; or &lt;cite&gt;bitshuffle&lt;/cite&gt; filter, this leads to more contiguous zeros, which are compressed better.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A filter pipeline:&lt;/strong&gt; the different filters can be pipelined so that the output of one can the input for the other.  A possible example is a &lt;cite&gt;delta&lt;/cite&gt; followed by &lt;cite&gt;shuffle&lt;/cite&gt;, or as described above, &lt;cite&gt;trunc_prec&lt;/cite&gt; followed by &lt;cite&gt;bitshuffle&lt;/cite&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prefilters:&lt;/strong&gt; allows to apply user-defined C callbacks &lt;strong&gt;prior&lt;/strong&gt; the filter pipeline during compression.  See &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/tests/test_prefilter.c"&gt;test_prefilter.c&lt;/a&gt; for an example of use.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Postfilters:&lt;/strong&gt; allows to apply user-defined C callbacks &lt;strong&gt;after&lt;/strong&gt; the filter pipeline during decompression. The combination of prefilters and postfilters could be interesting for supporting e.g. encryption (via prefilters) and decryption (via postfilters).  Also, a postfilter alone can used to produce on-the-flight computation based on existing data (or other metadata, like e.g. coordinates). See &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/tests/test_postfilter.c"&gt;test_postfilter.c&lt;/a&gt; for an example of use.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SIMD support for ARM (NEON):&lt;/strong&gt; this allows for faster operation on ARM architectures.  Only &lt;cite&gt;shuffle&lt;/cite&gt; is supported right now, but the idea is to implement &lt;cite&gt;bitshuffle&lt;/cite&gt; for NEON too.  Thanks to Lucian Marc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SIMD support for PowerPC (ALTIVEC):&lt;/strong&gt; this allows for faster operation on PowerPC architectures.  Both &lt;cite&gt;shuffle&lt;/cite&gt;  and &lt;cite&gt;bitshuffle&lt;/cite&gt; are supported; however, this has been done via a transparent mapping from SSE2 into ALTIVEC emulation in GCC 8, so performance could be better (but still, it is already a nice improvement over native C code; see PR &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/pull/59"&gt;https://github.com/Blosc/c-blosc2/pull/59&lt;/a&gt; for details).  Thanks to Jerome Kieffer and &lt;a class="reference external" href="https://www.esrf.fr"&gt;ESRF&lt;/a&gt; for sponsoring the Blosc team in helping him in this task.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dictionaries:&lt;/strong&gt; when a block is going to be compressed, C-Blosc2 can use a previously made dictionary (stored in the header of the super-chunk) for compressing all the blocks that are part of the chunks.  This usually improves the compression ratio, as well as the decompression speed, at the expense of a (small) overhead in compression speed.  Currently, it is only supported in the &lt;cite&gt;zstd&lt;/cite&gt; codec, but would be nice to extend it to &lt;cite&gt;lz4&lt;/cite&gt; and &lt;cite&gt;blosclz&lt;/cite&gt; at least.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Contiguous frames:&lt;/strong&gt; allow to store super-chunks contiguously, either on-disk or in-memory.  When a super-chunk is backed by a frame, instead of storing all the chunks sparsely in-memory, they are serialized inside the frame container.  The frame can be stored on-disk too, meaning that persistence of super-chunks is supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sparse frames (on-disk):&lt;/strong&gt; each chunk in a super-chunk is stored in a separate file, as well as the metadata.  This is the counterpart of in-memory super-chunk, and allows for more efficient updates than in frames (i.e. avoiding 'holes' in monolithic files).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Partial chunk reads:&lt;/strong&gt; there is support for reading just part of chunks, so avoiding to read the whole thing and then discard the unnecessary data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Parallel chunk reads:&lt;/strong&gt; when several blocks of a chunk are to be read, this is done in parallel by the decompressing machinery.  That means that every thread is responsible to read, post-filter and decompress a block by itself, leading to an efficient overlap of I/O and CPU usage that optimizes reads to a maximum.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Meta-layers:&lt;/strong&gt; optionally, the user can add meta-data for different uses and in different layers.  For example, one may think on providing a meta-layer for &lt;a class="reference external" href="http://www.numpy.org"&gt;NumPy&lt;/a&gt; so that most of the meta-data for it is stored in a meta-layer; then, one can place another meta-layer on top of the latter for adding more high-level info if desired (e.g. geo-spatial, meteorological...).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Variable length meta-layers:&lt;/strong&gt; the user may want to add variable-length meta information that can be potentially very large (up to 2 GB). The regular meta-layer described above is very quick to read, but meant to store fixed-length and relatively small meta information.  Variable length metalayers are stored in the trailer of a frame, whereas regular meta-layers are in the header.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Efficient support for special values:&lt;/strong&gt; large sequences of repeated values can be represented with an efficient, simple and fast run-length representation, without the need to use regular codecs.  With that, chunks or super-chunks with values that are the same (zeros, NaNs or any value in general) can be built in constant time, regardless of the size.  This can be useful in situations where a lot of zeros (or NaNs) need to be stored (e.g. sparse matrices).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Nice markup for documentation:&lt;/strong&gt; we are currently using a combination of Sphinx + Doxygen + Breathe for documenting the C-API.  See &lt;a class="reference external" href="https://c-blosc2.readthedocs.io"&gt;https://c-blosc2.readthedocs.io&lt;/a&gt;.  Thanks to Alberto Sabater and Aleix Alcacer for contributing the support for this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Plugin capabilities for filters and codecs:&lt;/strong&gt; we have a plugin register capability inplace so that the info about the new filters and codecs can be persisted and transmitted to different machines.  Thanks to the NumFOCUS foundation for providing a grant for doing this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pluggable tuning capabilities:&lt;/strong&gt; this will allow users with different needs to define an interface so as to better tune different parameters like the codec, the compression level, the filters to use, the blocksize or the shuffle size.  Thanks to ironArray for sponsoring us in doing this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Support for I/O plugins:&lt;/strong&gt; so that users can extend the I/O capabilities beyond the current filesystem support.  Things like use databases or S3 interfaces should be possible by implementing these interfaces.  Thanks to ironArray for sponsoring us in doing this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python wrapper:&lt;/strong&gt;  we have a preliminary wrapper in the works.  You can have a look at our ongoing efforts in the &lt;a class="reference external" href="https://github.com/Blosc/python-blosc2"&gt;python-blosc2 repo&lt;/a&gt;.  Thanks to the Python Software Foundation for providing a grant for doing this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Security:&lt;/strong&gt; we are actively using using the &lt;a class="reference external" href="https://github.com/google/oss-fuzz"&gt;OSS-Fuzz&lt;/a&gt; and &lt;a class="reference external" href="https://oss-fuzz.com"&gt;ClusterFuzz&lt;/a&gt; for uncovering programming errors in C-Blosc2.  Thanks to Google for sponsoring us in doing this.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you see, the list is long and hopefully you will find compelling enough features for your own needs.  Blosc2 is not only about speed, but also about
providing&lt;/p&gt;
&lt;/section&gt;
&lt;section id="tasks-to-be-done"&gt;
&lt;h2&gt;Tasks to be done&lt;/h2&gt;
&lt;p&gt;Even if the list of features above is long, we still have things to do in Blosc2; and the plan is to continue the development, although always respecting the existing API and format.  Here are some of the things in our TODO list:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Centralized plugin repository:&lt;/strong&gt; we have got a grant from NumFOCUS for implementing a centralized repository so that people can send their plugins (using the existing machinery) to the Blosc2 team.  If the plugins fulfill a series of requirements, they will be officially accepted, and distributed withing the library.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Improve the safety of the library:&lt;/strong&gt;  although this is always a work in progress, we did a long way in improving our safety, mainly thanks to the efforts of Nathan Moinvaziri.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Support for lossy compression codecs:&lt;/strong&gt; although we already support the &lt;cite&gt;trunc_prec&lt;/cite&gt; filter, this is only valid for floating point data; we should come with lossy codecs that are meant for any data type.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Checksums:&lt;/strong&gt; the frame can benefit from having a checksum per every chunk/index/metalayer.  This will provide more safety towards frames that are damaged for whatever reason.  Also, this would provide better feedback when trying to determine the parts of the frame that are corrupted.  Candidates for checksums can be the xxhash32 or xxhash64, depending on the goals (to be decided).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; utterly important for attracting new users and making the life easier for existing ones.  Important points to have in mind here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Quality of API docstrings:&lt;/strong&gt; is the mission of the functions or data structures clearly and succinctly explained? Are all the parameters explained?  Is the return value explained?  What are the possible errors that can be returned?.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tutorials/book:&lt;/strong&gt; besides the API docstrings, more documentation materials should be provided, like tutorials or a book about Blosc (or at least, the beginnings of it).  Due to its adoption in GitHub and Jupyter notebooks, one of the most extended and useful markup systems is Markdown, so this should also be the first candidate to use here.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lock support for super-chunks:&lt;/strong&gt; when different processes are accessing concurrently to super-chunks, make them to sync properly by using locks, either on-disk (frame-backed super-chunks), or in-memory. Such a lock support would be configured in build time, so it could be disabled with a cmake flag.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It would be nice that, in case some of this feature (or a new one) sounds useful for you, you can help us in providing either code or sponsorship.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Since 2015, it has been a long time to get C-Blosc2 so much featured and tested.
But hopefully the journey will continue because as &lt;a class="reference external" href="https://www.poetryfoundation.org/poems/51296/ithaka-56d22eef917ec"&gt;Kavafis said&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;As you set out for Ithaka
hope your road is a long one,
full of adventure, full of discovery.&lt;/pre&gt;
&lt;p&gt;Let me thank again all the people and sponsors that we have had during the life of the Blosc project; without them we would not be where we are now.  We do hope that C-Blosc2 will have a long life and we as a team will put our soul in making that trip to last as long as possible.&lt;/p&gt;
&lt;p&gt;Now is your turn.  We expect you to start testing the library as much as possible and report back.  With your help we can get C-Blosc2 in production stage hopefully very soon.  Thanks in advance!&lt;/p&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>blosc2 release candidate</category><guid>http://blosc.org/posts/blosc2-ready-general-review/</guid><pubDate>Thu, 06 May 2021 10:32:20 GMT</pubDate></item><item><title>Mid 2020 Progress Report</title><link>http://blosc.org/posts/mid-2020-progress-report/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;2020 has been a year where the Blosc projects have received important donations, totalling an amount of $55,000 USD so far.  In the present report we list the most important tasks that have been carried out during the period that goes from January 2020 to August 2020.  Most of these tasks are related to the most fast-paced projects under development: C-Blosc2 and Caterva (including its cat4py wrapper).  Having said that, the Blosc development team has been active in other projects too (C-Blosc, python-blosc), although mainly for maintenance purposes.&lt;/p&gt;
&lt;p&gt;Besides, we also list the roadmap for the C-Blosc2, Caterva and cat4py projects that we plan to tackle during the next few months.&lt;/p&gt;
&lt;section id="c-blosc2"&gt;
&lt;h2&gt;C-Blosc2&lt;/h2&gt;
&lt;p&gt;C-Blosc2 adds new data containers, called superchunks, that are essentially a set of compressed chunks in memory that can be accessed randomly and enlarged during its lifetime.  Also, a new frame serialization layer has been added, so that superchunks can be persisted on disk, while keeping the same properties of superchunks in memory.  Finally, a metalayer capability allow for higher level containers to be created on top of superchunks/frames.&lt;/p&gt;
&lt;section id="highligths"&gt;
&lt;h3&gt;Highligths&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Maskout functionality.  This allows for selectively choose the blocks of a chunk that are going to be decompressed.  This paves the road for faster multidimensional slicing in Caterva (see below in the Caterva section).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prefilters introduced and declared stable.  Prefilters allow for the user to pass C functions for performing arbitrary computations on a chunk prior to the filter/codec pipeline.  In addition, the C function can even have access to more chunks than just the one that is being compressed.  This opens the door to a way to operate with different super-chunks and produce a new one very efficiently. See &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/tests/test_prefilter.c"&gt;https://github.com/Blosc/c-blosc2/blob/master/tests/test_prefilter.c&lt;/a&gt; for some examples of use.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for PowerPC/Altivec.  We added support for PowerPC SIMD (Altivec/VSX) instructions for faster operation of shuffle and bitshuffle filters.  For details, see &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/pull/98"&gt;https://github.com/Blosc/c-blosc2/pull/98&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improvements in compression ratio for LZ4/BloscLZ.  New processors are continually increasing the amount of memory in their caches.  In recent C-Blosc and C-Blosc2 releases we increased the size of the internal blocks so that LZ4/BloscLZ codecs have better opportunities for finding duplicates and hence, increasing their compression ratios.  But due to the increased cache sizes, performance has kept close to the original, fast speeds.  For some benchmarks, see &lt;a class="reference external" href="https://blosc.org/posts/beast-release/"&gt;https://blosc.org/posts/beast-release/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;New entropy probing method for BloscLZ.  BloscLZ is a native codec for Blosc whose mission is to be able to compress synthetic data efficiently.  Synthetic data can appear in multiple situations and having a codec that is meant to compress/decompress that with high compression ratios in a fast manner is important.  The new entropy probing method included in recent BloscLZ 2.3 (introduced in both C-Blosc and C-Blosc2) allows for even better compression ratios for highly compressible data, while giving up early when blocks are going to be difficult to compress at all.  For details see: &lt;a class="reference external" href="https://blosc.org/posts/beast-release/"&gt;https://blosc.org/posts/beast-release/&lt;/a&gt; too.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="roadmap-for-c-blosc2"&gt;
&lt;h3&gt;Roadmap for C-Blosc2&lt;/h3&gt;
&lt;p&gt;During the next few months, we plan to tackle the next tasks:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Postfilters.  The same way that prefilters allows to do user-defined computations prior to the compression pipeline, the postfilter would allow to do the same &lt;em&gt;after&lt;/em&gt; the decompression pipeline.  This could be useful in e.g. creating superchunks out of functions taking simple data as input (for example, a [min, max] range of values).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finalize the frame implementation.  Although the frame specification is almost complete (bar small modifications/additions), we still miss some features that are included in the specification, but not implemented yet.  An example of this is the fingerprint support at the end of the frames.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chunk insertion.  Right now only chunk appends are supported.  It should be possible to support chunk insertion in any position, and not only at the end of a superchunk.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Security.  Although we already started actions to improve the safety of the package using tools like OSS-Fuzz, this is an always work in progress task, and we plan indeed continuing improving it in the future.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wheels.  We would like to deliver wheels on every release soon.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="caterva-cat4py"&gt;
&lt;h2&gt;Caterva/cat4py&lt;/h2&gt;
&lt;p&gt;Caterva is a multidimensional container on top of C-Blosc2 containers.  It uses the metalayer capabilities present in superchunks/frames in order to store the multidimensionality information necessary to define arrays up to 8 dimensions and up to 2^63 elements.  Besides being able to create such arrays, Caterva provides functionality to get (multidimensional) slices of the arrays easyly and efficiently.  cat4py is the Python wrapper for Caterva.&lt;/p&gt;
&lt;section id="id1"&gt;
&lt;h3&gt;Highligths&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Multidimensional blocks.  Chunks inside superchunk containers are endowed with a multidimensional structure so as to enable efficient slicing.  However, in many cases there is a tension between defining large chunks so as to reduce the amount of indexing to find chunks or smaller ones in order to avoid reading data that falls outside of a slice.  In order to reduce such a tension, we endowed the blocks inside chunks with a multidimensional structure too, so that the user has two parameters (chunkshape and blockshape) to play with in order to optimize I/O for their use case.  For an example of the kind of performance enhancements you can expect, see &lt;a class="reference external" href="https://htmlpreview.github.io/?https://github.com/Blosc/cat4py/blob/269270695d7f6e27e6796541709e98e2f67434fd/notebooks/slicing-performance.html"&gt;https://htmlpreview.github.io/?https://github.com/Blosc/cat4py/blob/269270695d7f6e27e6796541709e98e2f67434fd/notebooks/slicing-performance.html&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;API refactoring.  Caterva is a relatively young project, and its API grew up organically and hence, in a quite disorganized manner.  We recognized that and proceeded with a big API refactoring, trying to put more sense in the naming schema of the functions, as well as in providing a minimal set of C structs that allows for a simpler and better API.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improved documentation.  A nice API is useless if it is not well documented, so we decided to put a significant amount of effort in creating high-quality documentation and examples so that the user can quickly figure out how to create and access Caterva containers with their own data.  Although this is still a work in progress, we are pretty happy with how docs are shaping up.  See &lt;a class="reference external" href="https://caterva.readthedocs.io/"&gt;https://caterva.readthedocs.io/&lt;/a&gt; and &lt;a class="reference external" href="https://cat4py.readthedocs.io/"&gt;https://cat4py.readthedocs.io/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Better Python integration (cat4py).  Python, specially thanks to the NumPy project, is a major player in handling multidimensional datasets, so have greatly bettered the integration of cat4py, our Python wrapper for Caterva, with NumPy.  In particular, we implemented support for the NumPy array protocol in cat4py containers, as well as an improved NumPy-esque API in cat4py package.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="roadmap-for-caterva-cat4py"&gt;
&lt;h3&gt;Roadmap for Caterva / cat4py&lt;/h3&gt;
&lt;p&gt;During the next months, we plan to tackle the next tasks:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Append chunks in any order. This will make it easier for the user to create arrays, since they will not be forced to use a row-wise order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update array elements. With this, users will be able to update their arrays without having to make a copy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Resize array dimensions. This feature will allow Caterva to increase or decrease in size any dimension of the arrays.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wheels.  Once Caterva/cat4py would be in beta stage, we plan to deliver wheels on every release.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="final-thoughts"&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;We are very grateful to our sponsors in 2020; they allowed us to implement what we think would be nice features for the whole Blosc ecosystem.  However, and although we did a lot of progress towards making C-Blosc2 and Caterva as featured and stable as possible, we still need to finalize our efforts so as to see both projects stable enough to allow them to be used in production.  Our expectation is to release a 2.0.0 (final) release for C-Blosc2 by the end of the year, whereas Caterva (and cat4py) should be declared stable during 2021.&lt;/p&gt;
&lt;p&gt;Also, we are happy to have enrolled new members on Blosc crew: Ãscar GriÃ±Ã³n, who proved to be instrumental in implementing the multidimensional blocks in Caterva and Nathan Moinvaziri, who is making great strides in making C-Blosc and C-Blosc2 more secure.  Thanks guys!&lt;/p&gt;
&lt;p&gt;Hopefully 2021 will also be a good year for seeing the Blosc ecosystem to evolve.  If you are interested on what we are building and want to help, we are open to any kind of contribution, including &lt;a class="reference external" href="https://blosc.org/pages/donate/"&gt;donations&lt;/a&gt;.  Thank you for your interest!&lt;/p&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>blosc progress report grants</category><guid>http://blosc.org/posts/mid-2020-progress-report/</guid><pubDate>Thu, 27 Aug 2020 12:32:20 GMT</pubDate></item><item><title>C-Blosc Beast Release</title><link>http://blosc.org/posts/beast-release/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;TL;DR;&lt;/strong&gt; The improvements in new CPUs allow for more cores and (much) larger caches. Latest C-Blosc release leverages these facts so as to allow better compression ratios, while keeping the speed on par with previous releases.&lt;/p&gt;
&lt;p&gt;During the past two months we have been working hard at increasing the efficiency of Blosc for the new processors that are coming with more cores than ever before (8 can be considered quite normal, even for laptops, and 16 is not that unusual for rigs).  Furthermore, their caches are increasing beyond limits that we thought unthinkable just a few years ago (for example, AMD is putting 64 MB in L3 for their mid-range Ryzen2 39x0 processors).  This is mainly a consequence of the recent introduction of the 7nm process for both ARM and AMD64 architectures.  It turns out that compression ratios are quite dependent on the sizes of the streams to compress, so having access to more cores and significantly larger caches, it was clear that Blosc was in a pressing need to catch-up and fine-tune its performance for such a new 'beasts'.&lt;/p&gt;
&lt;p&gt;So, the version released today (&lt;a class="reference external" href="https://github.com/Blosc/c-blosc/releases/tag/v1.20.0"&gt;C-Blosc 1.20.0&lt;/a&gt;) has been carefully fine-tuned to take the most of recent CPUs, specially for fast codecs, where even if speed is more important than compression ratio, the latter is still a very important parameter.  With that, we decided to increase the amount of every compressed stream in a block from 64 KB to 256 KB (most of CPUs nowadays have this amount of private L2 cache or even larger).   Also, it is important to allow a minimum of shared L3 cache to every thread so that they do not have to compete for resources, so a new restriction has been added so that no thread has to deal with streams larger than 1 MB (both old and modern CPUs seem to guarantee that they provide at least this amount of L3 per thread).&lt;/p&gt;
&lt;p&gt;Below you will find the net effects of this new fine-tuning of fast codecs like LZ4 and BloscLZ on our AMD 3900X box (12 physical cores, 64 MB L3).  Here we will be comparing results from C-Blosc 1.18.1 and C-Blosc 1.20.0 (we will skip the comparison against 1.19.x because this can be considered an intermediate release in our pursuit).  Spoiler: you will be seeing an important boost of compression ratios, while the high speed of LZ4 and BloscLZ codecs is largely kept.&lt;/p&gt;
&lt;p&gt;On the plots below, on the left is the performance of 1.18.1 release, whereas on the right is the performance of the new 1.20.0 release.&lt;/p&gt;
&lt;section id="effects-in-lz4"&gt;
&lt;h2&gt;Effects in LZ4&lt;/h2&gt;
&lt;p&gt;Let's start by looking at how the new fine tuning affected &lt;em&gt;compression&lt;/em&gt; performance:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 52%"&gt;
&lt;col style="width: 48%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-c-before" src="http://blosc.org/images/beast-release/ryzen12-lz4-1.18.1-c.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-c-after" src="http://blosc.org/images/beast-release/ryzen12-lz4-1.20.0-c.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Look at how much compression ratio has improved.  This is mainly a consequence of using compression streams of up to 256 KB, instead of the previous 64 KB --incidentally, this is just for this synthetic data, but it is clear that real data is going to be benefited as well; besides, synthetic data is something that frequently appears in data science (e.g. a uniformly spaced array of values).  One can also see that compression speed has not dropped in general which is great considering that we allow for much better compression ratios now.&lt;/p&gt;
&lt;p&gt;Regarding decompression we can see a similar pattern:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 52%"&gt;
&lt;col style="width: 48%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-d-before" src="http://blosc.org/images/beast-release/ryzen12-lz4-1.18.1-d.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-d-after" src="http://blosc.org/images/beast-release/ryzen12-lz4-1.20.0-d.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So the decompression speed is generally the same, even for data that can be compressed with high compression ratios.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="effects-in-blosclz"&gt;
&lt;h2&gt;Effects in BloscLZ&lt;/h2&gt;
&lt;p&gt;Now it is the turn for BloscLZ.  Similarly to LZ4, this codec is also meant for speed, but another reason for its existence is that it usually provides better compression ratios than LZ4 when using synthetic data.  In that sense, BloscLZ complements well LZ4 because the latter can be used for real data, whereas BloscLZ is usually a better bet for highly repetitive synthetic data.  In new C-Blosc we have introduced BloscLZ 2.3.0 which brings a brand new entropy detector which will disable compression early when entropy is high, allowing to selectively put CPU cycles where there are more low-hanging data compression opportunities.&lt;/p&gt;
&lt;p&gt;Here it is how performance changes for &lt;em&gt;compression&lt;/em&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 51%"&gt;
&lt;col style="width: 49%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-c-before" src="http://blosc.org/images/beast-release/ryzen12-blosclz-1.18.1-c.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-c-after" src="http://blosc.org/images/beast-release/ryzen12-blosclz-1.20.0-c.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case, the compression ratio has improved a lot too, and even if compression speed suffers a bit for small compression levels, it is still on par to the original speed for higher compression levels (compressing at more than 30 GB/s while reaching large compression ratios is a big achievement indeed).&lt;/p&gt;
&lt;p&gt;Regarding decompression we have this:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 51%"&gt;
&lt;col style="width: 49%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-d-before" src="http://blosc.org/images/beast-release/ryzen12-blosclz-1.18.1-d.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-d-after" src="http://blosc.org/images/beast-release/ryzen12-blosclz-1.20.0-d.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As usual for the new release, the decompression speed is generally the same, and performance can still exceed 80 GB/s for the whole range of compression levels.  Also noticeable is that fact that single-thread speed is pretty competitive with a regular &lt;cite&gt;memcpy()&lt;/cite&gt;.  Again, Ryzen2 architecture is showing its muscle here.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="final-thoughts"&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;Due to technological reasons, CPUs are evolving towards having more cores and larger caches.  Hence, compressors and specially Blosc, has to adapt to the new status quo.  With the new parametrization and new algorithms (early entropy detector) introduced today, we can achieve much better results.  In new Blosc you can expect a good bump in compression ratios with fast codecs (LZ4, BloscLZ) while keeping speed as good as always.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="appendix-hardware-and-software-used"&gt;
&lt;h2&gt;Appendix: Hardware and Software Used&lt;/h2&gt;
&lt;p&gt;For reference, here it is the software that has been used for this blog entry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: AMD Ryzen2 3900X, 12 physical cores, 64 MB L3, 32 GB RAM.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu 20.04&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: Clang 10.0.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;C-Blosc&lt;/strong&gt;: 1.18.1 (2020-03-29) and 1.20.0 (2020-07-25)&lt;/p&gt;
&lt;p&gt;** Enjoy Data!**&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>blosc performance tuning</category><guid>http://blosc.org/posts/beast-release/</guid><pubDate>Sat, 25 Jul 2020 14:32:20 GMT</pubDate></item><item><title>Blosc Received a $50,000 USD donation</title><link>http://blosc.org/posts/blosc-donation/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;I am happy to announce that the Blosc project recently received a donation of $50,000 USD from Huawei via NumFOCUS.  Now that we have such an important amount available, our plan is to use it in order to continue making Blosc and its ecosystem more useful for the community.  In order to do so, it is important to stress out that our priorities are going to be on the fundamentals of the stack: getting &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;C-Blosc2&lt;/a&gt; out of beta and pushing for making &lt;a class="reference external" href="https://github.com/Blosc/Caterva"&gt;Caterva&lt;/a&gt; (the multi-dimensional container on top of C-Blosc2) actually usable.&lt;/p&gt;
&lt;section id="critical-tasks-pushing-c-blosc2-and-caterva"&gt;
&lt;h2&gt;Critical Tasks: Pushing C-Blosc2 and Caterva&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;C-Blosc2&lt;/a&gt; has been kind of a laboratory that we used for testing new ideas, like new 64-bit containers, new filters, a new serialization system, the concept of pre-filters and others, for the past 5 years.  Although the fork from C-Blosc happened such a long time ago, we tried hard to keep the API backwards compatible so that C-Blosc2 can be used as a drop-in replacement of C-Blosc1 --but beware, the C-Blosc2 format will not be forward-compatible with C-Blosc1, but will be backward-compatible, that is, it will be able to read C-Blosc1 compressed chunks.&lt;/p&gt;
&lt;p&gt;On its hand, &lt;a class="reference external" href="https://github.com/Blosc/Caterva"&gt;Caterva&lt;/a&gt; is our attempt to build a multidimensional container that is tightly built on top of C-Blosc2, so leveraging its unique features.  Caterva is a C99 library (the same than C-Blosc2) that will allow an easy adoption by many different libraries that are about matrix manipulation.  The fact that it supports on-the-flight compression and persistency will open new possibilities in that the size of matrices will not be limited to the available memory anymore: data may span through available memory &lt;em&gt;or&lt;/em&gt; disk in &lt;em&gt;compressed&lt;/em&gt; state.&lt;/p&gt;
&lt;p&gt;Provided how fundamental C-Blosc2 and Caterva packages are meant to be, we think that the usefulness of the Blosc project as a whole will be largely benefited from putting most of our efforts here for the next months/years.  For this, we already established a series of priorities for working in these projects, as specified in the roadmaps below&lt;/p&gt;
&lt;/section&gt;
&lt;section id="roadmap-for-c-blosc2"&gt;
&lt;h2&gt;Roadmap for C-Blosc2&lt;/h2&gt;
&lt;p&gt;C-Blosc2 is already in beta stage, and in the next few months we should see it in production stage.  Here are some of the more important the things that we want to tackle in order to make this happen:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Plugin capabilities for allowing users to add more filters and codecs. There should also be a plugin register capability so that the info about the new filters and codecs can be persistent and propagated to different machines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Checksums: the frame can benefit from having a checksum per every chunk/index/metalayer. This will provide more safety towards frames that are damaged for whatever reason. Also, this would provide better feedback when trying to determine the parts of the frame that are corrupted.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Documentation: utterly important for attracting new users and making the life easier for existing ones. Important points to have in mind here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Quality of API docstrings: is the mission of the functions or data structures clearly and succinctly explained? Are all the parameters explained? Is the return value explained? What are the possible errors that can be returned?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tutorials/book: besides the API docstrings, more documentation materials should be provided, like tutorials or a book about Blosc (or at least, the beginnings of it). Due to its adoption in GitHub and Jupyter notebooks, one of the most extended and useful markup systems is MarkDown, so this should also be the first candidate to use here.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wrappers for other languages: Python and Java are the most obvious candidates, but others like R or Julia would be nice to have. Still not sure if these should be produced and maintained by the Blosc development team, or leave them for third-party players that would be interested.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed discussion see: &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/ROADMAP.md"&gt;https://github.com/Blosc/c-blosc2/blob/master/ROADMAP.md&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;section id="roadmap-for-caterva"&gt;
&lt;h2&gt;Roadmap for Caterva&lt;/h2&gt;
&lt;p&gt;Caterva is a much more young project and as such, one may say that it is still in alpha stage, although the basic functionality like creating multidimensional containers, getting items or multidimensional slices or accessing persistent data without a previous load is already there.  However, we still miss important things like:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;A complete refactorization of the Caterva C code to facilitate its usability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adapt the Python interface to the refactorization done in C code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add examples into the Python wrapper documentation and create some jupyter notebooks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build wheels to make the Python wrapper easier for the user.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implements a new level of multidimensionality in Caterva. After that, we will support three layers of multidimensionality in a Caterva container: the shape, the chunk shape and the block shape.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed discussion see: &lt;a class="reference external" href="https://github.com/Blosc/Caterva/blob/master/ROADMAP.md"&gt;https://github.com/Blosc/Caterva/blob/master/ROADMAP.md&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;section id="how-we-are-spending-resources"&gt;
&lt;h2&gt;How we are spending resources&lt;/h2&gt;
&lt;p&gt;Money is important, but not everything: you need people to work on a project.  We are slowly starting to put consistent human resources in the Blosc project.  To start with, I (Francesc Alted) and Aleix Alcacer will be putting 25% of our time in the project for the next months, and hopefully others will join too.  We will also be using funds to invest in our main tool, that is laptops and desktop computers, but also some furniture like proper seats and tables; the office space is important for creating a happy team.  Finally, our plan is to use a part of the donation in facilitating meeting among the Blosc development team.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="your-input-is-important-for-us"&gt;
&lt;h2&gt;Your input is important for us&lt;/h2&gt;
&lt;p&gt;Although during the next year or so, we plan to organize some meetings of the board of directors and the Blosc development team, we think that our ideas cannot grow isolated from the community of users.  So in case you want to convey ideas or better, contribute with &lt;em&gt;implementation&lt;/em&gt; of ideas, we will be happy to hear and discuss.  You can get in touch with us via the Blosc mailing list (&lt;a class="reference external" href="https://groups.google.com/forum/#!forum/blosc"&gt;https://groups.google.com/forum/#!forum/blosc&lt;/a&gt;), and the @Blosc2 twitter account.  We are thinking that having other tools like Discourse may help in driving discussions more to the point, but so far we have little experience with it; if you have other suggestions please tell us.&lt;/p&gt;
&lt;p&gt;All in all, the Blosc development team is very excited about this new development, and we are putting all our enthusiasm for delivering a new set of tools that we sincerely hope will of of help for the data community out there.&lt;/p&gt;
&lt;p&gt;Finally, let me thank our main sponsor for their generous donation, NumFOCUS for accepting our project inside its umbrella, and to all the users and contributors that made Blosc and its ecosystem to help people through the past years (a bit more than 10 since the first C-Blosc 1.0 release).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Enjoy Data!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>donation blosc2 caterva</category><guid>http://blosc.org/posts/blosc-donation/</guid><pubDate>Thu, 20 Feb 2020 01:32:20 GMT</pubDate></item><item><title>Blosc2-Meets-Rome</title><link>http://blosc.org/posts/blosc2-meets-rome/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;On August 7, 2019, AMD released a new generation of its series of EPYC processors, the EPYC 7002, also known as Rome, which are based on the new &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Zen_2"&gt;Zen 2&lt;/a&gt; micro-architecture.  Zen 2 is a significant departure from the physical design paradigm of AMD's previous Zen architectures, mainly in that the I/O components of the CPU are laid out on a separate die, different from computing dies; this is quite different from Naples (aka EPYC 7001), its antecessor in the EPYC series:&lt;/p&gt;
&lt;img alt="/images/blosc2-meets-rome/amd-rome-arch-multi-die.png" class="align-center" src="http://blosc.org/images/blosc2-meets-rome/amd-rome-arch-multi-die.png"&gt;
&lt;p&gt;Such a separation of dies for I/O and computing has quite &lt;a class="reference external" href="https://www.anandtech.com/show/15044/the-amd-ryzen-threadripper-3960x-and-3970x-review-24-and-32-cores-on-7nm/3"&gt;large consequences in terms of scalability when accessing memory&lt;/a&gt;, which is critical for Blosc operation, and here we want to check how Blosc and AMD Rome couple behaves.  As there is no replacement for experimentation, we are going to use the same benchmark that was introduced in our previous &lt;a class="reference external" href="https://blosc.org/posts/breaking-memory-walls/"&gt;Breaking Down Memory Walls&lt;/a&gt;.  This essentially boils down to compute an aggregation with a simple loop like:&lt;/p&gt;
&lt;pre class="code c"&gt;&lt;a id="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-1" name="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#pragma omp parallel for reduction (+:sum)&lt;/span&gt;
&lt;a id="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-2" name="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a id="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-3" name="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-3"&gt;&lt;/a&gt;  &lt;span class="n"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;udata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;a id="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-4" name="rest_code_ac5368f6f0ff4531a83ec645c1f7b00a-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;As described in the original blog post, the different &lt;cite&gt;udata&lt;/cite&gt; arrays are just chunks of the original dataset that are decompressed just in time for performing the partial aggregation operation; the final result is indeed the sum of all the partial aggregations.  Also we have seen that the time to execute the aggregation is going to depend quite a lot on the kind of data that is decompressed: carefully chosen synthetic data can be decompressed much more quickly than real data.  But syntehtic data is nevertheless interesting as it allows for a roof analysis of where the performance can grow up to.&lt;/p&gt;
&lt;p&gt;In this blog, we are going to see how the AMD EPYC 7402 (Rome), a 24-core processor performs on both synthetic and real data.&lt;/p&gt;
&lt;section id="aggregating-the-synthetic-dataset-on-amd-epyc-7402-24-core"&gt;
&lt;h2&gt;Aggregating the Synthetic Dataset on AMD EPYC 7402 24-Core&lt;/h2&gt;
&lt;p&gt;The synthetic data chosen for this benchmark allows to be compressed/decompressed very easily with applying the shuffle filter before the actual compression codec.  Interestingly, and as good example of how filters can benefit the compression process, if we would not apply the shuffle filter first, synthetic data was going to take much more time to compress/decompress (test it by yourself if you don't believe this).&lt;/p&gt;
&lt;p&gt;After some experiments, and as usual for synthetic datasets, the codec inside Blosc2 that has shown the best speed while keeping a decent compression ratio (54.6x), has been BloscLZ with compression level 3.  Here are the results:&lt;/p&gt;
&lt;img alt="/images/blosc2-meets-rome/sum_openmp_synthetic-blosclz-3.png" class="align-center" src="http://blosc.org/images/blosc2-meets-rome/sum_openmp_synthetic-blosclz-3.png"&gt;
&lt;p&gt;As we can see, the uncompressed dataset scales pretty well until 8 threads, where it hits the memory wall for this machine (around 74 GB/s).  On its hand, even if data compressed with Blosc2 (in combination with BloscLZ codec) shows less performance initially, it scales quite smoothly up to 12 threads, where it reaches a higher performance than its uncompressed counterpart (and reaching the 90 GB/s mark).&lt;/p&gt;
&lt;p&gt;After that, the compressed dataset can perform aggregations at speeds that are typically faster than uncompressed ones, reaching important peaks at some magical number of threads (up to 210 GB/s at 48 threads).  Why these peaks exist at all is probably related with the architecture of the AMD Rome processor, but provided that we are using a 24-core CPU there is little wonder that numbers like 12, 24 (28 is an exception here) and 48 are reaching the highest figures.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="aggregating-the-precipitation-dataset-on-amd-epyc-7402-24-core"&gt;
&lt;h2&gt;Aggregating the Precipitation Dataset on AMD EPYC 7402 24-Core&lt;/h2&gt;
&lt;p&gt;Now it is time to check the performance of the aggregation with the 100 million values dataset coming from a &lt;a class="reference external" href="http://reanalysis.meteo.uni-bonn.de/"&gt;precipitation dataset from Central Europe&lt;/a&gt;.  Computing the aggregation of this data is representative of a catchment average of precipitation over a drainage area.  This time, the best codec inside Blosc2 was determined to be LZ4 with compression level 9:&lt;/p&gt;
&lt;img alt="/images/blosc2-meets-rome/sum_openmp_rainfall-lz4-9-lz4-9-ipp.png" class="align-center" src="http://blosc.org/images/blosc2-meets-rome/sum_openmp_rainfall-lz4-9-lz4-9-ipp.png"&gt;
&lt;p&gt;As expected, the uncompressed aggregation scales pretty much the same than for the synthetic dataset (in the end, the Arithmetic and Logical Unit in the CPU is completely agnostic on what kind of data it operates with).  But on its hand, the compressed dataset scales more slowly, but more steadily towards hitting a maximum at 48 threads, where it reaches almost the same speed than the uncompressed dataset, which is quite a feat, provided the high memory bandwidth of this machine (~74 GB/s).&lt;/p&gt;
&lt;p&gt;Also, as Blosc2 recently gained support for the  &lt;a class="reference external" href="https://blosc.org/posts/blosc2-first-beta/"&gt;accelerated LZ4 codec inside Intel IPP&lt;/a&gt;, figures for it have been added to the plot above.  There one can see that Intel's accelerated LZ4 can get an up to 10% boost in speed compared with regular LZ4; this additional 10% actually allows Blosc2/LZ4 to be clearly faster than the uncompressed dataset at 48 threads.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="final-thoughts"&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;AMD EPYC Rome represents a significant leap forward in adding a high number of cores to CPUs in a way that scales really well, allowing to put more computational resources to our problems at hand.  Here we have shown how nicely a 24-core AMD Rome CPU performs when performing tasks with in-memory compressed datasets; first, by allowing competitive speed when using compression with real data and second, allowing speeds of more than 200 GB/s (with synthetic datasets).&lt;/p&gt;
&lt;p&gt;Finally, the 24-core CPU that we have exercised here is just for whetting your appetite, as CPUs of 32 or even 64 cores are going to happen more and more often in the next future.  Although I should have better said in &lt;em&gt;present times&lt;/em&gt;, as &lt;a class="reference external" href="https://www.anandtech.com/show/15044/the-amd-ryzen-threadripper-3960x-and-3970x-review-24-and-32-cores-on-7nm"&gt;AMD announced today the availability of 32-core CPUs for the workstation market&lt;/a&gt;, with 64-core ones coming next year.  Definitely, compression is going to play an increasingly important role in getting the most out of these beasts.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="appendix-software-used"&gt;
&lt;h2&gt;Appendix: Software used&lt;/h2&gt;
&lt;p&gt;For reference, here it is the software that has been used for this blog entry:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu 19.10&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: Clang 8.0.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;C-Blosc2&lt;/strong&gt;: 2.0.0b5.dev (2019-09-13)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="acknowledgments"&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;a class="reference external" href="https://www.packet.com"&gt;packet.com&lt;/a&gt; for kindly providing the hardware for the purposes of this benchmark.  Packet guys have been really collaborative through the time in allowing me testing new, bare-metal hardware, and I must say that I am quite impressed on how easy is to start using their services with almost no effort on user's side.&lt;/p&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>amd</category><category>memory wall</category><category>rome</category><guid>http://blosc.org/posts/blosc2-meets-rome/</guid><pubDate>Mon, 25 Nov 2019 18:32:20 GMT</pubDate></item><item><title>C-Blosc2 Enters Beta Stage</title><link>http://blosc.org/posts/blosc2-first-beta/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;The first beta version of C-Blosc2 has been released today.  C-Blosc2 is the new iteration of C-Blosc 1.x series, adding more features and better documentation and is the outcome of more than 4 years of slow, but steady development.  This blog entry describes the main features that you may see in next generation of C-Blosc, as well as an overview of what is in our roadmap.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: C-Blosc2 is currently in beta stage, so not ready to be used in production yet.  Having said this, being in beta means that the API has been declared frozen, so there is guarantee that your programs will continue to work with future versions of the library.  If you want to collaborate in this development, you are welcome: have a look at our roadmap below and contribute PR's or just go to the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/issues"&gt;open issues&lt;/a&gt; and help us with them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: the term &lt;cite&gt;C-Blosc1&lt;/cite&gt; will be used instead of the official &lt;cite&gt;C-Blosc&lt;/cite&gt; name for referring to the 1.x series of the library.  This is to make the distinction between the C-Blosc 2.x series and C-Blosc 1.x series more explicit.&lt;/p&gt;
&lt;section id="main-features-in-c-blosc2"&gt;
&lt;h2&gt;Main features in C-Blosc2&lt;/h2&gt;
&lt;section id="new-64-bit-containers"&gt;
&lt;h3&gt;New 64-bit containers&lt;/h3&gt;
&lt;p&gt;The main container in C-Blosc2 is the &lt;cite&gt;super-chunk&lt;/cite&gt; or, for brevity, &lt;cite&gt;schunk&lt;/cite&gt;, that is made by smaller containers which are essentially C-Blosc1 32-bit containers.  The &lt;cite&gt;super-chunk&lt;/cite&gt; can be backed (or not) by another container which is called a &lt;cite&gt;frame&lt;/cite&gt;.  If a &lt;cite&gt;schunk&lt;/cite&gt; is not backed by a &lt;cite&gt;frame&lt;/cite&gt; (the default), the different chunks will be stored sparsely in-memory.&lt;/p&gt;
&lt;p&gt;The &lt;cite&gt;frame&lt;/cite&gt; object allows to store super-chunks contiguously, either on-disk or in-memory.  When a super-chunk is backed by a frame, instead of storing all the chunks sparsely in-memory, they are serialized inside the frame container.  The frame can be stored on-disk too, meaning that persistence of super-chunks is supported and that data can be accessed using the same API independently of where it is stored, memory or disk.&lt;/p&gt;
&lt;p&gt;Finally, the user can add meta-data to frames for different uses and in different layers.  For example, one may think on providing a meta-layer for &lt;a class="reference external" href="http://www.numpy.org"&gt;NumPy&lt;/a&gt; so that most of the meta-data for it is stored in a meta-layer; then, one can place another meta-layer on top of the latter can add more high-level info (e.g. geo-spatial, meteorological...), if desired.&lt;/p&gt;
&lt;p&gt;When taken together, these features represent a pretty powerful way to store and retrieve compressed data that goes well beyond of the previous contiguous compressed buffer, 32-bit limited, of C-Blosc1.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="new-filters-and-filters-pipeline"&gt;
&lt;h3&gt;New filters and filters pipeline&lt;/h3&gt;
&lt;p&gt;Besides &lt;cite&gt;shuffle&lt;/cite&gt; and &lt;cite&gt;bitshuffle&lt;/cite&gt; already present in C-Blosc1, C-Blosc2 already implements:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;cite&gt;delta&lt;/cite&gt;: the stored blocks inside a chunk are diff'ed with respect to first block in the chunk.  The basic idea here is that, in some situations, the diff will have more zeros than the original data, leading to better compression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;cite&gt;trunc_prec&lt;/cite&gt;: it zeroes the least significant bits of the mantissa of float32 and float64 types.  When combined with the &lt;cite&gt;shuffle&lt;/cite&gt; or &lt;cite&gt;bitshuffle&lt;/cite&gt; filter, this leads to more contiguous zeros, which are compressed better and faster.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, a new filter pipeline has been implemented.  With it, the different filters can be pipelined so that the output of one filter can be the input for the next; this happens at the block level, so minimizing the size of temporary buffers, and hence, accelerating the process.  Possible examples of pipelines are a &lt;cite&gt;delta&lt;/cite&gt; filter followed by &lt;cite&gt;shuffle&lt;/cite&gt;, or a &lt;cite&gt;trunc_prec&lt;/cite&gt; followed by &lt;cite&gt;bitshuffle&lt;/cite&gt;.  Up to 6 filters can be pipelined, so there is plenty of space for upcoming new filters to collaborate among them.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="more-simd-support-for-arm-and-powerpc"&gt;
&lt;h3&gt;More SIMD support for ARM and PowerPC&lt;/h3&gt;
&lt;p&gt;New SIMD support for ARM (NEON), allowing for faster operation on ARM architectures.  Only &lt;cite&gt;shuffle&lt;/cite&gt; is supported right now, but the idea is to implement &lt;cite&gt;bitshuffle&lt;/cite&gt; for NEON too.&lt;/p&gt;
&lt;p&gt;Also, SIMD support for PowerPC (ALTIVEC) is here, and both &lt;cite&gt;shuffle&lt;/cite&gt;  and &lt;cite&gt;bitshuffle&lt;/cite&gt; are supported.  However, this has been done via a transparent mapping from SSE2 into ALTIVEC emulation in GCC 8, so performance could be better (but still, it is already a nice improvement over native C code; see PR &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/pull/59"&gt;https://github.com/Blosc/c-blosc2/pull/59&lt;/a&gt; for details).  Thanks to Jerome Kieffer.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="new-codecs"&gt;
&lt;h3&gt;New codecs&lt;/h3&gt;
&lt;p&gt;There is a new &lt;a class="reference external" href="https://github.com/inikep/lizard"&gt;Lizard codec&lt;/a&gt;, which is an efficient compressor with very fast decompression. It achieves compression ratio that is comparable to &lt;cite&gt;zip/zlib&lt;/cite&gt; and &lt;cite&gt;zstd/brotli&lt;/cite&gt; (at low and medium compression levels) that is able to attain decompression speeds of 1 GB/s or more.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="new-dictionary-support-for-better-compression-ratio"&gt;
&lt;h3&gt;New dictionary support for better compression ratio&lt;/h3&gt;
&lt;p&gt;Dictionaries allow for better discovery of data duplicates among different blocks: when a block is going to be compressed, C-Blosc2 can use a previously made dictionary (stored in the header of the super-chunk) for compressing all the blocks that are part of the chunks.  This usually improves the compression ratio, as well as the decompression speed, at the expense of a (small) overhead in compression speed.  Currently, this is only supported in the &lt;cite&gt;zstd&lt;/cite&gt; codec, but would be nice to extend it to &lt;cite&gt;lz4&lt;/cite&gt; and &lt;cite&gt;blosclz&lt;/cite&gt; at least.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="much-improved-documentation-mark-up"&gt;
&lt;h3&gt;Much improved documentation mark-up&lt;/h3&gt;
&lt;p&gt;We are currently using a combination of Sphinx + Doxygen + Breathe for documenting the &lt;a class="reference external" href="https://blosc-doc.readthedocs.io"&gt;C API for C-Blosc2&lt;/a&gt;.  This is a huge step further compared with the documentation of C-Blosc1, where the developer needed to go the    &lt;a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/blosc/blosc.h"&gt;blosc.h&lt;/a&gt; header for reading the docstrings there.  Thanks to Alberto Sabater for contributing the support for this.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="support-for-intel-ipp-integrated-performance-primitives"&gt;
&lt;h3&gt;Support for Intel IPP (Integrated Performance Primitives)&lt;/h3&gt;
&lt;p&gt;Intel is producing a series of optimizations in their &lt;a class="reference external" href="https://software.intel.com/en-us/ipp"&gt;IPP library&lt;/a&gt; and among them, and &lt;a class="reference external" href="https://software.intel.com/en-us/ipp-dev-reference-lz4-compression-functions"&gt;accelerated version of the LZ4 codec&lt;/a&gt;.  Due to its excellent compression capabilities and speed, LZ4 is probably the most used codec in Blosc, so enabling even a bit more of optimization on LZ4 is always a good news.  And judging by the plots below, the Intel guys seem to have done an excellent job:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-no-ipp" src="http://blosc.org/images/blosc2-first-beta/Blosc2-4MB-LZ4-NO-IPP-Shuffle.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-ipp" src="http://blosc.org/images/blosc2-first-beta/Blosc2-4MB-LZ4-IPP-Shuffle.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the plots above we see a couple of things: 1) the IPP/LZ4 functions can compress &lt;em&gt;more&lt;/em&gt; than regular LZ4, and 2) they are quite a bit &lt;em&gt;faster&lt;/em&gt; than regular LZ4.  As always, take these plots with a grain of salt, as actual datasets will see more similar compression ratios and speed (but still, the difference can be significant).  Of course, IPP/LZ4 should generate LZ4 chunks that are completely compatible with the original LZ4 library (but in case you detect any incompatibility, please shout!).&lt;/p&gt;
&lt;p&gt;C-Blosc2 beta.1 comes with support for LZ4/IPP out-of-the-box, that is, if IPP is detected in the system, its optimized LZ4 functions are automatically linked and used with the Blosc2 library.  If, for portability or other reasons, you don't want to create a Blosc2 library that is linked with Intel IPP, you can disable support for it passing the &lt;cite&gt;-DDEACTIVATE_IPP=ON&lt;/cite&gt; to cmake.  In the future, we surely may give support for other optimized codecs in IPP too (Zstd would be an excellent candidate).&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="roadmap"&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;Of course, C-Blosc2 is not done yet, and there are many interesting enhancements that we would like to tackle sooner or later.  Here it is a more or less comprehensive list of our roadmap:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Lock support for &lt;cite&gt;super-chunks&lt;/cite&gt;: when different processes are accessing concurrently to super-chunks, make them to sync properly by using locks, either on-disk (frame-backed super-chunks), or in-memory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Checksums: the frame can benefit from having a checksum per every chunk/index/metalayer.  This will provide more safety towards frames that are damaged for whatever reason.  Also, this would provide better feedback when trying to determine the parts of the frame that are corrupted.  Candidates for checksums can be the xxhash32 or xxhash64, depending on the gaols (to be decided).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Documentation: utterly important for attracting new users and making the life easier for existing ones.  Important points to have in mind here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Quality of API docstrings: is the mission of the functions or data structures clearly and succinctly explained? Are all the parameters explained?  Is the return value explained?  What are the possible errors that can be returned?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tutorials/book: besides the API docstrings, more documentation materials should be provided, like tutorials or a book about Blosc (or at least, the beginnings of it).  Due to its adoption in GitHub and Jupyter notebooks, one of the most extended and useful markup systems is MarkDown, so this should also be the first candidate to use here.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wrappers for other languages: Python and Java are the most obvious candidates, but others like R or Julia would be nice to have.  Still not sure if these should be produced and maintained by the Blosc development team, or leave them for third-party players that would be interested.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It would be nice to use &lt;a class="reference external" href="https://lgtm.com"&gt;LGTM&lt;/a&gt;, a CI-friendly analyzer for security.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add support for &lt;a class="reference external" href="https://buildkite.com"&gt;buildkite&lt;/a&gt; as another CI would be handy because it allows to use on-premise machines, potentially speeding-up the time to do the builds, but also to setup pipelines with more complex dependencies and analyzers.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The implementation of these features will require the help of people, either by contributing code (see  &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/DEVELOPING-GUIDE.rst"&gt;our developing guidelines&lt;/a&gt;) or, as it turns out that &lt;a class="reference external" href="https://numfocus.org/project/blosc"&gt;Blosc is a project sponsored by NumFOCUS&lt;/a&gt;, you may want to  &lt;a class="reference external" href="https://numfocus.org/donate-to-blosc"&gt;make a donation to the project&lt;/a&gt;.  If you plan to contribute in any way, thanks so much in the name of the community!&lt;/p&gt;
&lt;/section&gt;
&lt;section id="addendum-special-thanks-to-developers"&gt;
&lt;h2&gt;Addendum: Special thanks to developers&lt;/h2&gt;
&lt;p&gt;C-Blosc2 is the outcome of the work of &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/graphs/contributors"&gt;many developers&lt;/a&gt; that worked not only on C-Blosc2 itself, but also on C-Blosc1, from which C-Blosc2 inherits a lot of features.  I am very grateful to Jack Pappas, who contributed important portability enhancements, specially runtime and cross-platform detection of SSE2/AVX2 (with the help of Julian Taylor) as well as high precision timers (HPET) which are essential for benchmarking purposes.  Lucian Marc also contributed the support for ARM/NEON for the shuffle filter.  Jerome Kieffer contributed support for PowerPC/ALTIVEC.  Alberto Sabater, for his great efforts on producing really nice Blosc2 docs, among other aspects. And last but not least, to Valentin Haenel for general support, bug fixes and other enhancements through the years.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;** Enjoy Data!**&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>blosc2 beta</category><guid>http://blosc.org/posts/blosc2-first-beta/</guid><pubDate>Wed, 14 Aug 2019 01:32:20 GMT</pubDate></item><item><title>Is ARM Hungry Enough to Eat Intel's Favorite Pie?</title><link>http://blosc.org/posts/arm-memory-walls-followup/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This entry is a follow-up of the &lt;a class="reference external" href="http://blosc.org/posts/breaking-memory-walls/"&gt;Breaking Down Memory Walls&lt;/a&gt; blog.  Please make sure that you have read it if you want to fully understand all the benchmarks performed here.&lt;/p&gt;
&lt;p&gt;At the beginning of the 1990s the computing world was mainly using RISC (Reduced Instruction Set Computer) architectures, namely SPARC, Alpha, Power and MIPS CPUs for performing serious calculations and Intel CPUs were seen as something that was appropriate just to run essentially personal applications on PCs, but almost nobody was thinking about them as a serious contender for server environments.  But Intel had an argument that almost nobody was ready to recognize how important it could become; with its dominance of the PC market it quickly ranked to be the largest CPU maker in the world and, with such an enormous revenue, Intel played its cards well and, by the beginning of 2000s, they were able to make of its CISC architecture (Complex Instruction Set Computer) the one with the best compute/price ratio, clearly beating the RISC offerings at that time.  That amazing achievement shut the mouths of CISC critics (to the point that nowadays almost everybody recognizes that performance has very little to do with using RISC or CISC) and cleared the path for Intel to dominate not only the PC world, but also the world of server computing for the next 20 years.&lt;/p&gt;
&lt;p&gt;Fast forward to the beginning of 2010s, with Intel clearly dominating the market of CPUs for servers.  However, at the same time something potentially disruptive happened: the market for mobile and embedded systems exploded making &lt;a class="reference external" href="https://cacm.acm.org/magazines/2011/5/107684-an-interview-with-steve-furber/fulltext"&gt;the ARM architecture the most widely used architecture in this area&lt;/a&gt;.  By 2017, with over 100 billion ARM processors produced, ARM was already the most widely used architecture in the world.  Now, the smart reader will have noted here a clear parallelism between the situation of Intel at the end of 1990s and ARM at the end of 2010s: both companies were responsible of the design of the most used CPUs in the world.  There was an important difference though: while Intel was able to implement its own designs, ARM was leaving the implementation job to third party vendors.  Of course, this fact will have consequences on the way ARM will be competing with Intel (see below).&lt;/p&gt;
&lt;section id="arm-plans-for-improving-cpu-performance"&gt;
&lt;h2&gt;ARM Plans for Improving CPU Performance&lt;/h2&gt;
&lt;p&gt;So with ARM CPUs dominating the world of mobile and embedded, the question is whether ARM would be interested in having a stab at the client market (laptops and PC desktops) and, by extension, to the server computing market during the 2020s decade or they would renounce to that because they comfortable enough with the current situation?  In 2018 ARM provided an important hint to answer this question: they really want to push hard for the client market with the &lt;a class="reference external" href="https://www.anandtech.com/show/13226/arm-unveils-client-cpu-performance-roadmap"&gt;introduction of the Cortex A76 CPU&lt;/a&gt; which aspires to redefine the capability of ARM to compete with Intel at its own game:&lt;/p&gt;
&lt;img alt="/images/arm-memory-walls-followup/arm-compute-plans.png" class="align-center" src="http://blosc.org/images/arm-memory-walls-followup/arm-compute-plans.png"&gt;
&lt;p&gt;On the other hand, the fact that ARM is not just providing licenses to use its IP cores, but also the possibility to buy an architectural licence for vendors to design their own CPU cores using the ARM instruction sets makes possible that other players like Apple, AppliedMicro, Broadcom, Cavium (now Marvell), Nvidia, Qualcomm, and Samsung Electronics can produce ARM CPUs that can be adapted to be used in different scenarios.  One example that is interesting for this discussion is Marvell who, with its ThunderX2 CPU, is already entering into the computing servers market --actually, a new super-computer with more than 100,000 ThunderX2 cores has recently entered into the &lt;a class="reference external" href="https://t.co/LM2wXQrXm8"&gt;TOP500 ranking&lt;/a&gt;; this is the first time that an ARM-based computer enters that list, overwhelmingly dominated by Intel architectures for almost two decades now.&lt;/p&gt;
&lt;p&gt;In the next sections we are trying to bring more hints (experimentally tested) on whether ARM (and its licensees) are fulfilling their promise, or their claims were just bare marketing.  For checking this, I was able to use two recent (2018) implementations of the ARMv8-A architecture, one meant for the client market and the other for servers, replicated the benchmarks of my previous &lt;a class="reference external" href="http://blosc.org/posts/breaking-memory-walls/"&gt;Breaking Down Memory Walls&lt;/a&gt; blog entry and extracted some interesting results.  Keep reading.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="the-kirin-980-cpu"&gt;
&lt;h2&gt;The Kirin 980 CPU&lt;/h2&gt;
&lt;p&gt;Here we are going to analyze &lt;a class="reference external" href="https://www.anandtech.com/show/13503/the-mate-20-mate-20-pro-review"&gt;Huawei's Kirin 980 CPU&lt;/a&gt; , a SoC (System On a Chip) that uses the ARM A76 core internally.  This is a fine example of an internal IP core design of ARM that is licensed to be used in a CPU chipset (or SoC) by another vendor (Huawei in this case).  The Kirin 980 wears 4 A76 cores plus 4 A55 cores, but the more powerful ones are the A76 (the A55 are more headed to do light tasks with very little energy consumption, which is critical for phones).  The A76 core is designed to be implemented using a 7nm technology (as it is the case for the Kirin 980, the second SoC in the world to use a 7 nm node, after Apple A12), and supports ARM's DynamIQ technology which allows scalability to target the specific requirements of a SoC.  In our case the Kirin 980 is running in a phone (Humawei's Mate 20), and in this scenario the power dissipation (TDP) cannot exceed the 4 W figure, so DynamIQ should try to be very conservative here and avoid putting too many cores active at the same time.&lt;/p&gt;
&lt;p&gt;ARM is saying that they designed the &lt;a class="reference external" href="https://arstechnica.com/gadgets/2018/06/arm-promises-laptop-level-performance-in-2019/"&gt;A76 to be a competitor of the Intel Skylake Core i5&lt;/a&gt;, so this is what we are going to check here.  For this, we are going to compare a Kirin 980 in a Huawei Mate 20 phone against a Core i5 included in a MacBook Pro (late 2016).  Here it is the side-by-side performance for the precipitation dataset that I used in the &lt;a class="reference external" href="http://blosc.org/posts/breaking-memory-walls/"&gt;previous blog&lt;/a&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="rainfall-kirin980" src="http://blosc.org/images/arm-memory-walls-followup/kirin980-rainfall-lz4-9.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="rainfall-i5laptop" src="http://blosc.org/images/arm-memory-walls-followup/i5laptop-lz4-9.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here we can already see a couple of things.  First, the speed of the calculation when there is no compression is similar for both CPUs.  This is interesting because, although the bottleneck for this benchmark is in the memory access, the fact that the Kirin 980 performance is almost the same than the Core i5 is a testimony of how well ARM performed in the design of a memory prefetcher, clearly allowing for a good memory-level parallelism.&lt;/p&gt;
&lt;p&gt;Second, for the compressed case, the Core i5 is still a 50% faster than the Kirin 980, but the performance scales similarly (up to 4 threads) for both CPUs.  The big news here is that the Core i5 has a TDP of 28 W, whereas for the Kirin 980 is just 4 W (and probably less than that), so that means that ARM's DynamIQ works beautifully so as to allow 4 (powerful) cores to run simultaneously in such a restrictive scenario (remember that we are running this benchmark &lt;em&gt;inside a phone&lt;/em&gt;).  It is also true that we are comparing an Intel CPU from 2016 against an ARM CPU from 2018 and that nowadays probably we can find Intel exemplars showing a similar performance than this i5 for probably no more than 10 W (e.g. an &lt;a class="reference external" href="https://ark.intel.com/products/149088/Intel-Core-i5-8265U-Processor-6M-Cache-up-to-3-90-GHz-"&gt;i5-8265U with configurable TDP-down&lt;/a&gt;), although I am not really sure how an Intel CPU will perform with such a strict power constraint.  At any rate, the Kirin 980 still consumes less than half of the power than its Intel counterpart --and its price would probably be a fraction of it too.&lt;/p&gt;
&lt;p&gt;I believe that these facts are really a good testimony of how serious ARM was on their claim that they were going to catch Intel in the performance side of the things for client devices, and probably with an important advantage in consuming less energy too.  The fact that ARM CPUs are more energy efficient should not be surprising given the experience of ARM in that area for decades.  But another reason for that is the important reduction in the manufacturing technology that ARM has achieved on their new designs (7nm node for ARM vs 14nm node for Intel); undoubtedly, ARM advantage in power consumption is going to be important for their world-domination plans.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="the-thunderx2-cpu"&gt;
&lt;h2&gt;The ThunderX2 CPU&lt;/h2&gt;
&lt;p&gt;The second way in which ARM sells licenses is the so-called &lt;em&gt;architectural license&lt;/em&gt; allowing companies to design their own CPU cores using the ARM instruction sets.  Cavium (now bought by Marvell) was one of these companies, and they produced different CPU designs that culminated with Vulcan, the micro-architecture that powers the ThunderX2 CPU, which was made available in May 2018.  &lt;a class="reference external" href="https://en.wikichip.org/wiki/cavium/microarchitectures/vulcan"&gt;Vulcan is a 16 nm high-performance 64-bit ARM micro-architecture&lt;/a&gt; that is specifically meant to compete in compute/data server facilities (think of it as a  a Xeon-class ARM-based server microprocessor).  ThunderX2 can pack up to 32 Vulcan cores, and as every Vulcan core supports up to 4 threads, the whole CPU can run up to 128 threads.  With its capability to handle so many threads simultaneously, I expected that its raw compute power should be nothing to sneeze at.&lt;/p&gt;
&lt;p&gt;So as to check how powerful a ThunderX2 can be, we are going to compare &lt;a class="reference external" href="https://en.wikichip.org/wiki/cavium/thunderx2/cn9975"&gt;ThunderX2 CN9975&lt;/a&gt; (actually a box with 2 instances of it, each containing 28 cores) against one of its natural competitor, the Intel Scalable Gold 5120 (actually a box with 2 instances of it, each containing 14 cores):&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 51%"&gt;
&lt;col style="width: 49%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="rainfall-thunderx2" src="http://blosc.org/images/arm-memory-walls-followup/thunderx2-rainfall-lz4-9.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="rainfall-scalable" src="http://blosc.org/images/arm-memory-walls-followup/scalable-rainfall-lz4-9.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here we see that, when no compression is used, the Intel instance scales much better and more predictably; however the ThunderX2 is able to reach a similar performance (almost 70 GB/s) than the Intel when enough threads are thrown at the computing task.  This is a really interesting fact, because it is showing that, for first time ever, an ARM CPU can match the memory bandwidth of a latest generation Intel CPU (which BTW, was pretty good at that already).&lt;/p&gt;
&lt;p&gt;Regarding the compressed scenario, Intel Scalable still performs more than 2x faster than the ThunderX2 and it continues to show a really nice scalability.  On the other hand, although the ThunderX2 represents a good step in improving the performance of the ARM architecture, it is still quite far from being able to reach Intel in terms of both raw computing performance and the capacity to scale smoothly.&lt;/p&gt;
&lt;p&gt;When we look at power consumption, although I was not able to find the exact figure for the ThunderX2 CN9975 model that has been used in the benchmarks above, it is probably in the range of 150 W per CPU, which is quite larger than its Intel Scalable 5120 counterpart which is around 100 W per CPU.  That means that Intel is using quite far less power in their CPU, giving them a clear advantage in server computing at this time.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="final-thoughts"&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;From these results, it is quite evident that ARM is making large strides in catching Intel performance, specially in the client side of the things (laptops, and PC desktops), with an important reduction in power consumption, which is specially important for laptops.  Keep these facts in mind when you are going to buy your next laptop or desktop PC and do not blindly assume that Intel is the only reasonable option anymore ;-)&lt;/p&gt;
&lt;p&gt;On the server side, Intel still holds an important advantage though, and it will not be easy to take the performance crown away from them.  However, the fact that ARM is allowing different vendors to produce their own implementations means that the competition can be more specific and each vendor is free to tackle different aspects of server computing.  So it is not difficult to realize that in the next few years we are going to see new ARM exemplars that would be meant not only for crunching numbers, but that will also specialize in different tasks, like storing and serving big data, routing data or performing artificial intelligence, to just mention a few cases (for example, &lt;a class="reference external" href="https://www.marvell.com/documents/8ru3g25b5f77f5pbjwl9/"&gt;Marvell is trying to position the ThunderX2 more specifically for the data server scenario&lt;/a&gt;) that are going to put Intel architectures in difficulties to maintain its current dominance in the data centers.&lt;/p&gt;
&lt;p&gt;Finally, we should not forget the fact that software developers (including myself) have been building high performance libraries using exclusively Intel boxes for &lt;em&gt;decades&lt;/em&gt;, so making them extremely efficient for Intel architectures.  If, as we have seen here, ARM architectures are going to be an alternative in the performance client and server scenarios, then software developers will have to increasingly adopt ARM boxes as part of their tooling so as to continue being competitive in a world that is quite likely it won't necessarily be ruled by Intel anymore.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I would like to thank &lt;a class="reference external" href="https://www.packet.com/"&gt;Packet&lt;/a&gt;, a provider of bare metal servers in the cloud (among other things) for allowing me not only to use their machines for free, but also helping me in different questions about the configuration of the machines.  In particular, Ed Vielmetti has been instrumental in providing me early access to a ThunderX2 server, and making sure that everything was stable enough for the benchmark needs.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="appendix-software-used"&gt;
&lt;h2&gt;Appendix: Software used&lt;/h2&gt;
&lt;p&gt;For reference, here it is the software that has been used for this blog entry.&lt;/p&gt;
&lt;p&gt;For the Kirin 980:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Android 9 - Linux Kernel 4.9.97&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: clang 7.0.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;C-Blosc2&lt;/strong&gt;: 2.0.0a6.dev (2018-05-18)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the ThunderX2:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu 18.04&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: GCC 7.3.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;C-Blosc2&lt;/strong&gt;: 2.0.0a6.dev (2018-05-18)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>ARM</category><category>memory wall</category><category>tuning</category><guid>http://blosc.org/posts/arm-memory-walls-followup/</guid><pubDate>Mon, 07 Jan 2019 10:12:20 GMT</pubDate></item><item><title>Breaking Down Memory Walls</title><link>http://blosc.org/posts/breaking-memory-walls/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Update (2018-08-09)&lt;/strong&gt;: An extended version of this blog post can be found in this &lt;a class="reference external" href="http://www.blosc.org/docs/Breaking-Down-Memory-Walls.pdf"&gt;article&lt;/a&gt;.  On it, you will find a complementary study with synthetic data (mainly for finding ultimate performance limits), a more comprehensive set of CPUs has been used, as well as more discussion about the results.&lt;/p&gt;
&lt;p&gt;Nowadays CPUs struggle to get data at enough speed to feed their cores.  The reason for this is that memory speed is &lt;a class="reference external" href="http://www.blosc.org/docs/StarvingCPUs-CISE-2010.pdf"&gt;growing at a slower pace than CPUs increase their speed at crunching numbers&lt;/a&gt;.   This memory slowness compared with CPUs is generally known as the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall"&gt;Memory Wall&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, let's suppose that we want to compute the aggregation of a some large array; here it is how to do that using OpenMP for leveraging all cores in a CPU:&lt;/p&gt;
&lt;pre class="code c"&gt;&lt;a id="rest_code_58491ebb254444d8842f21044e277cba-1" name="rest_code_58491ebb254444d8842f21044e277cba-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#pragma omp parallel for reduction (+:sum)&lt;/span&gt;
&lt;a id="rest_code_58491ebb254444d8842f21044e277cba-2" name="rest_code_58491ebb254444d8842f21044e277cba-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a id="rest_code_58491ebb254444d8842f21044e277cba-3" name="rest_code_58491ebb254444d8842f21044e277cba-3"&gt;&lt;/a&gt;  &lt;span class="n"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;udata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;a id="rest_code_58491ebb254444d8842f21044e277cba-4" name="rest_code_58491ebb254444d8842f21044e277cba-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;With this, some server (an Intel Xeon E3-1245 v5 @ 3.50GHz, with 4 physical cores and hyperthreading) takes about 14 ms for doing the aggregation of an array with 100 million of float32 values when using 8 OpenMP threads (optimal number for this CPU).  However, if instead of bringing the whole 100 million elements from memory to the CPU we generate the data inside the loop, we are avoiding the data transmission between memory and CPU, like in:&lt;/p&gt;
&lt;pre class="code c"&gt;&lt;a id="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-1" name="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#pragma omp parallel for reduction (+:sum)&lt;/span&gt;
&lt;a id="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-2" name="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a id="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-3" name="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-3"&gt;&lt;/a&gt;  &lt;span class="n"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a id="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-4" name="rest_code_c18e9c044b9a44a7b01ac5e9b119a398-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This loop takes just 3.5 ms, that is, 4x less than the original one.  That means that our CPU could compute the aggregation at a speed that is 4x faster than the speed at which the memory subsystem can provide data elements to the CPU; or put in another words, the CPU is idle, doing nothing during the 75% of the time, waiting for data to arrive (for this example, but there could be other, more extreme cases).  Here we have the memory wall in action indeed.&lt;/p&gt;
&lt;p&gt;That the memory wall exists is an excellent reason to think about ways to workaround it.  One of the most promising venues is to use compression: what if we could store data in compressed state in-memory and use the spare clock cycles of the CPU for decompressing it just when it is needed?  In this blog entry we will see how to implement such a computational kernel on top of data structures that are cache- and compression-friendly and we will examine how they perform on a range of modern CPU architectures.  Some surprises are in store.&lt;/p&gt;
&lt;p&gt;For demonstration purposes, I will run a simple task: summing up the same array of values than above but using a &lt;em&gt;compressed&lt;/em&gt; dataset instead.  While computing sums of values seems trivial, it exposes a couple of properties that are important for our discussion:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;This is a memory-bounded task.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is representative of many aggregation/reduction algorithms that are routinely used out in the wild.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;section id="operating-with-compressed-datasets"&gt;
&lt;h2&gt;Operating with Compressed Datasets&lt;/h2&gt;
&lt;p&gt;Now let's see how to run our aggregation efficiently when using compressed data.  For this, we need:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;A data container that supports on-the-flight compression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A blocking algorithm that leverages the caches in CPUs.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As for the data container, we are going to use the &lt;em&gt;super-chunk&lt;/em&gt; object that comes with the Blosc2 library.  A super-chunk is a data structure that is meant to host many data chunks in a compressed form, and that has some interesting features; more specifically:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compactness&lt;/strong&gt;: everything in a super-chunk is designed to take as little space as possible, not only by using compression, but also my minimizing the amount of associated metadata (like indexes).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Small fragmentation&lt;/strong&gt;: by splitting the data in large enough chunks that are contiguous, the resulting structure ends stored in memory with a pretty small amount of 'holes' in it, allowing a more efficient memory management by both the hardware and the software.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Support for contexts&lt;/strong&gt;: useful when we have different threads and we want to decompress data simultaneously.  Assigning a context per each thread is enough to allow the simultaneous use of the different cores without badly interfering with each other.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Easy access to chunks&lt;/strong&gt;: an integer is assigned to the different chunks so that requesting a specific chunk is just a matter of specifying its number and then it gets decompressed and returned in one shot.  So pointer arithmetic is replaced by indexing operations, making the code less prone to get severe errors (e.g. if a chunk does not exist, an error code is returned instead of creating a segmentation fault).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are curious on how the super-chunk can be created and used, just check the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/sum_openmp.c#L144-L157"&gt;sources for the benchmark&lt;/a&gt; used for this blog.&lt;/p&gt;
&lt;p&gt;Regarding the computing algorithm, I will use one that follows the principles of the blocking computing technique:  for every chunk, bring it to the CPU, decompress it (so that it stays in cache), run all the necessary operations on it, and then proceed to the next chunk:&lt;/p&gt;
&lt;img alt="/images/breaking-down-memory-walls/blocking-technique.png" class="align-center" src="http://blosc.org/images/breaking-down-memory-walls/blocking-technique.png"&gt;
&lt;p&gt;For implementation details, have a look at the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/sum_openmp.c#L191-L209"&gt;benchmark sources&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, and in order to allow maximum efficiency when performing multi-threaded operations, the size of each chunk in the super-chunk should fit in non-shared caches (namely, L1 and L2 in modern CPUs).  This optimization avoids concurrent access to bus caches as much as possible, thereby allowing dedicated access to data caches in each core.&lt;/p&gt;
&lt;p&gt;For our experiments below, we are going to choose a chunksize of 4,000 elements because Blosc2 needs 2 internal buffers for performing the decompression besides the source and destination buffer.  Also, we are using 32-bit (4 bytes) float values for our exercise, so the final size used in caches will be 4,000 * (2 + 2) * 4 = 64,000 bytes, which should fit comfortably in L2 caches in most modern CPU architectures (which normally sports 256 KB or even higher).  Please note that finding an optimal value for this size might require some fine-tuning, not only for different architectures, but also for different datasets.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="the-precipitation-dataset"&gt;
&lt;h2&gt;The Precipitation Dataset&lt;/h2&gt;
&lt;p&gt;There are plenty of datasets out there exposing different data distributions so, depending on your scenario, your mileage may vary.  The dataset chosen here is the result of a &lt;a class="reference external" href="http://reanalysis.meteo.uni-bonn.de"&gt;regional reanalysis covering the European continent&lt;/a&gt;, and in particular, the precipitation data in a certain region of Europe.  Computing the aggregation of this data is representative of a catchment average of precipitation over a drainage area.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Caveat&lt;/em&gt;: For the sake of easy reproducibility, for building the 100 million dataset I have chosen a small &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/read-grid-150x150.py"&gt;geographical area with a size of 150x150&lt;/a&gt; and reused it repeatedly so as to fill the final dataset completely.  As the size of the chunks is lesser than this area, and the super-chunk (as configured here) does not use data redundancies from other chunks, the results obtained here can be safely extrapolated to the actual dataset made from real data (bar some small differences).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="choosing-the-compression-codec"&gt;
&lt;h2&gt;Choosing the Compression Codec&lt;/h2&gt;
&lt;p&gt;When determining the best codec to use inside Blosc2 (it has support for BloscLZ, LZ4, LZ4HC, Zstd, Zlib and Lizard), it turns out that they behave quite differently, both in terms of compression and speed, with the dataset they have to compress &lt;em&gt;and&lt;/em&gt; with the CPU architecture in which they run.  This is quite usual, and the reason why you should always try to find the best codec for your use case.  Here we have how the different codecs behaves for our precipitation dataset in terms of decompression speed for our reference platform (Intel Xeon E3-1245):&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="i7server-codecs" src="http://blosc.org/images/breaking-down-memory-walls/i7server-rainfall-codecs.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="rainfall-cr" src="http://blosc.org/images/breaking-down-memory-walls/rainfall-cr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case LZ4HC is the codec that decompress faster for any number of threads and hence, the one selected for the benchmarks for the reference platform.  A similar procedure has been followed to select the codec for the CPUs.  The selected codec for every CPU will be conveniently specified in the discussion of the results below.&lt;/p&gt;
&lt;p&gt;For completeness, I am also showing the compression ratios achieved by the different codecs for the precipitation dataset.  Although there are significant differences for them, these usually come at the cost of compression/decompression time.  At any rate, even though compression ratio is important, in this blog we are mainly interested in the best decompression speed, so we will use this latter as the only important parameter for codec selection.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="results-on-different-cpus"&gt;
&lt;h2&gt;Results on Different CPUs&lt;/h2&gt;
&lt;p&gt;Now it is time to see how our compressed sum algorithm performs compared with the original uncompressed one.  However, as not all the CPUs are created equal, we are going to see how different CPUs perform doing exactly the same computation.&lt;/p&gt;
&lt;section id="reference-cpu-intel-xeon-e3-1245-v5-4-core-processor-3-50ghz"&gt;
&lt;h3&gt;Reference CPU: Intel Xeon E3-1245 v5 4-Core processor @ 3.50GHz&lt;/h3&gt;
&lt;p&gt;This is a mainstream, somewhat 'small' processor for servers that has an excellent price/performance ratio.  Its main virtue is that, due to its small core count, the CPU can be run at considerably high clock speeds which, combined with a high IPC (Instructions Per Clock) count, delivers considerable computational power.  These results are a good baseline reference point for comparing other CPUs packing a larger number of cores (and hence, lower clock speeds).  Here it is how it performs:&lt;/p&gt;
&lt;img alt="/images/breaking-down-memory-walls/i7server-rainfall-lz4hc-9.png" class="align-center" src="http://blosc.org/images/breaking-down-memory-walls/i7server-rainfall-lz4hc-9.png"&gt;
&lt;p&gt;We see here that, even though the uncompressed dataset does not scale too well, the compressed dataset shows a nice scalability even when using using hyperthreading (&amp;gt; 4 threads); this is a remarkable fact for a feature (hyperthreading) that, despite marketing promises, does not always deliver 2x the performance of the physical cores.  With that, the performance peak for the compressed precipitation dataset (22 GB/s, using LZ4HC) is really close to the uncompressed one (27 GB/s); quite an achievement for a CPU with just 4 physical cores.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="amd-epyc-7401p-24-core-processor-2-0ghz"&gt;
&lt;h3&gt;AMD EPYC 7401P 24-Core Processor @ 2.0GHz&lt;/h3&gt;
&lt;p&gt;This CPU implements EPYC, one of the most powerful architectures ever created by AMD.  It packs 24 physical cores, although internally they are split into 2 blocks with 12 cores each.  Here is how it behaves:&lt;/p&gt;
&lt;img alt="/images/breaking-down-memory-walls/epyc-rainfall-lz4-9.png" class="align-center" src="http://blosc.org/images/breaking-down-memory-walls/epyc-rainfall-lz4-9.png"&gt;
&lt;p&gt;Stalling at 4/8 threads, the EPYC scalability for the uncompressed dataset is definitely not good.  On its hand, the compressed dataset behaves quite differently: it shows a nice scalability through the whole range of cores in the CPU (again, even when using hyperthreading), achieving the best performance (45 GB/s, using LZ4) at precisely 48 threads, well above the maximum performance reached by the uncompressed dataset (30 GB/s).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="intel-scalable-gold-5120-2x-14-core-processor-2-2ghz"&gt;
&lt;h3&gt;Intel Scalable Gold 5120 2x 14-Core Processor @ 2.2GHz&lt;/h3&gt;
&lt;p&gt;Here we have one of the latest and most powerful CPU architectures developed by Intel.  We are testing it here within a machine with 2 CPUs, each containing 14 cores.  Hereâs it how it performed:&lt;/p&gt;
&lt;img alt="/images/breaking-down-memory-walls/scalable-rainfall-lz4-9.png" class="align-center" src="http://blosc.org/images/breaking-down-memory-walls/scalable-rainfall-lz4-9.png"&gt;
&lt;p&gt;In this case, and stalling at 24/28 threads, the Intel Scalable shows a quite remarkable scalability for the uncompressed dataset (apparently, Intel has finally chosen a good name for an architecture; well done guys!).  More importantly, it also reveals an even nicer scalability on the compressed dataset, all the way up to 56 threads (which is expected provided the 2x 14-core CPUs with hyperthreading); this is a remarkable feat for such a memory bandwidth beast.  In absolute terms, the compressed dataset achieves a performance (68 GB/s, using LZ4) that is very close to the uncompressed one (72 GB/s).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="cavium-armv8-2x-48-core"&gt;
&lt;h3&gt;Cavium ARMv8 2x 48-Core&lt;/h3&gt;
&lt;p&gt;We are used to seeing ARM architectures powering most of our phones and tablets, but seeing them performing computational duties is far more uncommon.  This does not mean that there are not ARM implementations that cannot power big servers.  Cavium, with its 48-core in a single CPU, is an example of a server-grade chip.  In this case we are looking at a machine with two of these CPUs:&lt;/p&gt;
&lt;img alt="/images/breaking-down-memory-walls/cavium-rainfall-blosclz-9.png" class="align-center" src="http://blosc.org/images/breaking-down-memory-walls/cavium-rainfall-blosclz-9.png"&gt;
&lt;p&gt;Again, we see a nice scalability (while a bit bumpy) for the uncompressed dataset, reaching its maximum (35 GB/s) at 40 threads.  Regarding the compressed dataset, it scales much more smoothly, and we see how the performance peaks at 64 threads (15 GB/s, using BloscLZ) and then drops significantly after that point (even if the CPU still has enough cores to continue the scaling; I am not sure why is that).  Incidentally, the BloscLZ codec being the best performer here is not a coincidence as it recently received a lot of fine-tuning for ARM.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-we-learned"&gt;
&lt;h2&gt;What We Learned&lt;/h2&gt;
&lt;p&gt;We have explored how to use compression in an nearly optimal way to perform a very simple task: compute an aggregation out of a large dataset.  With a basic understanding of the cache and memory subsystem, and by using appropriate compressed data structures (the super-chunk), we have seen how we can easily produce code that enables modern CPUs to perform operations on compressed data at a speed that approaches the speed of the same operations on uncompressed data (and sometimes exceeding it).  More in particular:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;Performance for the compressed dataset scales very well on the number of threads for all the CPUs (even hyperthreading seems very beneficial at that, which is a welcome surprise).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The CPUs that benefit the most from compression are those with relatively low memory bandwidth and CPUs with many cores.  In particular, the EPYC architecture is a good example and we have shown how the compressed dataset can operate 50% faster that the uncompressed one.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when using CPUs with a low number of cores (e.g. our reference CPU, with only 4) we can achieve computational speeds on compressed data that can be on par with traditional, uncompressed computations, while saving precious amounts of memory and disk space.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The appropriate codec (and other parameters) to use within Blosc2 for maximum performance can vary depending on the dataset and the CPU used.  Having a way to automatically discover the optimal compression parameters would be a nice addition to the Blosc2 library.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
&lt;section id="final-thoughts"&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;To conclude, it is interesting to remember here what Linus Torvalds said back in 2006 (talking about the git system that he created the year before):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[...] git actually has a simple  design, with stable and reasonably well-documented data structures.  In fact, I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful.
[...] I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, we all know how drastic Linus can be in his statements, but I cannot agree more on how important is to adopt a data-driven view when designing our applications.  But I'd go further and say that, when trying to squeeze the last drop of performance out of modern CPUs, data containers need to be structured in a way that leverages the characteristics of the underlying CPU, as well as to facilitate the application of the blocking technique (and thereby allowing compression to run efficiently).  Hopefully, installments like this can help us explore new possibilities to break down the memory wall that bedevils modern computing.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks to my friend Scott Prater for his great advices on improving my writing style, Dirk Schwanenberg for pointing out to the precipitation dataset and for providing the script for reading it, and Robert McLeod, J. David IbÃ¡Ã±ez and Javier Sancho for suggesting general improvements (even though some of their suggestions required such a big amount of work that made me ponder about their actual friendship :).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="appendix-software-used"&gt;
&lt;h2&gt;Appendix: Software used&lt;/h2&gt;
&lt;p&gt;For reference, here it is the software that has been used for this blog entry:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu 18.04&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: GCC 7.3.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;C-Blosc2&lt;/strong&gt;: 2.0.0a6.dev (2018-05-18)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>caches</category><category>memory wall</category><category>tuning</category><guid>http://blosc.org/posts/breaking-memory-walls/</guid><pubDate>Mon, 25 Jun 2018 18:32:20 GMT</pubDate></item><item><title>New Forward Compatibility Policy</title><link>http://blosc.org/posts/new-forward-compat-policy/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;section id="the-215-issue"&gt;
&lt;h2&gt;The #215 issue&lt;/h2&gt;
&lt;p&gt;Recently, a C-Blosc user filed an &lt;a class="reference external" href="https://github.com/Blosc/c-blosc/issues/215"&gt;issue&lt;/a&gt; describing how buffers created with a modern version of C-Blosc (starting from 1.11.0) were not able to be decompressed with an older version of the library (1.7.0), i.e. C-Blosc was effectively breaking the so-called &lt;em&gt;forward-compatibility&lt;/em&gt;.  After some investigation, it turned out that the culprit was an optimization that was introduced in 1.11.0 in order to allow better compression ratios and in some cases, better speed too.&lt;/p&gt;
&lt;p&gt;Not all the codecs inside C-Blosc were equally affected; the ones that are experiencing the issue are LZ4, LZ4HC and Zlib (quite luckily, BloscLZ, the default codec, is not bitten by this and should be forward compatible probably til 1.0.0); that is, when a user is using a modern C-Blosc library (&amp;gt; 1.11.0) &lt;em&gt;and&lt;/em&gt; is using any of the affected codecs, there are situations (namely when the shuffle or bitshuffle filter are active and the buffers to be compressed are larger than 32 KB) that the result cannot be decompressed with older versions (&amp;lt; 1.11.0).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="why-this-occurred"&gt;
&lt;h2&gt;Why this occurred?&lt;/h2&gt;
&lt;p&gt;Prior to 1.11.0, Blosc has traditionally split the internal blocks (the different pieces in which the buffer to be compressed is divided) into smaller pieces composed by the &lt;em&gt;same significant&lt;/em&gt; bytes that the shuffle filter has put together.  The rational for doing this is that these pieces are, in many cases, hosting values that are either zeros or very close byte values (this is why shuffle works well in general for binary data), and asking the codec to compress these split-blocks separately was quite less effort than compressing a complete block hence providing more speed).&lt;/p&gt;
&lt;p&gt;However, I realized that the so-called High Compression Ration codecs (the HCR codecs inside BLosc are LZ4HC, Zlib and Zstd) generally benefited from this split not happening (the reason is clear: more data means more opportunities for finding more duplicates) and in some cases, the speed was better too.  So, in C-Blosc 1.11.0, I decided that the split was not going to happen &lt;em&gt;by default&lt;/em&gt; for HCR codecs (in the last minute I decided to include LZ4 too, for which the experiments showed a noticeable performance bump too, see below). Fortunately, the Zstd codec was introduced (in an out-of-beta way) at the very same 1.11.0 release than this split-block change, so in practice data compressed with the Zstd codec is not affected by this.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="new-forward-compatibility-enforcement-policy"&gt;
&lt;h2&gt;New forward compatibility enforcement policy&lt;/h2&gt;
&lt;p&gt;Although this change was deliberate and every measure was put in making it &lt;em&gt;backward compatible&lt;/em&gt; (i.e. new library versions could read buffers compressed with older versions), I was not really aware of the inconveniences that the change was putting for people creating data files using newer versions of the library and expecting these to be read with older versions.&lt;/p&gt;
&lt;p&gt;So in order to prevent something like this to happen again, I decided that &lt;em&gt;forward compatibility&lt;/em&gt; is going to be &lt;em&gt;enforced&lt;/em&gt; for future releases of C-Blosc (just for 1.x series; C-Blosc 2.x should be just backward compatible with 1.x).  By the way, this new &lt;em&gt;forward compatibility&lt;/em&gt; policy will require a significantly more costly &lt;a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/RELEASING.rst#forward-compatibility-testing"&gt;release procedure&lt;/a&gt;, as different libraries for a specific set of versions have to be manually re-created; if you know a more automatic way to test forward compatibility with old versions of a library, I'd love to hear your comments.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="measures-taken-and-new-split-mode"&gt;
&lt;h2&gt;Measures taken and new split mode&lt;/h2&gt;
&lt;p&gt;In order to alleviate this forward incompatibility issue, I decided to revert the split change introduced in 1.11.0 in forthcoming 1.14.0 release.  That means that, &lt;em&gt;by default&lt;/em&gt;, compressed buffers created with C-Blosc 1.14.0 and on will be forward compatible with all the previous C-Blosc libraries (till 1.3.0, which was when support for different codecs was introduced).  That is, the only buffers that will pose problems to be decompressed with old versions are those created with a C-Blosc library with versions between 1.11.0 and 1.14.0 &lt;em&gt;and&lt;/em&gt; using the shuffle/bitshuffle filter in combination with the LZ4, LZ4HC or Zlib codecs.&lt;/p&gt;
&lt;p&gt;For fine-tuning how the block-split would happen or not, I have introduced a new API function, &lt;cite&gt;void blosc_set_splitmode(int mode)&lt;/cite&gt;, that allows to select the split mode that is going to be used during the compression.  The split modes that can take the new function are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;BLOSC_FORWARD_COMPAT_SPLIT&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BLOSC_AUTO_SPLIT&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BLOSC_NEVER_SPLIT&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BLOSC_ALWAYS_SPLIT&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;cite&gt;BLOSC_FORWARD_COMPAT_SPLIT&lt;/cite&gt; offers reasonably forward compatibility (i.e. Zstd still will not split, but this is safe because it was introduced at the same time than the split change in 1.11.0), &lt;cite&gt;BLOSC_AUTO_SPLIT&lt;/cite&gt; is for nearly optimal results (based on heuristics; this is the same approach than the one introduced in 1.11.0), &lt;cite&gt;BLOSC_NEVER_SPLIT&lt;/cite&gt; and &lt;cite&gt;BLOSC_ALWAYS_SPLIT&lt;/cite&gt; are for the user interested in experimenting for getting best compression ratios and/or speed.  If &lt;cite&gt;blosc_set_splitmode()&lt;/cite&gt; is not called, the default mode will be BLOSC_FORWARD_COMPAT_SPLIT.&lt;/p&gt;
&lt;p&gt;Also, the user will be able to specify the split mode by using the &lt;cite&gt;BLOSC_SPLITMODE&lt;/cite&gt; variable environment.  If that variable exists in the environment, and has any value among:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;'BLOSC_FORWARD_COMPAT_SPLIT'&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;'BLOSC_AUTO_SPLIT'&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;'BLOSC_NEVER_SPLIT'&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;'BLOSC_ALWAYS_SPLIT'&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;this will select the corresponding split mode.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="how-this-change-affects-performance"&gt;
&lt;h2&gt;How this change affects performance&lt;/h2&gt;
&lt;p&gt;So as to allow to visualize at a glance the differences in performance that the new release is introducing, let's have a look at the impact on two of the most used codecs inside C-Blosc: LZ4 and LZ4HC.  In the plots below the left side is the pre-1.14.0 version (non-split blocks) and on the right, the forthcoming 1.14.0 (split blocks).  Note that I am using here the typical synthetic benchmarks for C-Blosc, so expect a different outcome for your own data.&lt;/p&gt;
&lt;p&gt;Let's start by LZ4HC, a High Compression Ratio codec (and the one that triggered the initial report of the &lt;a class="reference external" href="https://github.com/Blosc/c-blosc/issues/215"&gt;forward compatibility issue&lt;/a&gt;).  When compressing, we have this change in behavior:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4hc-c" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4hc-pre-1.14-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4hc-compat-c" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4hc-compat-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For LZ4HC decompression:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4hc-d" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4hc-pre-1.14-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4hc-compat-d" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4hc-compat-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For LZ4HC one can see that, when using non-split blocks, it can achieve better compression ratios (this is expected, as the block sizes are larger).  Speed-wise the performance is quite similar, with maybe some advantage for split blocks (expected as well).  As the raison d'Ãªtre for HCR codecs is maximize the compression ration, that was the reason why I did the split change for LZ4HC in 1.11.0.&lt;/p&gt;
&lt;p&gt;And now for LZ4, a codec meant for speed (although it normally gives pretty decent results in compression ratio).  When compressing, here it is the change:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-c" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4-pre-1.14-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-compat-c" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4-compat-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For LZ4 decompression:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-d" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4-pre-1.14-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="lz4-compat-d" src="http://blosc.org/images/new-forward-compat-policy/suite-lz4-compat-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So, here one can see that when using Blosc pre-1.14 (i.e. non-split blocks) we are getting a bit less compression ratio than for the forthcoming 1.14, which even if counter-intutive, it matches my experience with non-HCR codecs.  Speed-wise the difference is not that much during compression; however, decompression is significantly faster with non-split blocks.  As LZ4 is meant for speed, this was possibly the reason that pushed me towards making non-split blocks by default for LZ4 in addition to HCR codecs in 1.11.0.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="feedback"&gt;
&lt;h2&gt;Feedback&lt;/h2&gt;
&lt;p&gt;If you have suggestions on this forward compatibility issue or the solution that has been implemented, please shout!&lt;/p&gt;
&lt;/section&gt;
&lt;section id="appendix-hardware-and-software-used"&gt;
&lt;h2&gt;Appendix: Hardware and software used&lt;/h2&gt;
&lt;p&gt;For reference, here it is the configuration that I used for producing the plots in this blog entry.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;CPU: Intel Xeon E3-1245 v5 @ 3.50GHz (4 physical cores with hyper-threading)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;OS:  Ubuntu 16.04&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compiler: GCC 6.3.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;C-Blosc: 1.13.7 and 1.14.0 (release candidate)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;LZ4: 1.8.1&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>forward compatibility policy splitmode</category><guid>http://blosc.org/posts/new-forward-compat-policy/</guid><pubDate>Wed, 21 Feb 2018 15:32:20 GMT</pubDate></item><item><title>Blosc Has Won Google's Open Source Peer Bonus Program</title><link>http://blosc.org/posts/prize-push-Blosc2/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;Past month &lt;a class="reference external" href="https://opensource.googleblog.com/2017/10/more-open-source-peer-bonus-winners.html"&gt;Google announced the winners for the 2017âs second round of their Open Source Peer Bonus program&lt;/a&gt; and I was among them for my commitment to the Blosc project.  It took a bit, but I wanted to express my thoughts on this nice event.  Needless to say, I am proud and honored for this recognition, most specially when this is the first completely uninterested donation that someone made to me after 15 years of doing open source (in many occasions doing that as a full-time work), so thank you very much Google!  The assumption is that people does open source because 1) they believe in the concept and 2) they can earn a public consideration that allows them to get contracts (so allowing many of us to have a life!).  However, this time the unexpected happened, and that an important corporation like Google decided to publicly recognize this work makes me very happy (would that pave the way for others to follow? :-).&lt;/p&gt;
&lt;p&gt;Having said this, and as it happens with any open source project that has seen some success, the contributions for other people have been instrumental in making Blosc such a featured and stable library.  People like Jack Pappas, Valentin HÃ¤nel, Christohper Speller, Antonio Valentino and up to 30 contributors made important contributions to the project.  This award goes indeed to them too.&lt;/p&gt;
&lt;p&gt;This push comes very timely because it is giving me more stamina towards the release of &lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;Blosc2&lt;/a&gt;.  Blosc2 is the next generation of Blosc, and will add features like:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Full 64-bit support for chunks (i.e. not anymore limited to 2 GB).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;New filters, like delta and truncation of floating point precision.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A new filter pipeline that will allow to run more than one filter before the compression step.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for variable length objects (i.e. not limited to fixed-length datasets).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for dictionaries between different blocks in the same chunk.  That will be important for allowing smaller chunks (and hence improving decompression latency) while keeping compression ratio and performance mostly untouched.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for more codecs (&lt;a class="reference external" href="http://blosc.org/posts/new-lizard-codec/"&gt;lizard&lt;/a&gt; support is already in).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;New serialisation format which is meant to allow self-discovery via magic numbers and introspection.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;New super-chunk object that will allow to work seamlessly with arbitrarily large sets of chunks, both in-memory and on-disk.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for &lt;a class="reference external" href="http://blosc.org/posts/arm-is-becoming-a-first-class-citizen-for-blosc/"&gt;SIMD in ARM processors&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All in all, and after more than 2 years working in different aspects of these features, I am quite satisfied on the progress so far. My expectation was to do a beta release during this fall, and although the work is quite advanced, there are still some loose ends that require quite a bit of work.  If you like where I am headed and are interested in seeing this work to complete faster, a contribution to the project in the form of a pull request or, better yet, a donation suggesting which feature you would like the most will be greatly appreciated.&lt;/p&gt;
&lt;p&gt;Finally, I'd like to take the opportunity to annonunce that Blosc has a logo (finally!). You can admire it at the header of this page.  This is the work of &lt;a class="reference external" href="http://domenec123.blogspot.com.es"&gt;DomÃ¨nec Morera&lt;/a&gt; who also made for us the logo of &lt;a class="reference external" href="http://www.pytables.org"&gt;PyTables&lt;/a&gt;.  I really think he is a great artist and that he did an excellent job again; I hope the new logo will be beneficial for the Blosc project as a whole!&lt;/p&gt;&lt;/div&gt;</description><category>Blosc2</category><category>Google</category><category>Prize</category><guid>http://blosc.org/posts/prize-push-Blosc2/</guid><pubDate>Fri, 17 Nov 2017 17:32:20 GMT</pubDate></item></channel></rss>