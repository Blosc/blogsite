<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Blosc, an extremely fast, multi-threaded, meta-compressor library">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Blosc Main Blog Page  (old posts, page 1) | Blosc Main Blog Page </title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="rss.xml">
<link rel="canonical" href="http://blosc.org/index-1.html">
<link rel="icon" href="blosc-favicon_16x16.png" sizes="16x16">
<link rel="icon" href="blosc-favicon_32x32.png" sizes="32x32">
<link rel="icon" href="blosc-favicon_64x64.png" sizes="64x64">
<link rel="icon" href="blosc-favicon_128x128.png" sizes="128x128">
<link rel="prev" href="index-2.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-111342564-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111342564-2');
</script>
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://blosc.org/">
                <img src="blosc-logo_128.png" alt="Blosc Main Blog Page " id="logo"></a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="pages/blosc-in-depth/">Blosc In Depth</a>
                </li>
<li>
<a href="https://c-blosc2.readthedocs.io/">Documentation</a>
                </li>
<li>
<a href="pages/donate/">Donate to Blosc</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/prize-push-Blosc2/" class="u-url">Blosc Has Won Google's Open Source Peer Bonus Program</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/prize-push-Blosc2/" rel="bookmark">
            <time class="published dt-published" datetime="2017-11-17T17:32:20Z" itemprop="datePublished" title="2017-11-17 17:32">2017-11-17 17:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/prize-push-Blosc2/#disqus_thread" data-disqus-identifier="cache/posts/prize-push-blosc2.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Past month <a class="reference external" href="https://opensource.googleblog.com/2017/10/more-open-source-peer-bonus-winners.html">Google announced the winners for the 2017’s second round of their Open Source Peer Bonus program</a> and I was among them for my commitment to the Blosc project.  It took a bit, but I wanted to express my thoughts on this nice event.  Needless to say, I am proud and honored for this recognition, most specially when this is the first completely uninterested donation that someone made to me after 15 years of doing open source (in many occasions doing that as a full-time work), so thank you very much Google!  The assumption is that people does open source because 1) they believe in the concept and 2) they can earn a public consideration that allows them to get contracts (so allowing many of us to have a life!).  However, this time the unexpected happened, and that an important corporation like Google decided to publicly recognize this work makes me very happy (would that pave the way for others to follow? :-).</p>
<p>Having said this, and as it happens with any open source project that has seen some success, the contributions for other people have been instrumental in making Blosc such a featured and stable library.  People like Jack Pappas, Valentin Hänel, Christohper Speller, Antonio Valentino and up to 30 contributors made important contributions to the project.  This award goes indeed to them too.</p>
<p>This push comes very timely because it is giving me more stamina towards the release of <a class="reference external" href="https://github.com/Blosc/c-blosc2">Blosc2</a>.  Blosc2 is the next generation of Blosc, and will add features like:</p>
<ul class="simple">
<li><p>Full 64-bit support for chunks (i.e. not anymore limited to 2 GB).</p></li>
<li><p>New filters, like delta and truncation of floating point precision.</p></li>
<li><p>A new filter pipeline that will allow to run more than one filter before the compression step.</p></li>
<li><p>Support for variable length objects (i.e. not limited to fixed-length datasets).</p></li>
<li><p>Support for dictionaries between different blocks in the same chunk.  That will be important for allowing smaller chunks (and hence improving decompression latency) while keeping compression ratio and performance mostly untouched.</p></li>
<li><p>Support for more codecs (<a class="reference external" href="http://blosc.org/posts/new-lizard-codec/">lizard</a> support is already in).</p></li>
<li><p>New serialisation format which is meant to allow self-discovery via magic numbers and introspection.</p></li>
<li><p>New super-chunk object that will allow to work seamlessly with arbitrarily large sets of chunks, both in-memory and on-disk.</p></li>
<li><p>Support for <a class="reference external" href="http://blosc.org/posts/arm-is-becoming-a-first-class-citizen-for-blosc/">SIMD in ARM processors</a>.</p></li>
</ul>
<p>All in all, and after more than 2 years working in different aspects of these features, I am quite satisfied on the progress so far. My expectation was to do a beta release during this fall, and although the work is quite advanced, there are still some loose ends that require quite a bit of work.  If you like where I am headed and are interested in seeing this work to complete faster, a contribution to the project in the form of a pull request or, better yet, a donation suggesting which feature you would like the most will be greatly appreciated.</p>
<p>Finally, I'd like to take the opportunity to annonunce that Blosc has a logo (finally!). You can admire it at the header of this page.  This is the work of <a class="reference external" href="http://domenec123.blogspot.com.es">Domènec Morera</a> who also made for us the logo of <a class="reference external" href="http://www.pytables.org">PyTables</a>.  I really think he is a great artist and that he did an excellent job again; I hope the new logo will be beneficial for the Blosc project as a whole!</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/new-lizard-codec/" class="u-url">The Lizard Codec</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/new-lizard-codec/" rel="bookmark">
            <time class="published dt-published" datetime="2017-07-31T17:32:20Z" itemprop="datePublished" title="2017-07-31 17:32">2017-07-31 17:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/new-lizard-codec/#disqus_thread" data-disqus-identifier="cache/posts/the-lizard-codec.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>The past weekend I was putting some time in integrating one of the codecs that I was lately more curious about (specially since the release of its 1.0 version some months ago).  I am talking about <a class="reference external" href="https://github.com/inikep/lizard">Lizard</a>, a direct derivative of the LZ4 codec and whose author is Przemyslaw Skibinski.  One should remark that Przemyslaw is not new in the compression arena as he has helped Yann Collet quite a lot during the <a class="reference external" href="https://github.com/facebook/zstd">Zstandard</a> development, and also he is the author of <a class="reference external" href="https://github.com/inikep/lzbench">lzbench</a>, a nice and comprehensive in-memory benchmark of a series of open-source LZ77/LZSS/LZMA compressors.</p>
<p>The reason why I was thinking that Lizard was an interesting codec for Blosc is because it mixes some interesting optimizations of the LZ4 codec and, optionally, it can use them in combination with the <a class="reference external" href="http://www.geeksforgeeks.org/greedy-algorithms-set-3-huffman-coding/">Huffman coding</a> by selecting different compression levels (currently from 10 to 49).</p>
<p>After the initial support for Lizard in Blosc, it took me some time to determine a decent map between the compression levels in Blosc (1-9) to the ones in Lizard (10-49), mainly for allowing fast compression and decompression (what Blosc is all about).  During the way, I discovered that the most interesting compression levels in Lizard have been  <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/lizard/blosc/blosc.c#L606-L625">10, 20 and 41</a>.  This was indeed determined using the <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/bench.c">synthetic benchamrk</a> that comes with Blosc, but that is the usual path that gaves me quite good estimates for a first calibration (we are working on a more complete tuner that can adapt to actual data in real time, but I'll blog about it in another occasion).</p>
<section id="a-new-star-has-born"><h2>A new star has born</h2>
<p>After the calibration was done the results of the new codec are really surprising:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lizard-c" src="images/the-lizard-codec/lizard-suite-8p-compr.png"></p></td>
<td><p><img alt="lizard-d" src="images/the-lizard-codec/lizard-suite-8p-decompr.png"></p></td>
</tr></tbody>
</table>
<p>The interesting part of Lizard can be seen when large compression levels for Blosc are used, specially 8 and 9.  Those are mapped to compression level 41 in Lizard, which means that the LIZv1 + Huffman compression method is used.  Following the documentation, this matches the compression levels of Zlib and Zstd/Brotli, and it shows.</p>
<p>Just for reference, here it is the performance of the LZ4 codec, from which Lizard inherits a good part of its code:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4-c" src="images/codecs-pgo/lz4-comp-gcc-6.3.png"></p></td>
<td><p><img alt="lz4-d" src="images/codecs-pgo/lz4-decomp-gcc-6.3.png"></p></td>
</tr></tbody>
</table>
<p>And here the performance of Zstd, which also uses Huffman coding:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="zstd-c" src="images/codecs-pgo/zstd-comp-gcc-6.3.png"></p></td>
<td><p><img alt="zstd-d" src="images/codecs-pgo/zstd-decomp-gcc-6.3.png"></p></td>
</tr></tbody>
</table>
<p>So, while Lizard (or at least, the current mapping that I did for it inside Blosc) in low compression levels cannot beat the speed of LZ4 or the compression ratios of Zstd, for high compression levels it clearly beats LZ4 and Zstd speed both for compression and decompression.  Most specially, it works extremely well for achieving pretty reasonable compression ratios (typically better than Zlib, albeit not as good as Zstd) at very good decompression speed and exceptional compression speed (compressing at more than the memcpy() speed, at very good ratios, oh really?).</p>
<p>Finally, for those wondering why I have not used the LIZv1 + Huffman compression method also for the lower compression levels in Blosc, the answer is that I obviously tried that, but for some reason, this method only performs well for large buffers, whereas for small buffers (like the ones created by low compression levels in Blosc) its performance is rather poor.  I was kind of getting a similar behaviour with Zstd, where performance shines for decompressing large buffers (the difference is that Lizard can compress at tremendous speed when compared with Zstd in this scenario), so I suppose this is typical when Huffman methods are used.</p>
</section><section id="finding-its-place-among-blosc-codecs"><h2>Finding its place among Blosc codecs</h2>
<p>In my previous blog, I was saying that Zstd has virtually no competitor in Write Once Read Multiple scenarios.  However, I think there is still a niche for codecs that, without providing the extreme compression ratios of Zstd, they still show big enough compression muscle without loosing too much compression speed.  IMO, this is a good description of how Lizard performs.  However, in Blosc1 we only have slots for a couple of codecs more (but that will not be a problem for Blosc2, where much more codecs will be supported), and although I am pretty enthusiastic on adding Lizard it would be nice to gather users feedback before than that.  So in case you are a Blosc user, please use the <a class="reference external" href="https://github.com/Blosc/c-blosc/tree/lizard">lizard branch</a> of the C-Blosc repo (<a class="reference external" href="https://github.com/Blosc/c-blosc2/pull/21">UPDATE: Lizard has been merged into the C-Blosc2 repo recently</a>) and report back your results.</p>
</section><section id="appendix-hardware-and-software-used"><h2>Appendix: Hardware and software used</h2>
<p>For reference, here it is the configuration that I used for producing the plots in this blog entry.</p>
<ul class="simple">
<li><p>CPU: Intel Xeon E3-1245 v5 @ 3.50GHz (4 physical cores with hyper-threading)</p></li>
<li><p>OS:  Ubuntu 16.04</p></li>
<li><p>Compiler: GCC 6.3.0</p></li>
<li><p>C-Blosc2: 2.0.0a4.dev (2017-07-29, lizard branch)</p></li>
<li><p>Lizard: 1.0.0</p></li>
</ul></section>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/codecs-pgo/" class="u-url">Testing PGO with LZ4 and Zstd codecs</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/codecs-pgo/" rel="bookmark">
            <time class="published dt-published" datetime="2017-07-19T11:32:20Z" itemprop="datePublished" title="2017-07-19 11:32">2017-07-19 11:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/codecs-pgo/#disqus_thread" data-disqus-identifier="cache/posts/codecs-pgo.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>In <a class="reference external" href="http://blosc.org/posts/blosclz-tuning/">past week's post</a> I was showing how the PGO (<a class="reference external" href="https://en.wikipedia.org/wiki/Profile-guided_optimization">Profile Guided Optimization</a>) capability in modern compilers allowed for a good increase in the performance of the BloscLZ codec.  Today I'd like to test how the PGO optimization affected the speed of the same <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/bench.c">synthetic benchmark</a> that comes with C-Blosc2 for the two other of the most used codecs in Blosc: <a class="reference external" href="http://lz4.github.io/lz4/">LZ4</a> and <a class="reference external" href="http://facebook.github.io/zstd/">Zstd</a>.</p>
<section id="id1"><h2>LZ4</h2>
<p>First, for GCC without PGO:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4-old-c" src="images/codecs-pgo/lz4-comp-gcc-6.3.png"></p></td>
<td><p><img alt="lz4-old-d" src="images/codecs-pgo/lz4-decomp-gcc-6.3.png"></p></td>
</tr></tbody>
</table>
<p>Now with PGO enabled:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4-pgo-c" src="images/codecs-pgo/lz4-comp-pgo.png"></p></td>
<td><p><img alt="lz4-pgo-d" src="images/codecs-pgo/lz4-decomp-pgo.png"></p></td>
</tr></tbody>
</table>
<p>We can see here that, similarly to BloscLZ, although the compression speed has not improved significantly, the decompression is now reaching up to 30 GB/s, and for high compression levels, up to 20 GB/s, which is pretty good.</p>
</section><section id="id2"><h2>Zstd</h2>
<p>First, for GCC without PGO:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="zstd-old-c" src="images/codecs-pgo/zstd-comp-gcc-6.3.png"></p></td>
<td><p><img alt="zstd-old-d" src="images/codecs-pgo/zstd-decomp-gcc-6.3.png"></p></td>
</tr></tbody>
</table>
<p>Now with PGO enabled:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="zstd-pgo-c" src="images/codecs-pgo/zstd-comp-pgo.png"></p></td>
<td><p><img alt="zstd-pgo-d" src="images/codecs-pgo/zstd-decomp-pgo.png"></p></td>
</tr></tbody>
</table>
<p>Wow, in this case we <em>really</em> can see important speedups in both compressing and decompressing.  Specially interesting is the decompression case where, for the higher compression levels, Zstd can reach speeds exceeding 20 GB/s (whereas without PGO it was not able to exceed 12 GB/s) which seems a bit crazy provided the wonderful compression ratios that Zstd is able to achieve.  Beyond any doubt, for Write Once Read Multiple scenarios there is no competitor for Zstd, most specially when PGO is used.</p>
<p>This confirms that, once again, when performance is critical for your applications, PGO should be part of your daily weaponery.</p>
</section><section id="appendix-hardware-and-software-used"><h2>Appendix: Hardware and software used</h2>
<p>For reference, here it is the configuration that I used for producing the plots in this blog entry.</p>
<ul class="simple">
<li><p>CPU: Intel Xeon E3-1245 v5 @ 3.50GHz (4 physical cores with hyper-threading)</p></li>
<li><p>OS:  Ubuntu 16.04</p></li>
<li><p>Compiler: GCC 6.3.0 (using PGO)</p></li>
<li><p>C-Blosc2: 2.0.0a4.dev (2017-07-11)</p></li>
<li><p>LZ4 codec: 1.7.5</p></li>
<li><p>Zstd codec: 1.3.0</p></li>
</ul></section>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/blosclz-tuning/" class="u-url">Fine Tuning the BloscLZ codec</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/blosclz-tuning/" rel="bookmark">
            <time class="published dt-published" datetime="2017-07-14T06:32:20Z" itemprop="datePublished" title="2017-07-14 06:32">2017-07-14 06:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/blosclz-tuning/#disqus_thread" data-disqus-identifier="cache/posts/blosclz-tuning.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Yesterday I was reading about the exciting new CPU architectures that both <a class="reference external" href="http://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade">AMD and Intel are introducing</a> and I was wondering how the improved architecture of the new cores and most specially, its caches, could apply to Blosc.  It turns out that I have access to a server with a relatively modern CPU (Xeon E3-1245 v5 @ 3.50GHz, with 4 physical cores) and I decided to have a go at fine-tune the included BloscLZ codec (the one that I know the best) inside C-Blosc2.  Of course, I already spent some time tuning BloscLZ, but that was some years ago and provided the fast pace at which CPUs are evolving I thought that this was excellent timing for another round of fine-tuning, most specially in preparation for users adopting the forthcoming  RYZEN, Threadripper, EPYC and Skylake-SP architectures.</p>
<p>Frankly speaking, I was expecting to get very little improvements in this front, but the results have been unexpectedly good.  Keep reading.</p>
<section id="where-we-come-from"><h2>Where we come from</h2>
<p>Just for reference, here it is the performance of the BloscLZ codec in my server before the new tuning work:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="blosclz-old-c" src="images/blosclz-tuning/blosclz-suite-8p-old-param-compr.png"></p></td>
<td><p><img alt="blosclz-old-d" src="images/blosclz-tuning/blosclz-suite-8p-old-param-decompr.png"></p></td>
</tr></tbody>
</table>
<p>That is the typical synthetic benchmark in Blosc, but for the plotting function in the C-Blosc2 project, the actual size of each compressed buffer is shown (and not the size of the whole dataset, as in C-Blosc1).  In this case, the dataset (256 MB) is split in chunks of 4 MB, and provided that our CPU has a LLC (Last Level Cache) of 8 MB, this is sort of an optimal size for achieving maximum performance (the buffers meant for Blosc usually do not exceed 4 MB for most of its common usages).</p>
<p>As can be seen, performance is quite good, although compression ratios left something to be desired.  Furthermore, for the maximum compression level (9), the compression ratio has a regression with respect to the previous level (8).  This is not too bad, and sometimes happens in any codec, but the nice thing would be to avoid it if possible.</p>
</section><section id="the-new-blosclz-after-fine-tuning"><h2>The new BloscLZ after fine tuning</h2>
<p>So, after a couple of hours playing with different parameters in BloscLZ and C-Blosc2, I started to realize that the new Intel CPU performed exceedingly well when asked to compress more, to the point that high compression settings were not performing that slow in comparision with low compression ones; rather the contrary: high compression settings were operating at almost the same speed than lower ones (which was a welcome surprise indeed).  Hence I tried to be set quite more aggressive parameters in BloscLZ, while trying to keep the size of internal blocks in Blosc2 below 256 KB (the typical size of L2 caches in modern CPUs).  This is the result:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="blosclz-new-c" src="images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6-compr.png"></p></td>
<td><p><img alt="blosclz-new-d" src="images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6-decompr.png"></p></td>
</tr></tbody>
</table>
<p>So the compression ratios have increased quite a bit, specially for the larger compression levels (going from  less than 10x to more than 20x for this benchmark).  This is courtesy of the new, more agressive compression parameters.  Strikingly enough, performance has also increased in general, but specially for these large compression levels.  I am not completely certain on why this is the case, but probably this new CPU architecture is much better at out-of-order execution and prefetching larger blocks of data, which benefits compressing both faster even in large buffers; similarly, I am pretty sure that improvements in compiler technology (I am using a recent GCC 6.3.0 here) is pretty important for getting faster binary code.  We can also see that when using 4 threads (i.e. using all the physical cores available in our CPU at hand), BloscLZ can compress <em>faster</em> than a memcpy() call for most of the cases, and most specially at large compression levels, as mentioned before.  Oh, and we can see that we also got rid of the regression in the compression ratio for compression level 9, which is cool.</p>
<p>Regarding decompression speed, we can see that the new tuning gave general speed-ups of between 10% and 20%, with no significant slowdowns in any case.  All in all, quite good results indeed!</p>
</section><section id="room-for-more-improvements-enter-pgo"><h2>Room for more improvements?  Enter PGO.</h2>
<p>To temporary end (optimization is a never ending task) this quest for speed, I am curious about the speed that we can buy by using the PGO (<a class="reference external" href="https://en.wikipedia.org/wiki/Profile-guided_optimization">Profile Guided Optimization</a>) capability that is present in most of the modern compilers.  Here I am going to use the PGO of GCC in combination with our benchmark at hand so as to provide the profile for the compiler optimizer.  Here are the results when PGO is applied to the new parametrization:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="blosclz-pgo-c" src="images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6.pgo-compr.png"></p></td>
<td><p><img alt="blosclz-pgo-d" src="images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6.pgo-decompr.png"></p></td>
</tr></tbody>
</table>
<p>So, while the speed improvement for compression is not significant (albeit a bit better), the big improvement comes in the decompression speed, where we see speeds almost reaching 50 GB/s and perhaps more interestingly, more than 35 GB/s for maximum compression level, and for first time in my life as Blosc developer, I can see the speed of decompressing with <em>one single thread</em> being faster than memcpy() for <em>all</em> the compression levels.</p>
<p>I wonder what the PGO technique can bring to other codecs in Blosc, but that is stuff for other blog post.  At any rate, the reader is encouraged to try PGO on their own setups.  I am pretty sure that she will be pleased to see nice speed improvements.</p>
</section><section id="appendix-hardware-and-software-used"><h2>Appendix: Hardware and software used</h2>
<p>For reference, here it is the configuration that I used for producing the plots in this blog entry.</p>
<ul class="simple">
<li><p>CPU: Intel Xeon E3-1245 v5 @ 3.50GHz (4 physical cores with hyper-threading)</p></li>
<li><p>OS:  Ubuntu 16.04</p></li>
<li><p>Compiler: GCC 6.3.0</p></li>
<li><p>C-Blosc2: 2.0.0a4.dev (2017-07-14)</p></li>
<li><p>BloscLZ: 1.0.6 (2017-07-14)</p></li>
</ul></section>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/zstd-has-just-landed-in-blosc/" class="u-url">Zstd has just landed in Blosc</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/zstd-has-just-landed-in-blosc/" rel="bookmark">
            <time class="published dt-published" datetime="2016-07-20T11:32:20Z" itemprop="datePublished" title="2016-07-20 11:32">2016-07-20 11:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/zstd-has-just-landed-in-blosc/#disqus_thread" data-disqus-identifier="cache/posts/zstd-has-just-landed-in-blosc.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p><a class="reference external" href="http://www.zstd.net">Zstd</a>, aka Zstandard, is a new breed of compression
library that promises to achieve better compression ratios than <a class="reference external" href="http://www.zlib.net/">Zlib</a>, and at better speeds too. The fact that Zstd is geared
towards fast compression / decompression since the beginning was an indication
for me that it could be a good fit for Blosc. After some months of
experimentation with Zstd in <a class="reference external" href="https://github.com/Blosc/c-blosc2/pull/6">Blosc2</a>, I am really happy to say that I am
quite impressed on how the pair performs.</p>
<p>And now that the Zstd format has been declared <a class="reference external" href="http://fastcompression.blogspot.com.es/2016_07_03_archive.html">stable</a> and that its
API is maturing rapidly, it is a good time for inclusion in the <a class="reference external" href="https://github.com/Blosc/c-blosc/pull/171">Blosc1</a> project too. In Blosc1 there was
still a couple of slots available for additional codecs, and after my positive
experiences with Zstd I decided that it would be an excellent candidate to take
one of the free seats (will see which one will take the last one, if any).</p>
<p>Beware: the Zstd support in Blosc should still be considered in <strong>beta</strong>
and so it is not recommended to use this new codec in production yet. It is
indeed recommended to start experimenting with it so as to see the kind of
improvements that it can bring to your scenario, and specially report possible
flaws back.</p>
<section id="a-compression-beast-for-blosc-operation"><h2>A compression beast for Blosc operation</h2>
<p>As said, Zstd is meant to achieve better compression ratios than Zlib, and this
is indeed the <a class="reference external" href="http://www.zstd.net">case for many situations already</a>. But it
turns out that Zstd shines specially when faced to the kind of data that is left
after the shuffle (or bitshuffle) filter passes.</p>
<p>As for one, here it is the typical benchmark plot for compressing with Zstd on
my machine (Intel Xeon E3-1245-v5 @ 3.5GHz), side-by-side with Zlib which was
the codec having the best compression ratios among all the supported inside
Blosc:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lap-zstd-c" src="images/zstd-E3-1245-v5-4p-compr.png"></p></td>
<td><p><img alt="lap-zlib-c" src="images/zlib-E3-1245-v5-compr.png"></p></td>
</tr></tbody>
</table>
<p>As can be seen, Zstd achieves a maximum compression ratio of more than 300x for
this specific dataset, which is quite a lot more than the 70x achieved by Zlib.
But the coolest thing is that we are not paying a performance price for this
increased compression ratio, rather the contrary, because Zstd is clearly
superior (up to a 25%) in compression speed to Zlib.</p>
<p>But one of the most distinctive features for Blosc is its ability to decompress
data very fast (sometimes faster than memcpy() as I like to remind). And look at
what Zstd is able to achieve in this case:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lap-zstd-d" src="images/zstd-E3-1245-v5-4p.png"></p></td>
<td><p><img alt="lap-zlib-d" src="images/zlib-E3-1245-v5.png"></p></td>
</tr></tbody>
</table>
<p>With peak speeds larger than 10 GB/s, Zstd can decompress data more than 2x
faster than Zlib peaks (~ 4 GB/s). And more importantly, when it comes to
decompress data at the highest compression level, Zstd can do that about 6x
faster than Zlib (~6 GB/s vs ~1 GB/s), which is a welcome surprise.</p>
</section><section id="not-the-fastest-but-a-nicely-balanced-one"><h2>Not the fastest, but a nicely balanced one</h2>
<p>Of course, Zstd is still far from the fastest codecs in Blosc. See for example
how the internal BloscLZ codec can perform in this machine:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lap-blosclz-c" src="images/blosclz-E3-1245-v5-compr.png"></p></td>
<td><p><img alt="lap-blosclz-d" src="images/blosclz-E3-1245-v5.png"></p></td>
</tr></tbody>
</table>
<p>But nevertheless, due to its impressive balance between compression ratio and
speed, Zstd is called to be one of the most attractive codecs in Blosc for the
near future.</p>
<p>As always, all these benchmarks here were made for the specific, synthetic
dataset that I am using for Blosc since the beginning (mainly for
reproducibility purposes). But I am pretty sure that most of the capabilities
shown here will be experienced in a large variety of datasets that Blosc is
meant to tackle (in fact, it would be nice if you can share your experience by
adding a comment below).</p>
<p>Finally, my special thanks to Yann Collet, the author of Zstd (as well as <a class="reference external" href="http://www.lz4.org/">LZ4</a>, also included in Blosc) for putting his genius at the
service of the community by opening not only his code, but also his mind in his
amazing series of blogs about compression: <a class="reference external" href="http://fastcompression.blogspot.com">http://fastcompression.blogspot.com</a></p>
</section><section id="appendix-what-can-be-expected-in-blosc2"><h2>Appendix: What can be expected in Blosc2</h2>
<p>Blosc2 has support for Zstd contexts and
a new way to split chunks into blocks that makes codecs go faster in general.
Below you have a couple of plots on how the Blosc2/Zstd couple behaves:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="blosc2-zstd-c" src="images/zstd-E3-1245-v5-compr-blosc2.png"></p></td>
<td><p><img alt="blosc2-zstd-d" src="images/zstd-E3-1245-v5-blosc2.png"></p></td>
</tr></tbody>
</table>
<p>As can be seen, in Blosc2 Zstd can get peaks of more than 15 GB/s, almost reaching
memcpy() speed in this machine (~17 GB/s).  Also, decompression speed at the
highest compression ratio can scale when throwing more threads at it (a thing
that Blosc1 is not able to achieve), and easily surpasses 10 GB/s.  Notice
that reaching such a high speed while decompressing a buffer with a really high
compression ratio (~300x) is really impressing.  On his part, compression speed
is a bit less (25%) than in Blosc1 but still quite competitive (and on par with Zlib).</p>
<p>This is really exciting news to be added on top of the new planned features for Blosc2.</p>
</section>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/arm-is-becoming-a-first-class-citizen-for-blosc/" class="u-url">ARM is becoming a first-class citizen for Blosc</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/arm-is-becoming-a-first-class-citizen-for-blosc/" rel="bookmark">
            <time class="published dt-published" datetime="2015-09-09T11:32:20Z" itemprop="datePublished" title="2015-09-09 11:32">2015-09-09 11:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/arm-is-becoming-a-first-class-citizen-for-blosc/#disqus_thread" data-disqus-identifier="cache/posts/arm-is-becoming-a-first-class-citizen-for-blosc.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>We are happy to announce that Blosc is receiving official support for
ARM processors.  Blosc has always been meant to support all platforms
where a C89 compliant C compiler can be found, but until now the only
hardware platforms that we were testing on a regular basis has been
Intel (on top of Unix/Linux, Mac OSX and Windows).</p>
<p>We want this to change and the ARM architecture has been our first
candidate to become a fully supported platform besides Intel/AMD.  You
may be wondering that we could have chosen any other architecture like
MIPS or PowerPC, so why ARM?</p>
<section id="arm-is-eating-the-world"><h2>ARM is eating the world</h2>
<p>ARM is an increasingly popular architecture and we can find
implementation exemplars of it not only in the phones, tablets or
ChromeBooks, but also acting as embedded processors, as well as in
providing computing power to immensely popular Raspberry Pi's and
Arduinos and even environments so <em>apparently</em> alien to it like <a class="reference external" href="http://www.theplatform.net/2015/06/16/mont-blanc-sets-the-stage-for-arm-hpc/">High
Performance Computing</a>.</p>
<p>Contrarily to what has been traditional for other computer platforms,
one of the most important design features for ARM is to keep energy
consumption under very strict limits.  Nowadays, the ARM architecture
can run decently powerful CPUs where each core <a class="reference external" href="http://www.androidauthority.com/arms-secret-recipe-for-power-efficient-processing-409850">consumes just 600 to
750 mWatt or less</a>.</p>
<p>In my opinion, it is precisely this energy efficiency what makes of
ARM one of the platforms with more projection to gain ground as a
general computer platform in the short future.  By now, we all know
that ARM allows packing more cores into a single die (e.g. your phone
having more cores than your laptop, anyone?).  And more cores also
means more combined computing throughput (albeit a bit more difficult
to program), but more importantly, <strong>more cores being able to bring
data from memory at the same time</strong>.  Contrarily to what one might
think, having different threads transmitting data from RAM to the CPU
caches provides a better utilization of memory buses, and hence, a
much better global memory bandwidth.  This can be seen, for example,
in <a class="reference external" href="http://blosc.org/benchmarks-blosclz.html">typical Blosc benchmarks</a> by looking at how the
bandwidth grows with the number of threads in all the dots, but
specially where compression ratio equals 1 (i.e. no compression is
active, so Blosc is only doing a <em>memory copy</em> in this case).</p>
</section><section id="blosc-is-getting-ready-for-arm"><h2>Blosc is getting ready for ARM</h2>
<p>So ARM is cool indeed, but what we are doing for making it a
first-class citizen?  For starters, we have created a new <a class="reference external" href="https://github.com/Blosc/c-blosc2">C-Blosc2</a> repository that is going to act
as a playground for some time and where we are going to experiment
with a new range of features (those will be discussed in a later
post).  And this is exactly the place where we have already started
implementing a NEON version of the shuffle filter.</p>
<p><a class="reference external" href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dht0002a/BABIIFHA.html">NEON</a>
is an SIMD extension in the same spirit than SSE2 or AVX2 present in
Intel/AMD offerings.  NEON extension was introduced in ARMv7
architecture, and is present in most of the current high-end devices
(including most of the phones and tablets floating around, including
the new Raspberry Pi 2).  As many of you know, leveraging SIMD in
modern CPUs is key for allowing Blosc to be one of the fastest
compressors around, and if we wanted to be serious about ARM, NEON
support had to be here, <strong>period</strong>.</p>
<p>The new NEON implementation of shuffle for Blosc has been entirely
made by <a class="reference external" href="https://github.com/LucianMarc">Lucian Marc</a>, a summer
student that joined the project at the beginning of July 2015.  Lucian
did a terrific work on implementing the <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/blosc/shuffle-neon.c">shuffle filter NEON</a>,
and during the 2-months stage he did not only that, but he also had
time to do a preliminary version of the bitshuffle filter as well (not
completely functional yet, but as time allows, he plans to finish that).</p>
</section><section id="some-hints-on-the-measured-increase-in-performance"><h2>Some hints on the measured increase in performance</h2>
<p>So you might be asking, how fast can perform Blosc on an ARM with
NEON?  Well, let's start first by showing how fast it works on a
Raspberry Pi 2 (Broadcom BCM2836 ARMv7 Quad Core Processor) having
NEON and running Raspbian (gcc 4.7.2).  To not bore people, we are
going to show just decompression speeds:</p>
<img alt="/images/blosclz-shuffle-neon-rpi2.png" src="images/blosclz-shuffle-neon-rpi2.png"><p>It turns out that, when using the 4 cores and low compression levels,
Blosc with NEON support already shows evidence that it can equal the
performance of memcpy() on ARM.  This is an important fact because I
did not think that ARM performance was enough to allow Blosc doing
that already.  I was wrong.</p>
<p>Okay, so Blosc using NEON can be fast, but exactly how much when
compared to a <a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/blosc/shuffle-generic.h">shuffle implementation in pure C</a>?
Here you have the figures for the generic C shuffle:</p>
<img alt="/images/blosclz-shuffle-generic-rpi2.png" src="images/blosclz-shuffle-generic-rpi2.png"><p>That means that NEON can accelerate the whole decompression process
between 2x and 3x, which is pretty significant, and also speaks highly
about the quality of Lucian's NEON implementation.</p>
<p>Does that mean that we can extrapolate these figures for all ARM
processors out there?  Not quite.  In fact, the performance of a
Raspberry Pi 2 is quite mild compared with other boards.  So, let's
see what is the performance on a <a class="reference external" href="http://www.hardkernel.com/main/products/prdt_info.php?g_code=G140448267127">ODROID-XU3</a>
(although it has been replaced by <a class="reference external" href="http://www.hardkernel.com/main/products/prdt_info.php">ODROID-XU4</a>, the XU3 has
the same processor, so we are testing a pretty powerful CPU model
here).  This board comes with a Samsung Exynos5422 Cortex-A15 2.0 GHz
quad core and Cortex™-A7 quad core CPUs, so it is a representative of
the ARM Heterogeneous Multi-Processing solution (aka big.LITTLE).
Here are its figures:</p>
<img alt="/images/blosclz-shuffle-neon-odroid.png" src="images/blosclz-shuffle-neon-odroid.png"><p>So, the first thing to note is the memcpy() speed that at 1.6 GB/s,
is considerably faster than the RPi2 (&lt; 0.9 GB/s).  Yeah, this is a
much more capable board from a computational point of view.  The
second thing is that decompression speed <em>almost doubles the memcpy()
speed</em>.  Again, I was very impressed because I did not expect this
range of speeds <em>at all</em>.  ARM definitely is getting in a situation
where compression can be used for an advantage, computationally
speaking.</p>
<p>The third thing to note is a bit disappointing though: why only 3
threads appear in the plot?  Well, it turns out that the benchmark
suite fails miserably when using 4 threads or more.  As the Raspberry
setup does not suffer from this problem at all, I presume that this is
more related with the board or the libraries that come with the
operating system (Ubuntu 14.04).  This is rather unfortunate because I
was really curious to see such an ARMv7 8-core beast running at full
steam using the 8 threads.  At any rate, time will tell if the problem
is in the board or in Blosc itself.</p>
<p>Just to make the benchmarks a bit more complete, let me finish this
benchmark section showing the performance using the generic C code for
the shuffling algorithm:</p>
<img alt="/images/blosclz-shuffle-generic-odroid.png" src="images/blosclz-shuffle-generic-odroid.png"><p>If we compare with NEON figures for the ODROID board, we can see again
an increase in speed of between 2x and 4x, which is crazy amazing
(sorry if I seem a bit over-enthusiastic, but again, I was not really
prepared for seeing this).  Again, only figures for 2 threads are
in this plot because the benchmark crashes for 3 threads (this is
another hint that points to the fault being outside Blosc itself
and not in its NEON implementation of the shuffle filter).</p>
<p>At decompression speeds of 3 GB/s and ~ 2 Watt of energy consumption,
the ARM platform has one of the best bandwidth/Watt ratios that you can find
in the market, and this can have (and will have) profound implications
on how computations will be made in the short future (as the <a class="reference external" href="http://www.montblanc-project.eu/publications/energy-efficiency-high-performance-computing-mont-blanc-project">Mont
Blanc initiative is trying to demonstrate</a>).</p>
</section><section id="what-to-expect-from-arm-blosc-in-the-forthcoming-months"><h2>What to expect from ARM/Blosc in the forthcoming months</h2>
<p>This work on supporting ARM platforms is just the beginning.  As ARM
processors get more spread, and most specially, <a class="reference external" href="http://www.arm.com/products/processors/cortex-a/cortex-a72-processor.php">faster</a>,
we will need to refine the support for ARM in Blosc.</p>
<p>NEON support is only a part of the game, and things like efficient
handling of ARM heterogeneous architectures (<a class="reference external" href="https://en.wikipedia.org/wiki/ARM_big.LITTLE">big.LITTLE</a>) or making specific
tweaks for ARM cache sizes will be critical so as to make of ARM a
truly first-citizen for the Blosc ecosystem.</p>
<p>If you have ideas on what can be improved, and most specially <strong>how</strong>,
we want to learn from you :) If you want to contribute code to the
project, your pull requests are very welcome too!  If you like what we
are doing and want to see more of this, you can also <a class="reference external" href="http://blosc.org/blog/seeking-sponsoship.html">sponsor us</a>.</p>
</section>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/hairy-msvc-situation.rst/" class="u-url">Hairy situation of Microsoft Windows compilers</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/hairy-msvc-situation.rst/" rel="bookmark">
            <time class="published dt-published" datetime="2015-07-06T10:04:20Z" itemprop="datePublished" title="2015-07-06 10:04">2015-07-06 10:04</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/hairy-msvc-situation.rst/#disqus_thread" data-disqus-identifier="cache/posts/hairy-msvc-situation.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Recently -- and to the requirement of a customer who recently
<a class="reference external" href="http://blosc.org/blog/seeking-sponsoship.html">sponsorized us</a> -- I
struggled a lot trying to get the maximum performance out of Visual
Studio compilers.  Here there are some quick benchmarks to show you an
overview of the kind of performance that C-Blosc can reach on Windows.</p>
<p>First, let's use Visual Studio 2008 32-bit (extremely common platform
because Python 2 still requires this compiler) and see how C-Blosc
performs for decompressing on my laptop with Windows 7 Pro (64-bit)
with an Intel i5-3380M @ 2.90GHz:</p>
<img alt="/images/vs2008-32bit-decompress.png" src="images/vs2008-32bit-decompress.png"><p>Now, let us see how the same benchmark performs with Visual Studio
2013:</p>
<img alt="/images/vs2013-64bit-decompress.png" src="images/vs2013-64bit-decompress.png"><p>Well, there is an important boost in speed, not only because a native
64-bit compiler has been used, but also because natural improvements
in compiler technology.</p>
<p>At this point I wondered whether Visual Studio 2013 is doing just a
decent job or if there is still some performance that can still be
squeezed.  So what kind of performance other compilers for Windows are
reaching?  For checking this, I tested the excellent <a class="reference external" href="https://sourceforge.net/projects/mingw-w64">MinGW-w64</a> compiler (thanks to
Jack Pappas for suggesting this!).  Here it is the result:</p>
<img alt="/images/mingw-w64-64bit-decompress.png" src="images/mingw-w64-64bit-decompress.png"><p>So, one can be seen that GCC 4.9 (included in latest Mingw-w64) can
reach a performance that is still far beyond of what you can reach
with modern Microsoft compilers (specially for lower compression
levels, which is an important scenario when maximum speed is
required), and very close to what I get on Linux.</p>
<p>Possibly the newest Visual Studio 2015 would allow more performance,
but IMO, there is still some time until this is more spread, whereas
GCC 4.9 (with GCC 5.1 starting to show up) is already shipping in many
distributions, Windows and Mac OSX, which gives GCC a lot of advantage
with respect to Visual Studio.</p>
<p>With regards the reason on why GCC shows that much performance for
C-Blosc is probably a consequence of how it has been developed.  It
turns out that C-Blosc main development platform was (and still is)
Linux/GCC, and after many profile/optimize cycles, this tends to favor
that combination respect to others.</p>
<p>Provided this, and regarding the original request to reach optimal
performance on Windows / Visual Studio 2013 64-bit environments, I
ended implementing an example where existing Visual Studio
applications can dynamically link a C-Blosc DLL that is in the PATH.
You can see how this technique works at:
<a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/examples/win-dynamic-linking.c">https://github.com/Blosc/c-blosc/blob/master/examples/win-dynamic-linking.c</a></p>
<p>This is quite interesting because at compilation time you don't need
to make reference to the C-Blosc DLL <em>at all</em>.  I.e. the next is
enough for compiling the example above:</p>
<pre class="literal-block">cl /Ox /Fewin-dynamic-linking.exe /I..\blosc win-dynamic-linking.c</pre>
<p>And that's all.  After that, you only need to place the C-Blosc DLL
anywhere in your PATH and it will be dynamically detected.  I have
tested that with different combinations of compilers (e.g. Visual
Studio for the app, and MinGW-w64 for the DLL library) and it works
beautifully.  I think this is quite powerful and certainly I don't
know an equivalent technique for Unix (although it probably exists
also), allowing to use top-performance DLLs in your apps using
different compilers in a quite easy way.</p>
<p>In case you have more hints on how to get better performance on
Windows, please tell us.</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/new-bitshuffle-filter/" class="u-url">New 'bitshuffle' filter</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/new-bitshuffle-filter/" rel="bookmark">
            <time class="published dt-published" datetime="2015-07-05T15:24:20Z" itemprop="datePublished" title="2015-07-05 15:24">2015-07-05 15:24</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/new-bitshuffle-filter/#disqus_thread" data-disqus-identifier="cache/posts/new-bitshuffle-filter.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Although Blosc was meant for hosting more than one filter since day 0,
it has traditionally came with just a single filter, known as
'shuffle', meant for shuffling bytes in binary blocks.  Today this has
changed, and c-blosc has officially received a new filter called
'bitshuffle' (a backport of the one included in the
<a class="reference external" href="https://github.com/kiyo-masui/bitshuffle">bithsuffle project</a>).
And you guess it, it works in a very similar way than
'shuffle', just that the shuffling happens at the bit level and not at
the byte one.</p>
<p>Just for whetting your appetite here there are some small synthetic
benchmarks on what you can expect from the newcomer.  I'll start using
my own laptop (Intel i5-3380M @ 2.90GHz, GCC 4.9.1, Ubuntu 14.10) and
showing how the benchmark that comes with c-blosc performs with the
LZ4 compressor and the regular 'shuffle' filter:</p>
<pre class="literal-block">$ bench/bench lz4 shuffle single 2
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
List of supported compressors in this build: blosclz,lz4,lz4hc,snappy,zlib
Supported compression libraries:
  BloscLZ: 1.0.5
  LZ4: 1.7.0
  Snappy: 1.1.1
  Zlib: 1.2.8
Using compressor: lz4
Using shuffle type: shuffle
Running suite: single
--&gt; 2, 2097152, 8, 19, lz4, shuffle
********************** Run info ******************************
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
Using synthetic data with 19 significant bits (out of 32)
Dataset size: 2097152 bytes     Type size: 8 bytes
Working set: 256.0 MB           Number of threads: 2
********************** Running benchmarks *********************
memcpy(write):            529.1 us, 3780.3 MB/s
memcpy(read):             245.6 us, 8143.4 MB/s
Compression level: 0
comp(write):      267.3 us, 7483.3 MB/s   Final bytes: 2097168  Ratio: 1.00
decomp(read):     200.0 us, 9997.5 MB/s   OK
Compression level: 1
comp(write):      462.8 us, 4321.1 MB/s   Final bytes: 554512  Ratio: 3.78
decomp(read):     246.6 us, 8111.5 MB/s   OK
Compression level: 2
comp(write):      506.6 us, 3947.7 MB/s   Final bytes: 498960  Ratio: 4.20
decomp(read):     331.9 us, 6025.1 MB/s   OK
Compression level: 3
comp(write):      486.8 us, 4108.8 MB/s   Final bytes: 520824  Ratio: 4.03
decomp(read):     233.5 us, 8565.2 MB/s   OK
Compression level: 4
comp(write):      497.9 us, 4017.0 MB/s   Final bytes: 332112  Ratio: 6.31
decomp(read):     258.3 us, 7743.8 MB/s   OK
Compression level: 5
comp(write):      474.6 us, 4214.5 MB/s   Final bytes: 327112  Ratio: 6.41
decomp(read):     287.8 us, 6949.0 MB/s   OK
Compression level: 6
comp(write):      558.0 us, 3584.4 MB/s   Final bytes: 226308  Ratio: 9.27
decomp(read):     284.8 us, 7022.7 MB/s   OK
Compression level: 7
comp(write):      689.9 us, 2899.1 MB/s   Final bytes: 211880  Ratio: 9.90
decomp(read):     363.0 us, 5509.1 MB/s   OK
Compression level: 8
comp(write):      691.9 us, 2890.6 MB/s   Final bytes: 220464  Ratio: 9.51
decomp(read):     385.5 us, 5188.5 MB/s   OK
Compression level: 9
comp(write):      567.0 us, 3527.6 MB/s   Final bytes: 132154  Ratio: 15.87
decomp(read):     627.3 us, 3188.2 MB/s   OK

Round-trip compr/decompr on 7.5 GB
Elapsed time:       3.6 s, 4755.3 MB/s</pre>
<p>Now, look at what bitshuffle can do with the same datasets and compressor:</p>
<pre class="literal-block">$ bench/bench lz4 bitshuffle single 2
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
List of supported compressors in this build: blosclz,lz4,lz4hc,snappy,zlib
Supported compression libraries:
  BloscLZ: 1.0.5
  LZ4: 1.7.0
  Snappy: 1.1.1
  Zlib: 1.2.8
Using compressor: lz4
Using shuffle type: bitshuffle
Running suite: single
--&gt; 2, 2097152, 8, 19, lz4, bitshuffle
********************** Run info ******************************
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
Using synthetic data with 19 significant bits (out of 32)
Dataset size: 2097152 bytes     Type size: 8 bytes
Working set: 256.0 MB           Number of threads: 2
********************** Running benchmarks *********************
memcpy(write):            518.5 us, 3857.3 MB/s
memcpy(read):             248.6 us, 8045.7 MB/s
Compression level: 0
comp(write):      259.5 us, 7706.1 MB/s   Final bytes: 2097168  Ratio: 1.00
decomp(read):     217.5 us, 9196.3 MB/s   OK
Compression level: 1
comp(write):     1099.0 us, 1819.9 MB/s   Final bytes: 72624  Ratio: 28.88
decomp(read):     824.2 us, 2426.5 MB/s   OK
Compression level: 2
comp(write):     1093.2 us, 1829.5 MB/s   Final bytes: 71376  Ratio: 29.38
decomp(read):    1293.2 us, 1546.5 MB/s   OK
Compression level: 3
comp(write):     1084.5 us, 1844.2 MB/s   Final bytes: 69200  Ratio: 30.31
decomp(read):    1331.2 us, 1502.4 MB/s   OK
Compression level: 4
comp(write):     1193.2 us, 1676.2 MB/s   Final bytes: 42480  Ratio: 49.37
decomp(read):     833.8 us, 2398.7 MB/s   OK
Compression level: 5
comp(write):     1190.9 us, 1679.4 MB/s   Final bytes: 42928  Ratio: 48.85
decomp(read):     880.2 us, 2272.2 MB/s   OK
Compression level: 6
comp(write):      969.7 us, 2062.5 MB/s   Final bytes: 32000  Ratio: 65.54
decomp(read):     854.8 us, 2339.8 MB/s   OK
Compression level: 7
comp(write):     1056.2 us, 1893.6 MB/s   Final bytes: 40474  Ratio: 51.81
decomp(read):     960.8 us, 2081.7 MB/s   OK
Compression level: 8
comp(write):     1018.5 us, 1963.8 MB/s   Final bytes: 28050  Ratio: 74.76
decomp(read):     966.8 us, 2068.7 MB/s   OK
Compression level: 9
comp(write):     1161.7 us, 1721.6 MB/s   Final bytes: 25188  Ratio: 83.26
decomp(read):    1245.5 us, 1605.8 MB/s   OK

Round-trip compr/decompr on 7.5 GB
Elapsed time:       7.8 s, 2161.7 MB/s</pre>
<p>Amazing! the compression ratios are much higher (up to 83x vs 16x)
which is very exciting.  The drawback is that with 'bitshuffle' the
compression/decompression speed is between 2x and 4x slower than with
the regular 'shuffle'.  In fact, this slowdown is unusually light
because the additional work should be much more (1 byte has 8 bits),
so that's not too bad.</p>
<p>But we have some good news: besides SSE2, 'bitshuffle' also supports
AVX2 SIMD instructions (as 'shuffle' itself) but unfortunately my
laptop does not have them (pre-Haswell).  So let's run the benchmark
above in a AVX2 server (Intel Xeon E3-1240 v3 @ 3.40GHz, GCC 4.9.3,
Gentoo 2.2):</p>
<pre class="literal-block">$ bench/bench lz4 bitshuffle single 8
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
List of supported compressors in this build: blosclz,lz4,lz4hc,snappy,zlib
Supported compression libraries:
  BloscLZ: 1.0.5
  LZ4: 1.7.0
  Snappy: 1.1.1
  Zlib: 1.2.8
Using compressor: lz4
Using shuffle type: bitshuffle
Running suite: single
--&gt; 8, 2097152, 8, 19, lz4, bitshuffle
********************** Run info ******************************
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
Using synthetic data with 19 significant bits (out of 32)
Dataset size: 2097152 bytes     Type size: 8 bytes
Working set: 256.0 MB           Number of threads: 8
********************** Running benchmarks *********************
memcpy(write):            264.9 us, 7551.1 MB/s
memcpy(read):             174.1 us, 11488.6 MB/s
Compression level: 0
comp(write):      173.1 us, 11551.7 MB/s          Final bytes: 2097168  Ratio: 1.00
decomp(read):     119.3 us, 16765.2 MB/s          OK
Compression level: 1
comp(write):      271.8 us, 7358.1 MB/s   Final bytes: 72624  Ratio: 28.88
decomp(read):     225.7 us, 8862.7 MB/s   OK
Compression level: 2
comp(write):      275.7 us, 7253.7 MB/s   Final bytes: 71376  Ratio: 29.38
decomp(read):     229.2 us, 8724.8 MB/s   OK
Compression level: 3
comp(write):      274.5 us, 7285.9 MB/s   Final bytes: 69200  Ratio: 30.31
decomp(read):     238.8 us, 8374.6 MB/s   OK
Compression level: 4
comp(write):      249.5 us, 8015.5 MB/s   Final bytes: 42480  Ratio: 49.37
decomp(read):     229.8 us, 8701.6 MB/s   OK
Compression level: 5
comp(write):      249.1 us, 8028.1 MB/s   Final bytes: 42928  Ratio: 48.85
decomp(read):     243.9 us, 8198.8 MB/s   OK
Compression level: 6
comp(write):      332.4 us, 6017.5 MB/s   Final bytes: 32000  Ratio: 65.54
decomp(read):     322.2 us, 6206.4 MB/s   OK
Compression level: 7
comp(write):      431.9 us, 4630.2 MB/s   Final bytes: 40474  Ratio: 51.81
decomp(read):     437.6 us, 4570.7 MB/s   OK
Compression level: 8
comp(write):      421.5 us, 4745.0 MB/s   Final bytes: 28050  Ratio: 74.76
decomp(read):     437.2 us, 4574.5 MB/s   OK
Compression level: 9
comp(write):      941.1 us, 2125.2 MB/s   Final bytes: 25188  Ratio: 83.26
decomp(read):     674.7 us, 2964.2 MB/s   OK

Round-trip compr/decompr on 7.5 GB
Elapsed time:       2.8 s, 6047.8 MB/s</pre>
<p>Wow, in this case we are having compression speed peaks even higher
than a memcpy (8 GB/s vs 7.5 GB/s), and decompression speed is pretty
good too (8.8 GB/s vs 11.5 GB/s memcpy).  With AVX2 support,
'bitshuffle' does have a pretty good performance.  But yeah, this
server has 8 physical cores, so we are not actually comparing pears
with pears.  So let's re-run the benchmark with just 2 threads:</p>
<pre class="literal-block">$ bench/bench lz4 bitshuffle single 2
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
List of supported compressors in this build: blosclz,lz4,lz4hc,snappy,zlib
Supported compression libraries:
  BloscLZ: 1.0.5
  LZ4: 1.7.0
  Snappy: 1.1.1
  Zlib: 1.2.8
Using compressor: lz4
Using shuffle type: bitshuffle
Running suite: single
--&gt; 2, 2097152, 8, 19, lz4, bitshuffle
********************** Run info ******************************
Blosc version: 1.7.0.dev ($Date:: 2015-05-27 #$)
Using synthetic data with 19 significant bits (out of 32)
Dataset size: 2097152 bytes     Type size: 8 bytes
Working set: 256.0 MB           Number of threads: 2
********************** Running benchmarks *********************
memcpy(write):            253.9 us, 7877.5 MB/s
memcpy(read):             174.1 us, 11488.8 MB/s
Compression level: 0
comp(write):      133.4 us, 14995.6 MB/s          Final bytes: 2097168  Ratio: 1.00
decomp(read):     117.5 us, 17026.6 MB/s          OK
Compression level: 1
comp(write):      604.1 us, 3310.7 MB/s   Final bytes: 72624  Ratio: 28.88
decomp(read):     431.2 us, 4638.3 MB/s   OK
Compression level: 2
comp(write):      624.3 us, 3203.5 MB/s   Final bytes: 71376  Ratio: 29.38
decomp(read):     452.3 us, 4421.5 MB/s   OK
Compression level: 3
comp(write):      623.7 us, 3206.8 MB/s   Final bytes: 69200  Ratio: 30.31
decomp(read):     442.3 us, 4521.9 MB/s   OK
Compression level: 4
comp(write):      585.2 us, 3417.6 MB/s   Final bytes: 42480  Ratio: 49.37
decomp(read):     395.3 us, 5058.9 MB/s   OK
Compression level: 5
comp(write):      530.0 us, 3773.4 MB/s   Final bytes: 42928  Ratio: 48.85
decomp(read):     400.5 us, 4994.0 MB/s   OK
Compression level: 6
comp(write):      542.6 us, 3686.0 MB/s   Final bytes: 32000  Ratio: 65.54
decomp(read):     426.7 us, 4687.2 MB/s   OK
Compression level: 7
comp(write):      605.6 us, 3302.4 MB/s   Final bytes: 40474  Ratio: 51.81
decomp(read):     494.5 us, 4044.5 MB/s   OK
Compression level: 8
comp(write):      588.1 us, 3400.7 MB/s   Final bytes: 28050  Ratio: 74.76
decomp(read):     487.3 us, 4104.6 MB/s   OK
Compression level: 9
comp(write):      692.5 us, 2888.2 MB/s   Final bytes: 25188  Ratio: 83.26
decomp(read):     591.4 us, 3381.8 MB/s   OK

Round-trip compr/decompr on 7.5 GB
Elapsed time:       3.9 s, 4294.1 MB/s</pre>
<p>Now, for 2 threads we are getting times that are about 2x slower than
for 8 threads.  But the interesting thing here is that the compression
speed is still ~2x faster than my laptop (peaks at 3.7 GB/s vs 1.8
GB/s) and the same goes for decompression (peaks at 5 GB/s vs 2.4
GB/s).  Agreed, the server can run at 3.4GHz vs 2.9 GHz of my laptop,
but this alone cannot explain the difference in speed, so the big
responsible for the speedup is the AVX2 support in 'bitshuffle'.</p>
<p>In summary, the new 'bitshuffle' filter is very good news for the
users of the Blosc ecosystem because it adds yet another powerful
resource that will help in the fight for storing datasets with less
space, but still keeping good performance.  Of course, this is just a
quick experiment with synthetic data, but I am pretty sure that the
new 'bitshuffle' filter will find a good niche in real world datasets.
Anyone interested in contributing some real data benchmark?</p>
<p>I'd like to thank Kiyo Masui for his help in this 'bitshuffle' backport.</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/seeking-sponsoship/" class="u-url">Seeking Sponsorship for Bcolz/Blosc</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/valentin-haenel/">Valentin Haenel</a>
            </span></p>
            <p class="dateline">
            <a href="posts/seeking-sponsoship/" rel="bookmark">
            <time class="published dt-published" datetime="2015-05-26T08:41:20Z" itemprop="datePublished" title="2015-05-26 08:41">2015-05-26 08:41</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/seeking-sponsoship/#disqus_thread" data-disqus-identifier="cache/posts/seeking-sponsorship.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Dear Everyone,</p>
<p>as you may or may not know, the <a class="reference external" href="https://github.com/blosc/c-blosc">Blosc</a>
compressor has become the basis for some novel, innovative technological
experiments in the PyData space.  Especially the <a class="reference external" href="https://github.com/blosc/bcolz">Bcolz</a> and <a class="reference external" href="https://github.com/blosc/bloscpack">Bloscpack</a> projects which provide a way to perform
out-of-core computations on column based datasets have become particularly
interesting for the analysis of medium-sized time-series datasets.</p>
<p>In this post, we would like to convince you to give us some money to
foster the project, development and accelerate growth of our community.
Historically, it has always been a difficult endeavour to monetize
open-source development and so, below is a non-exhaustive list of
potential models that we are considering:</p>
<ul>
<li>
<p>Direct sponsoring / Donations</p>
<p>This involves paying either a single lump-sum or monthly installments
to foster continued development and innovation. This type of
sponsoring isn't bound to any specific goal or feature and would allow
us for example maintain and release the projects regularly.</p>
</li>
<li>
<p>Feature-driven sponsoring</p>
<p>Paying for specific features to be implemented, bugs to be fixed or
paying to have a voice when it comes to prioritizing items in the
issue-tracker(s).</p>
</li>
<li>
<p>Hiring us as freelancers for Blosc/Bcolz projects</p>
<p>This means that you hire one or both of us to implement a project that
uses bcolz inside your company. Any bugs we find or improvements that
need to be made would flow back into the open source code-base.</p>
</li>
<li>
<p>Hiring us as part-time freelancers for general projects</p>
<p>This means you hire one or both of us as part-time freelancers for two
to three days a week to work on general projects. These can be related
to Python and data or open-source work on other projects. This would
allow us to spend the remaining days on Blosc/Bcolz.</p>
</li>
<li>
<p>PhD positions</p>
<p>There are still a few interesting theoretical aspects to be unlocked,
for example certain mathematical properties of the shuffle filter and
a compressed extension of the external-memory-model (EMM) to analyse
the runtime of Blosc style out-of-core algorithms and Bcolz operations
in general.</p>
</li>
</ul>
<p>We welcome any feedback regarding the above options and please do tell
us about any additional models that may be interesting to us or for you.</p>
<p>With best wishes and looking forward to your input,</p>
<p>Francesc Alted and Valentin Haenel</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/compress-me-stupid/" class="u-url">Compress Me, Stupid!</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/compress-me-stupid/" rel="bookmark">
            <time class="published dt-published" datetime="2014-08-28T17:01:20Z" itemprop="datePublished" title="2014-08-28 17:01">2014-08-28 17:01</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/compress-me-stupid/#disqus_thread" data-disqus-identifier="cache/posts/compress-me-stupid.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<section id="how-it-all-started"><h2>How it all started</h2>
<p>I think I began to become truly interested in compression when, back
in 1992, I was installing <a class="reference external" href="http://en.wikipedia.org/wiki/C_News">C-News</a>, a news server package meant
to handle <a class="reference external" href="http://en.wikipedia.org/wiki/Usenet">Usenet News</a>
articles in <a class="reference external" href="http://www.uji.es">our university</a>.  For younger
audiences, Usenet News was a very popular way to discuss about all
kind of topics, but at the same time it was pretty difficult to cope
with the huge amount of articles, specially because spam practices
started to appear by that time.  As Gene Spafford put it in 1992:</p>
<blockquote>
<p>"Usenet is like a herd of performing elephants with
diarrhea. Massive, difficult to redirect, awe-inspiring,
entertaining, and a source of mind-boggling amounts of excrement
when you least expect it."</p>
</blockquote>
<p>But one thing was clear: Usenet brought <strong>massive</strong> amounts of data
that had to be transmitted through the typical low-bandwidth data
lines of that time: 64 Kbps shared for everyone at our university.</p>
<p>My mission then was to bring the Usenet News feed by making use of as
low of resources as possible.  Of course, one of the first things that
I did was to start news transmission during the night, when everyone
was warm at bed and nobody was going to complain about others stealing
the precious and scarce Internet bandwidth.  Another measure was to
subscribe to just a selection of groups so that the transmission would
end before the new day would start.  And of course, I started
experimenting with compression for maximizing the number of groups
that we could bring to our community.</p>
</section><section id="compressing-usenet-news"><h2>Compressing Usenet News</h2>
<p>The most used compressor by 1992 was <a class="reference external" href="http://en.wikipedia.org/wiki/Compress">compress</a>, a Unix program based on the
<a class="reference external" href="http://en.wikipedia.org/wiki/LZW">LZW</a> compression algorithm.  But
LZW had patents issues, so by that time Jean-Loup Gailly and Mark
Adler started the work with <a class="reference external" href="http://en.wikipedia.org/wiki/Gzip">gzip</a>.  At the beginning of 1993 gzip
1.0 was ready for consumption and I find it exciting not only because
it was not patent-encumbered, but also because it compressed way
better than the previous <code class="docutils literal">compress</code> program, allowed different
compression levels, and it was pretty fast too (although <code class="docutils literal">compress</code>
still had an advantage here, IIRC).</p>
<p>So I talked with <a class="reference external" href="http://www.uv.es">the university</a> that was
providing us with the News feed and we manage to start compressing it,
first with <code class="docutils literal">compress</code> and then with <code class="docutils literal">gzip</code>.  Shortly after that,
while making measurements on the new gzip improvements, I discovered
that the bottleneck was in our News workstation (an HP 9000-730 with a
speedy <a class="reference external" href="http://en.wikipedia.org/wiki/PA-RISC">PA-7000 RISC microprocessor</a> @ 66 MHz) being unable to
decompress all the gzipped stream of subscribed news on-time.  The
bottleneck suddenly changed from the communication line to the CPU!</p>
<p>I remember spending large hours playing with different combinations of
data chunk sizes and gzip compression levels, plotting the results
(with the fine <a class="reference external" href="http://en.wikipedia.org/wiki/Gnuplot">gnuplot</a>)
before finally coming with a combination that stroked a fair balance
between available bandwidth and CPU speed, maximizing the amount of
news articles hitting our university.  I think this was my first
realization of how compression could help bringing data faster to the
system, making some processes more effective.  In fact, that actually
blew-up my mind and made me passionate about compression technologies
for the years to come.</p>
</section><section id="lzo-and-the-explosion-of-compression-technology"><h2>LZO and the explosion of compression technology</h2>
<p>During 1996, Markus F.X.J. Oberhumer started to announce the
availability of his own set of LZO compressors.  These consisted in
many different compressors, all of them being variations of his own
compression algorithm (LZO), but tweaked to achieve either better
compression ratios or compression speed.  The suite was claimed to
being able to achieve speeds reaching <strong>1/3 of the memory speed</strong> of
the typical Pentium-class computers available at that time.  An entire
set of compressors being able to approach memory speed? boy, that was
a very exciting news for me.</p>
<p>LZO was in the back of my mind when I started my work on <a class="reference external" href="http://www.pytables.org">PyTables</a> in August 2002 and shortly after, in <a class="reference external" href="http://pytables.org/svn/pytables/tags/std-0.5/README.txt">May
2003</a>,
PyTables gained support for LZO.  My goal was indeed to accelerate
data transmission from disk to the CPU (and back), and <a class="reference external" href="http://pytables.github.io/usersguide/optimization.html#understanding-chunking">these plots</a>
are testimonial of how beneficial LZO was for achieving that goal.
Again, compression was demonstrating that it could effectively
increase disk bandwidth, and not only slow internet lines.</p>
<p>However, although LZO was free of patent issues and fast as hell,
it had a big limitation for a project like PyTables: the licensing.
LZO was using the GPL license, and that prevented the inclusion of its
sources in distributions without re-licensing PyTables itself as GPL,
a thing that I was not willing to do (PyTables has a BSD license, as
it is usual in the NumPy ecosystem).  Because of that, LZO was a nice
compressor to be included in GPL projects like the Linux kernel
itself, but not a good fit for PyTables (although support for LZO still
exists, as long as it is downloaded and installed separately).</p>
<p>By that time (mid 2000's) it started to appear a plethora of fast
compressors with the same spirit than LZO, but with more permissive
licenses (typically BSD/MIT), many of them being a nice fit for PyTables.</p>
</section><section id="a-new-compressor-for-pytables-hdf5"><h2>A new compressor for PyTables/HDF5</h2>
<p>By 2008 it was clear that PyTables needed a compressor whose sources
could be included in the PyTables tarball, so minimizing the
installation requirements.  For this I started considering a series of
libraries and immediately devised <a class="reference external" href="http://fastlz.org/">FastLZ</a> as a
nice candidate because of its simplicity and performance.  Also,
FastLZ had a permissive MIT license, which was what I was looking for.</p>
<p>But pure FastLZ was not completely satisfactory because it was not
simple enough.  It had 2 compression levels that
complicated the implementation quite a bit, so I decided to keep just the
highest level, and then optimize certain parts of it so that speed
would be acceptable.  These modifications gave birth to BloscLZ, which
is still being default compressor in Blosc.</p>
<p>But I had more ideas on what other features the new Blosc compressor
should have, namely, multi-threading and an integrated shuffle filter.
Multi-threading made a lot of sense by 2008 because both Intel and AMD
already had a wide range of multi-core processors by then, and it was
clear that the race for throwing more and more cores into systems was
going to intensify.  A fast compressor had to be able to use all these
cores dancing around, <strong>period</strong>.</p>
<p>Shuffle (see slide 71 of this <a class="reference external" href="http://blosc.org/docs/StarvingCPUs.pdf">presentation</a>) was the other important
component of the new compressor.  This algorithm relies on
neighboring elements of a dataset being highly correlated to improve
data compression.  A shuffle filter already came as part of the <a class="reference external" href="http://www.hdfgroup.org/HDF5/">HDF5
library</a> but it was implemented in
pure C, and as it had an important overhead in terms of computation, I
decided to do an <a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/blosc/shuffle.c">SIMD version</a> using
the powerful <a class="reference external" href="http://en.wikipedia.org/wiki/SSE2">SSE2 instructions</a>
present in all Intel and AMD processors since 2003.  The result is
that this new shuffle implementation adds almost zero overhead
compared with the compression/decompression stages.</p>
<p>Once all of these features were implemented, I designed a pretty
comprehensive <a class="reference external" href="http://blosc.org/synthetic-benchmarks.html">suite of tests</a> and asked the PyTables
community to help me testing the new compressor in as much systems as
possible.  After some iterations, we were happy when the new
compressor worked flawlessly compressing and decompressing <strong>hundreds
of terabytes</strong> on many different Windows and Unix boxes, both in
32-bit and 64-bit.  The new beast was ready to ship.</p>
</section><section id="blosc-was-born"><h2>Blosc was born</h2>
<p>I then grabbed BloscLZ, the multi-threading support and the
SSE2-powered shuffle and put it all in the same package.  That also became a
<strong>standalone, pure C library</strong>, with no attachments to PyTables or HDF5,
so any application could make
use of it.  I have got the first stable version (1.0) of Blosc
released by <a class="reference external" href="http://www.groupsrv.com/science/about538609.html">July 2010</a>.
Before this, I already introduced Blosc publicly in my <a class="reference external" href="http://www.blosc.org/docs/StarvingCPUs.pdf">EuroSciPy 2009 keynote</a> and also made a small
reference to it in an article about <a class="reference external" href="http://www.blosc.org/docs/StarvingCPUs-CISE-2010.pdf">Starving CPUs</a> where I
stated:</p>
<blockquote>
<p>"As the gap between CPU and memory speed continues to widen, I
expect Blosc to improve memory-to-CPU data transmission rates over
an increasing range of datasets."</p>
</blockquote>
<p>And that is the thing.  As CPUs are getting faster, the chances for
using compression for an advantage can be applied to more and more
scenarios, to the point that improving the bandwidth of main memory
(RAM) is becoming possible now.  And surprisingly enough, the methodology
for achieving that is the same than back in the C-news ages: strike a good
balance between data block sizes and compression speed, and let
compression make your applications handle data faster and not only
making it more compact.</p>
<p>When seen in perspective, it has been a long quest over the last
decades.  During the 90's, compression was useful to improve the
bandwidth of slow internet connections.  In the 2000's, it made
possible accelerating disk I/O operation.  In the 2010's Blosc goal is
making the memory subsystem faster and whether it is able to
achieve this or not will be the subject of future blogs (hint: data
arrangement is critical too).  But one
thing is clear, achieving this (by Blosc or any other compressor out
there) is just a matter of time.  Such is the fate of the ever
increasing gap in CPU versus memory speeds.</p>
</section>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-2.html" rel="prev">Newer posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blosc";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2021         <a href="mailto:blosc@blosc.org">The Blosc Developers</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
