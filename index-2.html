<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Blosc, an extremely fast, multi-threaded, meta-compressor library">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Blosc Main Blog Page  (old posts, page 2) | Blosc Main Blog Page </title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="rss.xml">
<link rel="canonical" href="http://blosc.org/index-2.html">
<link rel="icon" href="blosc-favicon_16x16.png" sizes="16x16">
<link rel="icon" href="blosc-favicon_32x32.png" sizes="32x32">
<link rel="icon" href="blosc-favicon_64x64.png" sizes="64x64">
<link rel="icon" href="blosc-favicon_128x128.png" sizes="128x128">
<link rel="prev" href="." type="text/html">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-111342564-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111342564-2');
</script>
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://blosc.org/">
                <img src="blosc-logo_128.png" alt="Blosc Main Blog Page " id="logo"></a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="pages/blosc-in-depth/">Blosc In Depth</a>
                </li>
<li>
<a href="pages/synthetic-benchmarks/">Benchmarks</a>
                </li>
<li>
<a href="pages/donate/">Donate to Blosc</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/breaking-memory-walls/" class="u-url">Breaking Down Memory Walls</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/breaking-memory-walls/" rel="bookmark">
            <time class="published dt-published" datetime="2018-06-25T18:32:20Z" itemprop="datePublished" title="2018-06-25 18:32">2018-06-25 18:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/breaking-memory-walls/#disqus_thread" data-disqus-identifier="cache/posts/breaking-down-memory-walls.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p><strong>Update (2018-08-09)</strong>: An extended version of this blog post can be found in this <a class="reference external" href="http://www.blosc.org/docs/Breaking-Down-Memory-Walls.pdf">article</a>.  On it, you will find a complementary study with synthetic data (mainly for finding ultimate performance limits), a more comprehensive set of CPUs has been used, as well as more discussion about the results.</p>
<p>Nowadays CPUs struggle to get data at enough speed to feed their cores.  The reason for this is that memory speed is <a class="reference external" href="http://www.blosc.org/docs/StarvingCPUs-CISE-2010.pdf">growing at a slower pace than CPUs increase their speed at crunching numbers</a>.   This memory slowness compared with CPUs is generally known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall">Memory Wall</a>.</p>
<p>For example, let's suppose that we want to compute the aggregation of a some large array; here it is how to do that using OpenMP for leveraging all cores in a CPU:</p>
<pre class="code c"><a id="rest_code_506a4048a7a14391b3edc1418062b3a6-1" name="rest_code_506a4048a7a14391b3edc1418062b3a6-1"></a><span class="cp">#pragma omp parallel for reduction (+:sum)</span>
<a id="rest_code_506a4048a7a14391b3edc1418062b3a6-2" name="rest_code_506a4048a7a14391b3edc1418062b3a6-2"></a><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
<a id="rest_code_506a4048a7a14391b3edc1418062b3a6-3" name="rest_code_506a4048a7a14391b3edc1418062b3a6-3"></a>  <span class="n">sum</span> <span class="o">+=</span> <span class="n">udata</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<a id="rest_code_506a4048a7a14391b3edc1418062b3a6-4" name="rest_code_506a4048a7a14391b3edc1418062b3a6-4"></a><span class="p">}</span>
</pre>
<p>With this, some server (an Intel Xeon E3-1245 v5 @ 3.50GHz, with 4 physical cores and hyperthreading) takes about 14 ms for doing the aggregation of an array with 100 million of float32 values when using 8 OpenMP threads (optimal number for this CPU).  However, if instead of bringing the whole 100 million elements from memory to the CPU we generate the data inside the loop, we are avoiding the data transmission between memory and CPU, like in:</p>
<pre class="code c"><a id="rest_code_3ed1430cb1d74b589d5394be6e62bef1-1" name="rest_code_3ed1430cb1d74b589d5394be6e62bef1-1"></a><span class="cp">#pragma omp parallel for reduction (+:sum)</span>
<a id="rest_code_3ed1430cb1d74b589d5394be6e62bef1-2" name="rest_code_3ed1430cb1d74b589d5394be6e62bef1-2"></a><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
<a id="rest_code_3ed1430cb1d74b589d5394be6e62bef1-3" name="rest_code_3ed1430cb1d74b589d5394be6e62bef1-3"></a>  <span class="n">sum</span> <span class="o">+=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">i</span><span class="p">;</span>
<a id="rest_code_3ed1430cb1d74b589d5394be6e62bef1-4" name="rest_code_3ed1430cb1d74b589d5394be6e62bef1-4"></a><span class="p">}</span>
</pre>
<p>This loop takes just 3.5 ms, that is, 4x less than the original one.  That means that our CPU could compute the aggregation at a speed that is 4x faster than the speed at which the memory subsystem can provide data elements to the CPU; or put in another words, the CPU is idle, doing nothing during the 75% of the time, waiting for data to arrive (for this example, but there could be other, more extreme cases).  Here we have the memory wall in action indeed.</p>
<p>That the memory wall exists is an excellent reason to think about ways to workaround it.  One of the most promising venues is to use compression: what if we could store data in compressed state in-memory and use the spare clock cycles of the CPU for decompressing it just when it is needed?  In this blog entry we will see how to implement such a computational kernel on top of data structures that are cache- and compression-friendly and we will examine how they perform on a range of modern CPU architectures.  Some surprises are in store.</p>
<p>For demonstration purposes, I will run a simple task: summing up the same array of values than above but using a <em>compressed</em> dataset instead.  While computing sums of values seems trivial, it exposes a couple of properties that are important for our discussion:</p>
<ol class="arabic simple">
<li><p>This is a memory-bounded task.</p></li>
<li><p>It is representative of many aggregation/reduction algorithms that are routinely used out in the wild.</p></li>
</ol>
<section id="operating-with-compressed-datasets"><h2>Operating with Compressed Datasets</h2>
<p>Now let's see how to run our aggregation efficiently when using compressed data.  For this, we need:</p>
<ol class="arabic simple">
<li><p>A data container that supports on-the-flight compression.</p></li>
<li><p>A blocking algorithm that leverages the caches in CPUs.</p></li>
</ol>
<p>As for the data container, we are going to use the <em>super-chunk</em> object that comes with the Blosc2 library.  A super-chunk is a data structure that is meant to host many data chunks in a compressed form, and that has some interesting features; more specifically:</p>
<ul class="simple">
<li><p><strong>Compactness</strong>: everything in a super-chunk is designed to take as little space as possible, not only by using compression, but also my minimizing the amount of associated metadata (like indexes).</p></li>
<li><p><strong>Small fragmentation</strong>: by splitting the data in large enough chunks that are contiguous, the resulting structure ends stored in memory with a pretty small amount of 'holes' in it, allowing a more efficient memory management by both the hardware and the software.</p></li>
<li><p><strong>Support for contexts</strong>: useful when we have different threads and we want to decompress data simultaneously.  Assigning a context per each thread is enough to allow the simultaneous use of the different cores without badly interfering with each other.</p></li>
<li><p><strong>Easy access to chunks</strong>: an integer is assigned to the different chunks so that requesting a specific chunk is just a matter of specifying its number and then it gets decompressed and returned in one shot.  So pointer arithmetic is replaced by indexing operations, making the code less prone to get severe errors (e.g. if a chunk does not exist, an error code is returned instead of creating a segmentation fault).</p></li>
</ul>
<p>If you are curious on how the super-chunk can be created and used, just check the <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/sum_openmp.c#L144-L157">sources for the benchmark</a> used for this blog.</p>
<p>Regarding the computing algorithm, I will use one that follows the principles of the blocking computing technique:  for every chunk, bring it to the CPU, decompress it (so that it stays in cache), run all the necessary operations on it, and then proceed to the next chunk:</p>
<img alt="/images/breaking-down-memory-walls/blocking-technique.png" class="align-center" src="images/breaking-down-memory-walls/blocking-technique.png"><p>For implementation details, have a look at the <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/sum_openmp.c#L191-L209">benchmark sources</a>.</p>
<p>Also, and in order to allow maximum efficiency when performing multi-threaded operations, the size of each chunk in the super-chunk should fit in non-shared caches (namely, L1 and L2 in modern CPUs).  This optimization avoids concurrent access to bus caches as much as possible, thereby allowing dedicated access to data caches in each core.</p>
<p>For our experiments below, we are going to choose a chunksize of 4,000 elements because Blosc2 needs 2 internal buffers for performing the decompression besides the source and destination buffer.  Also, we are using 32-bit (4 bytes) float values for our exercise, so the final size used in caches will be 4,000 * (2 + 2) * 4 = 64,000 bytes, which should fit comfortably in L2 caches in most modern CPU architectures (which normally sports 256 KB or even higher).  Please note that finding an optimal value for this size might require some fine-tuning, not only for different architectures, but also for different datasets.</p>
</section><section id="the-precipitation-dataset"><h2>The Precipitation Dataset</h2>
<p>There are plenty of datasets out there exposing different data distributions so, depending on your scenario, your mileage may vary.  The dataset chosen here is the result of a <a class="reference external" href="http://reanalysis.meteo.uni-bonn.de">regional reanalysis covering the European continent</a>, and in particular, the precipitation data in a certain region of Europe.  Computing the aggregation of this data is representative of a catchment average of precipitation over a drainage area.</p>
<p><em>Caveat</em>: For the sake of easy reproducibility, for building the 100 million dataset I have chosen a small <a class="reference external" href="https://github.com/Blosc/c-blosc2/blob/master/bench/read-grid-150x150.py">geographical area with a size of 150x150</a> and reused it repeatedly so as to fill the final dataset completely.  As the size of the chunks is lesser than this area, and the super-chunk (as configured here) does not use data redundancies from other chunks, the results obtained here can be safely extrapolated to the actual dataset made from real data (bar some small differences).</p>
</section><section id="choosing-the-compression-codec"><h2>Choosing the Compression Codec</h2>
<p>When determining the best codec to use inside Blosc2 (it has support for BloscLZ, LZ4, LZ4HC, Zstd, Zlib and Lizard), it turns out that they behave quite differently, both in terms of compression and speed, with the dataset they have to compress <em>and</em> with the CPU architecture in which they run.  This is quite usual, and the reason why you should always try to find the best codec for your use case.  Here we have how the different codecs behaves for our precipitation dataset in terms of decompression speed for our reference platform (Intel Xeon E3-1245):</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="i7server-codecs" src="images/breaking-down-memory-walls/i7server-rainfall-codecs.png"></p></td>
<td><p><img alt="rainfall-cr" src="images/breaking-down-memory-walls/rainfall-cr.png"></p></td>
</tr></tbody>
</table>
<p>In this case LZ4HC is the codec that decompress faster for any number of threads and hence, the one selected for the benchmarks for the reference platform.  A similar procedure has been followed to select the codec for the CPUs.  The selected codec for every CPU will be conveniently specified in the discussion of the results below.</p>
<p>For completeness, I am also showing the compression ratios achieved by the different codecs for the precipitation dataset.  Although there are significant differences for them, these usually come at the cost of compression/decompression time.  At any rate, even though compression ratio is important, in this blog we are mainly interested in the best decompression speed, so we will use this latter as the only important parameter for codec selection.</p>
</section><section id="results-on-different-cpus"><h2>Results on Different CPUs</h2>
<p>Now it is time to see how our compressed sum algorithm performs compared with the original uncompressed one.  However, as not all the CPUs are created equal, we are going to see how different CPUs perform doing exactly the same computation.</p>
<section id="reference-cpu-intel-xeon-e3-1245-v5-4-core-processor-3-50ghz"><h3>Reference CPU: Intel Xeon E3-1245 v5 4-Core processor @ 3.50GHz</h3>
<p>This is a mainstream, somewhat 'small' processor for servers that has an excellent price/performance ratio.  Its main virtue is that, due to its small core count, the CPU can be run at considerably high clock speeds which, combined with a high IPC (Instructions Per Clock) count, delivers considerable computational power.  These results are a good baseline reference point for comparing other CPUs packing a larger number of cores (and hence, lower clock speeds).  Here it is how it performs:</p>
<img alt="/images/breaking-down-memory-walls/i7server-rainfall-lz4hc-9.png" class="align-center" src="images/breaking-down-memory-walls/i7server-rainfall-lz4hc-9.png"><p>We see here that, even though the uncompressed dataset does not scale too well, the compressed dataset shows a nice scalability even when using using hyperthreading (&gt; 4 threads); this is a remarkable fact for a feature (hyperthreading) that, despite marketing promises, does not always deliver 2x the performance of the physical cores.  With that, the performance peak for the compressed precipitation dataset (22 GB/s, using LZ4HC) is really close to the uncompressed one (27 GB/s); quite an achievement for a CPU with just 4 physical cores.</p>
</section><section id="amd-epyc-7401p-24-core-processor-2-0ghz"><h3>AMD EPYC 7401P 24-Core Processor @ 2.0GHz</h3>
<p>This CPU implements EPYC, one of the most powerful architectures ever created by AMD.  It packs 24 physical cores, although internally they are split into 2 blocks with 12 cores each.  Here is how it behaves:</p>
<img alt="/images/breaking-down-memory-walls/epyc-rainfall-lz4-9.png" class="align-center" src="images/breaking-down-memory-walls/epyc-rainfall-lz4-9.png"><p>Stalling at 4/8 threads, the EPYC scalability for the uncompressed dataset is definitely not good.  On its hand, the compressed dataset behaves quite differently: it shows a nice scalability through the whole range of cores in the CPU (again, even when using hyperthreading), achieving the best performance (45 GB/s, using LZ4) at precisely 48 threads, well above the maximum performance reached by the uncompressed dataset (30 GB/s).</p>
</section><section id="intel-scalable-gold-5120-2x-14-core-processor-2-2ghz"><h3>Intel Scalable Gold 5120 2x 14-Core Processor @ 2.2GHz</h3>
<p>Here we have one of the latest and most powerful CPU architectures developed by Intel.  We are testing it here within a machine with 2 CPUs, each containing 14 cores.  Here’s it how it performed:</p>
<img alt="/images/breaking-down-memory-walls/scalable-rainfall-lz4-9.png" class="align-center" src="images/breaking-down-memory-walls/scalable-rainfall-lz4-9.png"><p>In this case, and stalling at 24/28 threads, the Intel Scalable shows a quite remarkable scalability for the uncompressed dataset (apparently, Intel has finally chosen a good name for an architecture; well done guys!).  More importantly, it also reveals an even nicer scalability on the compressed dataset, all the way up to 56 threads (which is expected provided the 2x 14-core CPUs with hyperthreading); this is a remarkable feat for such a memory bandwidth beast.  In absolute terms, the compressed dataset achieves a performance (68 GB/s, using LZ4) that is very close to the uncompressed one (72 GB/s).</p>
</section><section id="cavium-armv8-2x-48-core"><h3>Cavium ARMv8 2x 48-Core</h3>
<p>We are used to seeing ARM architectures powering most of our phones and tablets, but seeing them performing computational duties is far more uncommon.  This does not mean that there are not ARM implementations that cannot power big servers.  Cavium, with its 48-core in a single CPU, is an example of a server-grade chip.  In this case we are looking at a machine with two of these CPUs:</p>
<img alt="/images/breaking-down-memory-walls/cavium-rainfall-blosclz-9.png" class="align-center" src="images/breaking-down-memory-walls/cavium-rainfall-blosclz-9.png"><p>Again, we see a nice scalability (while a bit bumpy) for the uncompressed dataset, reaching its maximum (35 GB/s) at 40 threads.  Regarding the compressed dataset, it scales much more smoothly, and we see how the performance peaks at 64 threads (15 GB/s, using BloscLZ) and then drops significantly after that point (even if the CPU still has enough cores to continue the scaling; I am not sure why is that).  Incidentally, the BloscLZ codec being the best performer here is not a coincidence as it recently received a lot of fine-tuning for ARM.</p>
</section></section><section id="what-we-learned"><h2>What We Learned</h2>
<p>We have explored how to use compression in an nearly optimal way to perform a very simple task: compute an aggregation out of a large dataset.  With a basic understanding of the cache and memory subsystem, and by using appropriate compressed data structures (the super-chunk), we have seen how we can easily produce code that enables modern CPUs to perform operations on compressed data at a speed that approaches the speed of the same operations on uncompressed data (and sometimes exceeding it).  More in particular:</p>
<ol class="arabic simple">
<li><p>Performance for the compressed dataset scales very well on the number of threads for all the CPUs (even hyperthreading seems very beneficial at that, which is a welcome surprise).</p></li>
<li><p>The CPUs that benefit the most from compression are those with relatively low memory bandwidth and CPUs with many cores.  In particular, the EPYC architecture is a good example and we have shown how the compressed dataset can operate 50% faster that the uncompressed one.</p></li>
<li><p>Even when using CPUs with a low number of cores (e.g. our reference CPU, with only 4) we can achieve computational speeds on compressed data that can be on par with traditional, uncompressed computations, while saving precious amounts of memory and disk space.</p></li>
<li><p>The appropriate codec (and other parameters) to use within Blosc2 for maximum performance can vary depending on the dataset and the CPU used.  Having a way to automatically discover the optimal compression parameters would be a nice addition to the Blosc2 library.</p></li>
</ol></section><section id="final-thoughts"><h2>Final Thoughts</h2>
<p>To conclude, it is interesting to remember here what Linus Torvalds said back in 2006 (talking about the git system that he created the year before):</p>
<blockquote>
<p>[...] git actually has a simple  design, with stable and reasonably well-documented data structures.  In fact, I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful.
[...] I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.</p>
</blockquote>
<p>Of course, we all know how drastic Linus can be in his statements, but I cannot agree more on how important is to adopt a data-driven view when designing our applications.  But I'd go further and say that, when trying to squeeze the last drop of performance out of modern CPUs, data containers need to be structured in a way that leverages the characteristics of the underlying CPU, as well as to facilitate the application of the blocking technique (and thereby allowing compression to run efficiently).  Hopefully, installments like this can help us explore new possibilities to break down the memory wall that bedevils modern computing.</p>
</section><section id="acknowledgements"><h2>Acknowledgements</h2>
<p>Thanks to my friend Scott Prater for his great advices on improving my writing style, Dirk Schwanenberg for pointing out to the precipitation dataset and for providing the script for reading it, and Robert McLeod, J. David Ibáñez and Javier Sancho for suggesting general improvements (even though some of their suggestions required such a big amount of work that made me ponder about their actual friendship :).</p>
</section><section id="appendix-software-used"><h2>Appendix: Software used</h2>
<p>For reference, here it is the software that has been used for this blog entry:</p>
<ul class="simple">
<li><p><strong>OS</strong>: Ubuntu 18.04</p></li>
<li><p><strong>Compiler</strong>: GCC 7.3.0</p></li>
<li><p><strong>C-Blosc2</strong>: 2.0.0a6.dev (2018-05-18)</p></li>
</ul></section>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/new-forward-compat-policy/" class="u-url">New Forward Compatibility Policy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/francesc-alted/">Francesc Alted</a>
            </span></p>
            <p class="dateline">
            <a href="posts/new-forward-compat-policy/" rel="bookmark">
            <time class="published dt-published" datetime="2018-02-21T15:32:20Z" itemprop="datePublished" title="2018-02-21 15:32">2018-02-21 15:32</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/new-forward-compat-policy/#disqus_thread" data-disqus-identifier="cache/posts/new-forward-compat-policy.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<section id="the-215-issue"><h2>The #215 issue</h2>
<p>Recently, a C-Blosc user filed an <a class="reference external" href="https://github.com/Blosc/c-blosc/issues/215">issue</a> describing how buffers created with a modern version of C-Blosc (starting from 1.11.0) were not able to be decompressed with an older version of the library (1.7.0), i.e. C-Blosc was effectively breaking the so-called <em>forward-compatibility</em>.  After some investigation, it turned out that the culprit was an optimization that was introduced in 1.11.0 in order to allow better compression ratios and in some cases, better speed too.</p>
<p>Not all the codecs inside C-Blosc were equally affected; the ones that are experiencing the issue are LZ4, LZ4HC and Zlib (quite luckily, BloscLZ, the default codec, is not bitten by this and should be forward compatible probably til 1.0.0); that is, when a user is using a modern C-Blosc library (&gt; 1.11.0) <em>and</em> is using any of the affected codecs, there are situations (namely when the shuffle or bitshuffle filter are active and the buffers to be compressed are larger than 32 KB) that the result cannot be decompressed with older versions (&lt; 1.11.0).</p>
</section><section id="why-this-occurred"><h2>Why this occurred?</h2>
<p>Prior to 1.11.0, Blosc has traditionally split the internal blocks (the different pieces in which the buffer to be compressed is divided) into smaller pieces composed by the <em>same significant</em> bytes that the shuffle filter has put together.  The rational for doing this is that these pieces are, in many cases, hosting values that are either zeros or very close byte values (this is why shuffle works well in general for binary data), and asking the codec to compress these split-blocks separately was quite less effort than compressing a complete block hence providing more speed).</p>
<p>However, I realized that the so-called High Compression Ration codecs (the HCR codecs inside BLosc are LZ4HC, Zlib and Zstd) generally benefited from this split not happening (the reason is clear: more data means more opportunities for finding more duplicates) and in some cases, the speed was better too.  So, in C-Blosc 1.11.0, I decided that the split was not going to happen <em>by default</em> for HCR codecs (in the last minute I decided to include LZ4 too, for which the experiments showed a noticeable performance bump too, see below). Fortunately, the Zstd codec was introduced (in an out-of-beta way) at the very same 1.11.0 release than this split-block change, so in practice data compressed with the Zstd codec is not affected by this.</p>
</section><section id="new-forward-compatibility-enforcement-policy"><h2>New forward compatibility enforcement policy</h2>
<p>Although this change was deliberate and every measure was put in making it <em>backward compatible</em> (i.e. new library versions could read buffers compressed with older versions), I was not really aware of the inconveniences that the change was putting for people creating data files using newer versions of the library and expecting these to be read with older versions.</p>
<p>So in order to prevent something like this to happen again, I decided that <em>forward compatibility</em> is going to be <em>enforced</em> for future releases of C-Blosc (just for 1.x series; C-Blosc 2.x should be just backward compatible with 1.x).  By the way, this new <em>forward compatibility</em> policy will require a significantly more costly <a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/RELEASING.rst#forward-compatibility-testing">release procedure</a>, as different libraries for a specific set of versions have to be manually re-created; if you know a more automatic way to test forward compatibility with old versions of a library, I'd love to hear your comments.</p>
</section><section id="measures-taken-and-new-split-mode"><h2>Measures taken and new split mode</h2>
<p>In order to alleviate this forward incompatibility issue, I decided to revert the split change introduced in 1.11.0 in forthcoming 1.14.0 release.  That means that, <em>by default</em>, compressed buffers created with C-Blosc 1.14.0 and on will be forward compatible with all the previous C-Blosc libraries (till 1.3.0, which was when support for different codecs was introduced).  That is, the only buffers that will pose problems to be decompressed with old versions are those created with a C-Blosc library with versions between 1.11.0 and 1.14.0 <em>and</em> using the shuffle/bitshuffle filter in combination with the LZ4, LZ4HC or Zlib codecs.</p>
<p>For fine-tuning how the block-split would happen or not, I have introduced a new API function, <cite>void blosc_set_splitmode(int mode)</cite>, that allows to select the split mode that is going to be used during the compression.  The split modes that can take the new function are:</p>
<ul class="simple">
<li><p>BLOSC_FORWARD_COMPAT_SPLIT</p></li>
<li><p>BLOSC_AUTO_SPLIT</p></li>
<li><p>BLOSC_NEVER_SPLIT</p></li>
<li><p>BLOSC_ALWAYS_SPLIT</p></li>
</ul>
<p><cite>BLOSC_FORWARD_COMPAT_SPLIT</cite> offers reasonably forward compatibility (i.e. Zstd still will not split, but this is safe because it was introduced at the same time than the split change in 1.11.0), <cite>BLOSC_AUTO_SPLIT</cite> is for nearly optimal results (based on heuristics; this is the same approach than the one introduced in 1.11.0), <cite>BLOSC_NEVER_SPLIT</cite> and <cite>BLOSC_ALWAYS_SPLIT</cite> are for the user interested in experimenting for getting best compression ratios and/or speed.  If <cite>blosc_set_splitmode()</cite> is not called, the default mode will be BLOSC_FORWARD_COMPAT_SPLIT.</p>
<p>Also, the user will be able to specify the split mode by using the <cite>BLOSC_SPLITMODE</cite> variable environment.  If that variable exists in the environment, and has any value among:</p>
<ul class="simple">
<li><p>'BLOSC_FORWARD_COMPAT_SPLIT'</p></li>
<li><p>'BLOSC_AUTO_SPLIT'</p></li>
<li><p>'BLOSC_NEVER_SPLIT'</p></li>
<li><p>'BLOSC_ALWAYS_SPLIT'</p></li>
</ul>
<p>this will select the corresponding split mode.</p>
</section><section id="how-this-change-affects-performance"><h2>How this change affects performance</h2>
<p>So as to allow to visualize at a glance the differences in performance that the new release is introducing, let's have a look at the impact on two of the most used codecs inside C-Blosc: LZ4 and LZ4HC.  In the plots below the left side is the pre-1.14.0 version (non-split blocks) and on the right, the forthcoming 1.14.0 (split blocks).  Note that I am using here the typical synthetic benchmarks for C-Blosc, so expect a different outcome for your own data.</p>
<p>Let's start by LZ4HC, a High Compression Ratio codec (and the one that triggered the initial report of the <a class="reference external" href="https://github.com/Blosc/c-blosc/issues/215">forward compatibility issue</a>).  When compressing, we have this change in behavior:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4hc-c" src="images/new-forward-compat-policy/suite-lz4hc-pre-1.14-compr.png"></p></td>
<td><p><img alt="lz4hc-compat-c" src="images/new-forward-compat-policy/suite-lz4hc-compat-compr.png"></p></td>
</tr></tbody>
</table>
<p>For LZ4HC decompression:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4hc-d" src="images/new-forward-compat-policy/suite-lz4hc-pre-1.14-decompr.png"></p></td>
<td><p><img alt="lz4hc-compat-d" src="images/new-forward-compat-policy/suite-lz4hc-compat-decompr.png"></p></td>
</tr></tbody>
</table>
<p>For LZ4HC one can see that, when using non-split blocks, it can achieve better compression ratios (this is expected, as the block sizes are larger).  Speed-wise the performance is quite similar, with maybe some advantage for split blocks (expected as well).  As the raison d'être for HCR codecs is maximize the compression ration, that was the reason why I did the split change for LZ4HC in 1.11.0.</p>
<p>And now for LZ4, a codec meant for speed (although it normally gives pretty decent results in compression ratio).  When compressing, here it is the change:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4-c" src="images/new-forward-compat-policy/suite-lz4-pre-1.14-compr.png"></p></td>
<td><p><img alt="lz4-compat-c" src="images/new-forward-compat-policy/suite-lz4-compat-compr.png"></p></td>
</tr></tbody>
</table>
<p>For LZ4 decompression:</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody><tr>
<td><p><img alt="lz4-d" src="images/new-forward-compat-policy/suite-lz4-pre-1.14-decompr.png"></p></td>
<td><p><img alt="lz4-compat-d" src="images/new-forward-compat-policy/suite-lz4-compat-decompr.png"></p></td>
</tr></tbody>
</table>
<p>So, here one can see that when using Blosc pre-1.14 (i.e. non-split blocks) we are getting a bit less compression ratio than for the forthcoming 1.14, which even if counter-intutive, it matches my experience with non-HCR codecs.  Speed-wise the difference is not that much during compression; however, decompression is significantly faster with non-split blocks.  As LZ4 is meant for speed, this was possibly the reason that pushed me towards making non-split blocks by default for LZ4 in addition to HCR codecs in 1.11.0.</p>
</section><section id="feedback"><h2>Feedback</h2>
<p>If you have suggestions on this forward compatibility issue or the solution that has been implemented, please shout!</p>
</section><section id="appendix-hardware-and-software-used"><h2>Appendix: Hardware and software used</h2>
<p>For reference, here it is the configuration that I used for producing the plots in this blog entry.</p>
<ul class="simple">
<li><p>CPU: Intel Xeon E3-1245 v5 @ 3.50GHz (4 physical cores with hyper-threading)</p></li>
<li><p>OS:  Ubuntu 16.04</p></li>
<li><p>Compiler: GCC 6.3.0</p></li>
<li><p>C-Blosc: 1.13.7 and 1.14.0 (release candidate)</p></li>
<li><p>LZ4: 1.8.1</p></li>
</ul></section>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="." rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blosc";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2021         <a href="mailto:blosc@blosc.org">The Blosc Developers</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
