<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blosc Main Blog Page  (Posts about blosclz)</title><link>http://blosc.org/</link><description></description><atom:link href="http://blosc.org/categories/blosclz.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2021 &lt;a href="mailto:blosc@blosc.org"&gt;The Blosc Developers&lt;/a&gt; </copyright><lastBuildDate>Wed, 30 Jun 2021 05:02:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Fine Tuning the BloscLZ codec</title><link>http://blosc.org/posts/blosclz-tuning/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;p&gt;Yesterday I was reading about the exciting new CPU architectures that both &lt;a class="reference external" href="http://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade"&gt;AMD and Intel are introducing&lt;/a&gt; and I was wondering how the improved architecture of the new cores and most specially, its caches, could apply to Blosc.  It turns out that I have access to a server with a relatively modern CPU (Xeon E3-1245 v5 @ 3.50GHz, with 4 physical cores) and I decided to have a go at fine-tune the included BloscLZ codec (the one that I know the best) inside C-Blosc2.  Of course, I already spent some time tuning BloscLZ, but that was some years ago and provided the fast pace at which CPUs are evolving I thought that this was excellent timing for another round of fine-tuning, most specially in preparation for users adopting the forthcoming  RYZEN, Threadripper, EPYC and Skylake-SP architectures.&lt;/p&gt;
&lt;p&gt;Frankly speaking, I was expecting to get very little improvements in this front, but the results have been unexpectedly good.  Keep reading.&lt;/p&gt;
&lt;section id="where-we-come-from"&gt;
&lt;h2&gt;Where we come from&lt;/h2&gt;
&lt;p&gt;Just for reference, here it is the performance of the BloscLZ codec in my server before the new tuning work:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-old-c" src="http://blosc.org/images/blosclz-tuning/blosclz-suite-8p-old-param-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-old-d" src="http://blosc.org/images/blosclz-tuning/blosclz-suite-8p-old-param-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That is the typical synthetic benchmark in Blosc, but for the plotting function in the C-Blosc2 project, the actual size of each compressed buffer is shown (and not the size of the whole dataset, as in C-Blosc1).  In this case, the dataset (256 MB) is split in chunks of 4 MB, and provided that our CPU has a LLC (Last Level Cache) of 8 MB, this is sort of an optimal size for achieving maximum performance (the buffers meant for Blosc usually do not exceed 4 MB for most of its common usages).&lt;/p&gt;
&lt;p&gt;As can be seen, performance is quite good, although compression ratios left something to be desired.  Furthermore, for the maximum compression level (9), the compression ratio has a regression with respect to the previous level (8).  This is not too bad, and sometimes happens in any codec, but the nice thing would be to avoid it if possible.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="the-new-blosclz-after-fine-tuning"&gt;
&lt;h2&gt;The new BloscLZ after fine tuning&lt;/h2&gt;
&lt;p&gt;So, after a couple of hours playing with different parameters in BloscLZ and C-Blosc2, I started to realize that the new Intel CPU performed exceedingly well when asked to compress more, to the point that high compression settings were not performing that slow in comparision with low compression ones; rather the contrary: high compression settings were operating at almost the same speed than lower ones (which was a welcome surprise indeed).  Hence I tried to be set quite more aggressive parameters in BloscLZ, while trying to keep the size of internal blocks in Blosc2 below 256 KB (the typical size of L2 caches in modern CPUs).  This is the result:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-new-c" src="http://blosc.org/images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-new-d" src="http://blosc.org/images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So the compression ratios have increased quite a bit, specially for the larger compression levels (going from  less than 10x to more than 20x for this benchmark).  This is courtesy of the new, more agressive compression parameters.  Strikingly enough, performance has also increased in general, but specially for these large compression levels.  I am not completely certain on why this is the case, but probably this new CPU architecture is much better at out-of-order execution and prefetching larger blocks of data, which benefits compressing both faster even in large buffers; similarly, I am pretty sure that improvements in compiler technology (I am using a recent GCC 6.3.0 here) is pretty important for getting faster binary code.  We can also see that when using 4 threads (i.e. using all the physical cores available in our CPU at hand), BloscLZ can compress &lt;em&gt;faster&lt;/em&gt; than a memcpy() call for most of the cases, and most specially at large compression levels, as mentioned before.  Oh, and we can see that we also got rid of the regression in the compression ratio for compression level 9, which is cool.&lt;/p&gt;
&lt;p&gt;Regarding decompression speed, we can see that the new tuning gave general speed-ups of between 10% and 20%, with no significant slowdowns in any case.  All in all, quite good results indeed!&lt;/p&gt;
&lt;/section&gt;
&lt;section id="room-for-more-improvements-enter-pgo"&gt;
&lt;h2&gt;Room for more improvements?  Enter PGO.&lt;/h2&gt;
&lt;p&gt;To temporary end (optimization is a never ending task) this quest for speed, I am curious about the speed that we can buy by using the PGO (&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Profile-guided_optimization"&gt;Profile Guided Optimization&lt;/a&gt;) capability that is present in most of the modern compilers.  Here I am going to use the PGO of GCC in combination with our benchmark at hand so as to provide the profile for the compiler optimizer.  Here are the results when PGO is applied to the new parametrization:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col style="width: 50%"&gt;
&lt;col style="width: 50%"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-pgo-c" src="http://blosc.org/images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6.pgo-compr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;img alt="blosclz-pgo-d" src="http://blosc.org/images/blosclz-tuning/blosclz-suite-8p-new-param2-gcc6.pgo-decompr.png"&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So, while the speed improvement for compression is not significant (albeit a bit better), the big improvement comes in the decompression speed, where we see speeds almost reaching 50 GB/s and perhaps more interestingly, more than 35 GB/s for maximum compression level, and for first time in my life as Blosc developer, I can see the speed of decompressing with &lt;em&gt;one single thread&lt;/em&gt; being faster than memcpy() for &lt;em&gt;all&lt;/em&gt; the compression levels.&lt;/p&gt;
&lt;p&gt;I wonder what the PGO technique can bring to other codecs in Blosc, but that is stuff for other blog post.  At any rate, the reader is encouraged to try PGO on their own setups.  I am pretty sure that she will be pleased to see nice speed improvements.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="appendix-hardware-and-software-used"&gt;
&lt;h2&gt;Appendix: Hardware and software used&lt;/h2&gt;
&lt;p&gt;For reference, here it is the configuration that I used for producing the plots in this blog entry.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;CPU: Intel Xeon E3-1245 v5 @ 3.50GHz (4 physical cores with hyper-threading)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;OS:  Ubuntu 16.04&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compiler: GCC 6.3.0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;C-Blosc2: 2.0.0a4.dev (2017-07-14)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BloscLZ: 1.0.6 (2017-07-14)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>blosclz</category><category>optimization</category><category>tuning</category><guid>http://blosc.org/posts/blosclz-tuning/</guid><pubDate>Fri, 14 Jul 2017 06:32:20 GMT</pubDate></item><item><title>Seeking Sponsorship for Bcolz/Blosc</title><link>http://blosc.org/posts/seeking-sponsoship/</link><dc:creator>Valentin Haenel</dc:creator><description>&lt;div&gt;&lt;p&gt;Dear Everyone,&lt;/p&gt;
&lt;p&gt;as you may or may not know, the &lt;a class="reference external" href="https://github.com/blosc/c-blosc"&gt;Blosc&lt;/a&gt;
compressor has become the basis for some novel, innovative technological
experiments in the PyData space.  Especially the &lt;a class="reference external" href="https://github.com/blosc/bcolz"&gt;Bcolz&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/blosc/bloscpack"&gt;Bloscpack&lt;/a&gt; projects which provide a way to perform
out-of-core computations on column based datasets have become particularly
interesting for the analysis of medium-sized time-series datasets.&lt;/p&gt;
&lt;p&gt;In this post, we would like to convince you to give us some money to
foster the project, development and accelerate growth of our community.
Historically, it has always been a difficult endeavour to monetize
open-source development and so, below is a non-exhaustive list of
potential models that we are considering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Direct sponsoring / Donations&lt;/p&gt;
&lt;p&gt;This involves paying either a single lump-sum or monthly installments
to foster continued development and innovation. This type of
sponsoring isn't bound to any specific goal or feature and would allow
us for example maintain and release the projects regularly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Feature-driven sponsoring&lt;/p&gt;
&lt;p&gt;Paying for specific features to be implemented, bugs to be fixed or
paying to have a voice when it comes to prioritizing items in the
issue-tracker(s).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hiring us as freelancers for Blosc/Bcolz projects&lt;/p&gt;
&lt;p&gt;This means that you hire one or both of us to implement a project that
uses bcolz inside your company. Any bugs we find or improvements that
need to be made would flow back into the open source code-base.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hiring us as part-time freelancers for general projects&lt;/p&gt;
&lt;p&gt;This means you hire one or both of us as part-time freelancers for two
to three days a week to work on general projects. These can be related
to Python and data or open-source work on other projects. This would
allow us to spend the remaining days on Blosc/Bcolz.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PhD positions&lt;/p&gt;
&lt;p&gt;There are still a few interesting theoretical aspects to be unlocked,
for example certain mathematical properties of the shuffle filter and
a compressed extension of the external-memory-model (EMM) to analyse
the runtime of Blosc style out-of-core algorithms and Bcolz operations
in general.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We welcome any feedback regarding the above options and please do tell
us about any additional models that may be interesting to us or for you.&lt;/p&gt;
&lt;p&gt;With best wishes and looking forward to your input,&lt;/p&gt;
&lt;p&gt;Francesc Alted and Valentin Haenel&lt;/p&gt;&lt;/div&gt;</description><category>blosc</category><category>blosclz</category><category>sponsorhip</category><guid>http://blosc.org/posts/seeking-sponsoship/</guid><pubDate>Tue, 26 May 2015 08:41:20 GMT</pubDate></item><item><title>Compress Me, Stupid!</title><link>http://blosc.org/posts/compress-me-stupid/</link><dc:creator>Francesc Alted</dc:creator><description>&lt;div&gt;&lt;section id="how-it-all-started"&gt;
&lt;h2&gt;How it all started&lt;/h2&gt;
&lt;p&gt;I think I began to become truly interested in compression when, back
in 1992, I was installing &lt;a class="reference external" href="http://en.wikipedia.org/wiki/C_News"&gt;C-News&lt;/a&gt;, a news server package meant
to handle &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Usenet"&gt;Usenet News&lt;/a&gt;
articles in &lt;a class="reference external" href="http://www.uji.es"&gt;our university&lt;/a&gt;.  For younger
audiences, Usenet News was a very popular way to discuss about all
kind of topics, but at the same time it was pretty difficult to cope
with the huge amount of articles, specially because spam practices
started to appear by that time.  As Gene Spafford put it in 1992:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Usenet is like a herd of performing elephants with
diarrhea. Massive, difficult to redirect, awe-inspiring,
entertaining, and a source of mind-boggling amounts of excrement
when you least expect it."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But one thing was clear: Usenet brought &lt;strong&gt;massive&lt;/strong&gt; amounts of data
that had to be transmitted through the typical low-bandwidth data
lines of that time: 64 Kbps shared for everyone at our university.&lt;/p&gt;
&lt;p&gt;My mission then was to bring the Usenet News feed by making use of as
low of resources as possible.  Of course, one of the first things that
I did was to start news transmission during the night, when everyone
was warm at bed and nobody was going to complain about others stealing
the precious and scarce Internet bandwidth.  Another measure was to
subscribe to just a selection of groups so that the transmission would
end before the new day would start.  And of course, I started
experimenting with compression for maximizing the number of groups
that we could bring to our community.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="compressing-usenet-news"&gt;
&lt;h2&gt;Compressing Usenet News&lt;/h2&gt;
&lt;p&gt;The most used compressor by 1992 was &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Compress"&gt;compress&lt;/a&gt;, a Unix program based on the
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/LZW"&gt;LZW&lt;/a&gt; compression algorithm.  But
LZW had patents issues, so by that time Jean-Loup Gailly and Mark
Adler started the work with &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Gzip"&gt;gzip&lt;/a&gt;.  At the beginning of 1993 gzip
1.0 was ready for consumption and I find it exciting not only because
it was not patent-encumbered, but also because it compressed way
better than the previous &lt;code class="docutils literal"&gt;compress&lt;/code&gt; program, allowed different
compression levels, and it was pretty fast too (although &lt;code class="docutils literal"&gt;compress&lt;/code&gt;
still had an advantage here, IIRC).&lt;/p&gt;
&lt;p&gt;So I talked with &lt;a class="reference external" href="http://www.uv.es"&gt;the university&lt;/a&gt; that was
providing us with the News feed and we manage to start compressing it,
first with &lt;code class="docutils literal"&gt;compress&lt;/code&gt; and then with &lt;code class="docutils literal"&gt;gzip&lt;/code&gt;.  Shortly after that,
while making measurements on the new gzip improvements, I discovered
that the bottleneck was in our News workstation (an HP 9000-730 with a
speedy &lt;a class="reference external" href="http://en.wikipedia.org/wiki/PA-RISC"&gt;PA-7000 RISC microprocessor&lt;/a&gt; @ 66 MHz) being unable to
decompress all the gzipped stream of subscribed news on-time.  The
bottleneck suddenly changed from the communication line to the CPU!&lt;/p&gt;
&lt;p&gt;I remember spending large hours playing with different combinations of
data chunk sizes and gzip compression levels, plotting the results
(with the fine &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Gnuplot"&gt;gnuplot&lt;/a&gt;)
before finally coming with a combination that stroked a fair balance
between available bandwidth and CPU speed, maximizing the amount of
news articles hitting our university.  I think this was my first
realization of how compression could help bringing data faster to the
system, making some processes more effective.  In fact, that actually
blew-up my mind and made me passionate about compression technologies
for the years to come.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="lzo-and-the-explosion-of-compression-technology"&gt;
&lt;h2&gt;LZO and the explosion of compression technology&lt;/h2&gt;
&lt;p&gt;During 1996, Markus F.X.J. Oberhumer started to announce the
availability of his own set of LZO compressors.  These consisted in
many different compressors, all of them being variations of his own
compression algorithm (LZO), but tweaked to achieve either better
compression ratios or compression speed.  The suite was claimed to
being able to achieve speeds reaching &lt;strong&gt;1/3 of the memory speed&lt;/strong&gt; of
the typical Pentium-class computers available at that time.  An entire
set of compressors being able to approach memory speed? boy, that was
a very exciting news for me.&lt;/p&gt;
&lt;p&gt;LZO was in the back of my mind when I started my work on &lt;a class="reference external" href="http://www.pytables.org"&gt;PyTables&lt;/a&gt; in August 2002 and shortly after, in &lt;a class="reference external" href="http://pytables.org/svn/pytables/tags/std-0.5/README.txt"&gt;May
2003&lt;/a&gt;,
PyTables gained support for LZO.  My goal was indeed to accelerate
data transmission from disk to the CPU (and back), and &lt;a class="reference external" href="http://pytables.github.io/usersguide/optimization.html#understanding-chunking"&gt;these plots&lt;/a&gt;
are testimonial of how beneficial LZO was for achieving that goal.
Again, compression was demonstrating that it could effectively
increase disk bandwidth, and not only slow internet lines.&lt;/p&gt;
&lt;p&gt;However, although LZO was free of patent issues and fast as hell,
it had a big limitation for a project like PyTables: the licensing.
LZO was using the GPL license, and that prevented the inclusion of its
sources in distributions without re-licensing PyTables itself as GPL,
a thing that I was not willing to do (PyTables has a BSD license, as
it is usual in the NumPy ecosystem).  Because of that, LZO was a nice
compressor to be included in GPL projects like the Linux kernel
itself, but not a good fit for PyTables (although support for LZO still
exists, as long as it is downloaded and installed separately).&lt;/p&gt;
&lt;p&gt;By that time (mid 2000's) it started to appear a plethora of fast
compressors with the same spirit than LZO, but with more permissive
licenses (typically BSD/MIT), many of them being a nice fit for PyTables.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="a-new-compressor-for-pytables-hdf5"&gt;
&lt;h2&gt;A new compressor for PyTables/HDF5&lt;/h2&gt;
&lt;p&gt;By 2008 it was clear that PyTables needed a compressor whose sources
could be included in the PyTables tarball, so minimizing the
installation requirements.  For this I started considering a series of
libraries and immediately devised &lt;a class="reference external" href="http://fastlz.org/"&gt;FastLZ&lt;/a&gt; as a
nice candidate because of its simplicity and performance.  Also,
FastLZ had a permissive MIT license, which was what I was looking for.&lt;/p&gt;
&lt;p&gt;But pure FastLZ was not completely satisfactory because it was not
simple enough.  It had 2 compression levels that
complicated the implementation quite a bit, so I decided to keep just the
highest level, and then optimize certain parts of it so that speed
would be acceptable.  These modifications gave birth to BloscLZ, which
is still being default compressor in Blosc.&lt;/p&gt;
&lt;p&gt;But I had more ideas on what other features the new Blosc compressor
should have, namely, multi-threading and an integrated shuffle filter.
Multi-threading made a lot of sense by 2008 because both Intel and AMD
already had a wide range of multi-core processors by then, and it was
clear that the race for throwing more and more cores into systems was
going to intensify.  A fast compressor had to be able to use all these
cores dancing around, &lt;strong&gt;period&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Shuffle (see slide 71 of this &lt;a class="reference external" href="http://blosc.org/docs/StarvingCPUs.pdf"&gt;presentation&lt;/a&gt;) was the other important
component of the new compressor.  This algorithm relies on
neighboring elements of a dataset being highly correlated to improve
data compression.  A shuffle filter already came as part of the &lt;a class="reference external" href="http://www.hdfgroup.org/HDF5/"&gt;HDF5
library&lt;/a&gt; but it was implemented in
pure C, and as it had an important overhead in terms of computation, I
decided to do an &lt;a class="reference external" href="https://github.com/Blosc/c-blosc/blob/master/blosc/shuffle.c"&gt;SIMD version&lt;/a&gt; using
the powerful &lt;a class="reference external" href="http://en.wikipedia.org/wiki/SSE2"&gt;SSE2 instructions&lt;/a&gt;
present in all Intel and AMD processors since 2003.  The result is
that this new shuffle implementation adds almost zero overhead
compared with the compression/decompression stages.&lt;/p&gt;
&lt;p&gt;Once all of these features were implemented, I designed a pretty
comprehensive &lt;a class="reference external" href="http://blosc.org/synthetic-benchmarks.html"&gt;suite of tests&lt;/a&gt; and asked the PyTables
community to help me testing the new compressor in as much systems as
possible.  After some iterations, we were happy when the new
compressor worked flawlessly compressing and decompressing &lt;strong&gt;hundreds
of terabytes&lt;/strong&gt; on many different Windows and Unix boxes, both in
32-bit and 64-bit.  The new beast was ready to ship.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="blosc-was-born"&gt;
&lt;h2&gt;Blosc was born&lt;/h2&gt;
&lt;p&gt;I then grabbed BloscLZ, the multi-threading support and the
SSE2-powered shuffle and put it all in the same package.  That also became a
&lt;strong&gt;standalone, pure C library&lt;/strong&gt;, with no attachments to PyTables or HDF5,
so any application could make
use of it.  I have got the first stable version (1.0) of Blosc
released by &lt;a class="reference external" href="http://www.groupsrv.com/science/about538609.html"&gt;July 2010&lt;/a&gt;.
Before this, I already introduced Blosc publicly in my &lt;a class="reference external" href="http://www.blosc.org/docs/StarvingCPUs.pdf"&gt;EuroSciPy 2009 keynote&lt;/a&gt; and also made a small
reference to it in an article about &lt;a class="reference external" href="http://www.blosc.org/docs/StarvingCPUs-CISE-2010.pdf"&gt;Starving CPUs&lt;/a&gt; where I
stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"As the gap between CPU and memory speed continues to widen, I
expect Blosc to improve memory-to-CPU data transmission rates over
an increasing range of datasets."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And that is the thing.  As CPUs are getting faster, the chances for
using compression for an advantage can be applied to more and more
scenarios, to the point that improving the bandwidth of main memory
(RAM) is becoming possible now.  And surprisingly enough, the methodology
for achieving that is the same than back in the C-news ages: strike a good
balance between data block sizes and compression speed, and let
compression make your applications handle data faster and not only
making it more compact.&lt;/p&gt;
&lt;p&gt;When seen in perspective, it has been a long quest over the last
decades.  During the 90's, compression was useful to improve the
bandwidth of slow internet connections.  In the 2000's, it made
possible accelerating disk I/O operation.  In the 2010's Blosc goal is
making the memory subsystem faster and whether it is able to
achieve this or not will be the subject of future blogs (hint: data
arrangement is critical too).  But one
thing is clear, achieving this (by Blosc or any other compressor out
there) is just a matter of time.  Such is the fate of the ever
increasing gap in CPU versus memory speeds.&lt;/p&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>blosc</category><category>blosclz</category><category>hdf5</category><category>history</category><category>pytables</category><guid>http://blosc.org/posts/compress-me-stupid/</guid><pubDate>Thu, 28 Aug 2014 17:01:20 GMT</pubDate></item></channel></rss>